<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="brief-report">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3221047</article-id>
      <article-id pub-id-type="pmid">21256225</article-id>
      <article-id pub-id-type="publisher-id">YNIMG8012</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.044</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Technical Note</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Comparing the similarity and spatial structure of neural representations: A pattern-component model</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Diedrichsen</surname>
            <given-names>Jörn</given-names>
          </name>
          <email>j.diedrichsen@ucl.ac.uk</email>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="cr0005" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ridgway</surname>
            <given-names>Gerard R.</given-names>
          </name>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>Karl J.</given-names>
          </name>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Wiestler</surname>
            <given-names>Tobias</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
        </contrib>
      </contrib-group>
      <aff id="af0005"><label>a</label>Institute of Cognitive Neuroscience, University College London, London, UK</aff>
      <aff id="af0010"><label>b</label>Wellcome Trust Centre for Neuroimaging, University College London, London, UK</aff>
      <author-notes>
        <corresp id="cr0005"><label>⁎</label>Corresponding author at: Institute of Cognitive Neuroscience, University College London, Alexandra House, 17 Queen Square, London WC1N 3AR, UK. Fax: + 44 20 7813 2835. <email>j.diedrichsen@ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>15</day>
        <month>4</month>
        <year>2011</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>15</day>
        <month>4</month>
        <year>2011</year>
      </pub-date>
      <volume>55</volume>
      <issue>4-3</issue>
      <fpage>1665</fpage>
      <lpage>1678</lpage>
      <history>
        <date date-type="received">
          <day>9</day>
          <month>11</month>
          <year>2010</year>
        </date>
        <date date-type="rev-recd">
          <day>11</day>
          <month>1</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>13</day>
          <month>1</month>
          <year>2011</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2011 Elsevier Inc.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>In recent years there has been growing interest in multivariate analyses of neuroimaging data, which can be used to detect distributed patterns of activity that encode an experimental factor of interest. In this setting, it has become common practice to study the correlations between patterns to make inferences about the way a brain region represents stimuli or tasks (known as representational similarity analysis). Although it would be of great interest to compare these correlations from different regions, direct comparisons are currently not possible. This is because sample correlations are strongly influenced by voxel-selection, fMRI noise, and nonspecific activation patterns, all of which can differ widely between regions. Here, we present a multivariate modeling framework in which the measured patterns are decomposed into their constituent parts. The model is based on a standard linear mixed model, in which pattern components are considered to be randomly distributed over voxels. The model allows one to estimate the true correlations of the underlying neuronal pattern components, thereby enabling comparisons between different regions or individuals. The pattern estimates also allow us to make inferences about the spatial structure of different response components. Thus, the new model provides a theoretical and analytical framework to study the structure of distributed neural representations.</p>
      </abstract>
      <abstract abstract-type="graphical">
        <title>Research Highlights</title>
        <p>► Method to decompose multivariate voxel patterns into different components. ► Yields unbiased estimates of pattern correlations that can be compared across regions. ► Allows the study of the spatial structure of different pattern components.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <title>Introduction</title>
      <p>Recent years have seen a rapid growth of multivariate approaches to the analysis of functional imaging data. In comparison to more traditional mass-univariate approaches (<xref rid="bb0035 bb0140" ref-type="bibr">Friston et al., 1995; Worsley et al., 2002</xref>), multivariate pattern analysis (MVPA) can reveal changes in distributed patterns of neural activity (<xref rid="bb0050 bb0055 bb0060" ref-type="bibr">Haxby et al., 2001; Haynes and Rees, 2005a, b</xref>). A particularly interesting variant of these approaches can be described as “local” multivariate analyses (<xref rid="bb0020 bb0065" ref-type="bibr">Friman et al., 2001; Kriegeskorte et al., 2006</xref>). Rather than using the whole brain (<xref rid="bb0040" ref-type="bibr">Friston et al., 1996</xref>), groups of neighbouring voxels (or cliques) are analyzed. Cliques can be selected using anatomically based regions-of-interest (ROI), or using a so-called search light, where a spherical ROI is moved across the brain to generate a map of local information content (<xref rid="bb0065 bb0095" ref-type="bibr">Kriegeskorte et al., 2006; Oosterhof et al., 2010a</xref>). The key question addressed by these analyses is whether a group of voxels encodes a stimulus dimension or experimental factor. This involves demonstrating a significant mapping between the experimental factor and the distributed measured pattern (encoding models) or vice versa (decoding or classification models) (<xref rid="bb0030" ref-type="bibr">Friston, 2009</xref>). This can be done using cross-validation (<xref rid="bb0085 bb0090 bb0110" ref-type="bibr">Misaki et al., 2010; Norman et al., 2006; Pereira et al., 2009</xref>) or Bayesian approaches (<xref rid="bb0045" ref-type="bibr">Friston et al., 2008</xref>).</p>
      <p>Multivariate analyses not only show that a variable is encoded in a region, but can also tell us how this variable is encoded. One common approach is the so-called representational-similarity analysis (<xref rid="bb0070" ref-type="bibr">Kriegeskorte et al., 2008</xref>), which investigates the correlations (or some other similarity metric) between mean patterns of activations evoked by different stimuli or task conditions. For example, one region may show very similar patterns for condition A and B and for C and D, but large differences between these pairs of conditions. This indicates the dimensions over which pattern activity is modulated by different experimental manipulations and therefore how the population of neurons may represent a factor of interest. Such an approach would be especially powerful if one could compare between-pattern correlations from different regions, thereby revealing regional differences in representation and (by inference) computational function.</p>
      <p>However, the comparison of correlations (calculated between two conditions across voxels) across different regions is statistically invalid. This is because sample correlation coefficients are not a direct measure of the underlying similarity of two patterns, but are influenced by a number of other factors. For example, if the BOLD signal is noisier in one region than another (e.g. due to higher susceptibility to physiological artifacts, etc.) correlations will tend to be lower. Furthermore, the criteria by which one selects voxels over which to compute the correlation will strongly influence their size: If one picks a set of highly informative voxels the correlation between two patterns may be very high, but will decrease as more uninformative voxels are included. Finally, a particularly high correlation between two patterns does not necessarily indicate that the two specific conditions are encoded similarly; it could simply mean that there is a common (shared) response to any stimulus of this class. For these reasons, differences between sample correlations are largely un-interpretable. Thus, the best we can currently do is to compare the rank-ordering of correlations across different regions (<xref rid="bb0070" ref-type="bibr">Kriegeskorte et al., 2008</xref>), thereby disregarding valuable quantitative information.</p>
      <p>Here, we present a generative model of multivariate responses that addresses these issues and furnishes correlations that are insensitive to the level of noise, common activation, and voxel-selection. The model assumes that the observed patterns are caused by a set of underlying pattern components that relate to the different experimental conditions or noise. Critically, and in contrast to traditional univariate approaches, these pattern components are not considered to be vectors of unknown constants (one for each voxel). Rather, they are conceptualized as a random variable with a probability distribution across voxels with a certain mean, variance (or power), and covariance (or similarity) with other patterns. The core idea of our approach is to estimate the variances and covariances of the underlying pattern components directly, using the sample covariance of the observed data. This allows us to derive unbiased estimates of the true correlation coefficients among the distributed condition-specific pattern components. The implicit random-effects model for distributed responses is inherently multivariate as it uses variance–covariance information over groups of voxels—in contrast to the univariate fixed-effects model, in which we would estimate the pattern associated with a condition by calculating the mean response to that condition and subtract the mean pattern across conditions. Our model accommodates the fact that part of this average response is caused by noise and adjusts its estimates of variances, covariances and correlations accordingly.</p>
      <p>As in a Gaussian process model (<xref rid="bb0120" ref-type="bibr">Rasmussen and Williams, 2006</xref>), we recast the problem of estimating response patterns into the problem of estimating the variance–covariance of the underlying components. Because we parameterize the model in terms of variances and covariances, the correlation between different patterns, induced under different experimental conditions, is estimated in an explicit and unbiased fashion and can be used as a summary statistic for subsequent hypothesis testing about representational similarities. Furthermore, the approach can handle a large number of voxels with no increase in computational overhead or identifiability problems. This is because we focus on the second-order behaviour of the data (power or variance) as opposed to the first-order behaviour (patterns or mean).</p>
      <p>This paper comprises four sections. The first presents our pattern component model and shows how covariances among patterns can be specified and estimated. In the next section, we use a simple one-factorial design with 3 different stimuli to show how our method robustly accommodates different levels of noise or common activations, to furnish unbiased (corrected) correlation coefficients that can then be used for further analysis. Thirdly, we provide a more complex example that uses a two-factorial 4 × 2 design, and show how our method can be used to test specific hypothesis about how main effects and interactions are expressed in terms of distributed patterns. Using this experimental design, we then provide an illustrative application to real data. We also show that spatial correlations between voxels do not bias our covariance component estimation process. Finally we show that we can recover information about the spatial smoothness for each of the underlying pattern components, thereby characterising not only the similarity, but also the spatial structure of the underlying neural representations. <xref rid="s0085" ref-type="sec">Appendix A</xref> provides a detailed presentation of the estimation algorithm and methods for accelerating its computations.</p>
    </sec>
    <sec id="s0010">
      <title>The pattern component model</title>
      <sec id="s0015">
        <title>Model structure</title>
        <p>Let <bold>Y</bold> = [<bold>y</bold><sub>1</sub><sup><italic>r</italic></sup>,..., <bold>y</bold><sub><italic>N</italic></sub><sup><italic>r</italic></sup>]<sup><italic>T</italic></sup> ∈ <italic>ℜ</italic><sup><italic>N</italic> × <italic>P</italic></sup> be the data for N trials, each of which contains <italic>P</italic> voxels or features (<xref rid="f0005" ref-type="fig">Fig. 1</xref>). We will assume here that the data are summary statistics (e.g., regression coefficients) from a first-level time-series analysis, for example, the activation for each behavioural trial in an event-related fMRI paradigm<xref rid="fn0005" ref-type="fn">1</xref>. In other words, we assume that each row (<bold>y</bold><sup><italic>r</italic></sup>) of our data matrix <bold>Y</bold> is the measured pattern over spatial features (e.g., voxels), and that the rows constitute independent samples from different trials. For simplicity, we will assume that effects of no interest have been removed from the summary data. We can also split the data into <italic>P</italic> column vectors, each encoding the activity of a particular voxel for the <italic>N</italic> trials: <bold>Y</bold> = [<bold>y</bold><sub>1</sub><sup><italic>c</italic></sup>,..., <bold>y</bold><sub><italic>P</italic></sub><sup><italic>c</italic></sup>]. For convenience of notation, both <bold>y</bold><sup><italic>c</italic></sup> and <bold>y</bold><sup><italic>r</italic></sup> are column vectors.</p>
        <p>Each trial has an associated experimental or explanatory variable <bold>z</bold><sub><italic>n</italic></sub> ∈ <italic>ℜ</italic><sup><italic>Q</italic> × 1</sup>. This vector may consist of indicator variables denoting the experimental condition in a one- or multi-factorial design. Alternatively, <bold>z</bold><sub><italic>n</italic></sub> may contain a set of parametric variables. We assemble the experimental variables into a design matrix, with each row of the matrix corresponding to a single trial and each column to an experimental effect, <bold>Z</bold> = [<bold>z</bold><sub>1</sub>,..., <bold>z</bold><sub><italic>N</italic></sub>]<sup><italic>T</italic></sup> ∈ <italic>ℜ</italic><sup><italic>N</italic> × <italic>Q</italic></sup>.</p>
        <p>We start with a model that assumes the data are generated as a linear combination of <italic>Q</italic> pattern components plus some noise (see <xref rid="f0005" ref-type="fig">Fig. 1</xref>).<disp-formula id="fo0005"><label>(1)</label><alternatives><textual-form specific-use="jats-markup"><bold>Y</bold> = <bold>Z</bold><bold>U</bold> + <bold>E</bold></textual-form><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">U</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">E</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>The rows of the matrix <bold>U</bold> = [<bold>u</bold><sub>1</sub><sup><italic>r</italic></sup>,..., <bold>u</bold><sub><italic>Q</italic></sub><sup><italic>r</italic></sup>]<sup><italic>T</italic></sup> : = [<bold>u</bold><sub>1</sub><sup><italic>c</italic></sup>,..., <bold>u</bold><sub><italic>P</italic></sub><sup><italic>c</italic></sup>] ∈ <italic>ℜ</italic><sup><italic>Q</italic> × <italic>P</italic></sup> are the underlying pattern components associated with the <italic>Q</italic> experimental effects. <bold>E</bold> ∈ <italic>ℜ</italic><sup><italic>N</italic> × <italic>P</italic></sup> is a noise matrix, in which terms for single voxels (the columns of <bold>E</bold>) are assumed to be independent and identically distributed (i.i.d.) over trials, i.e. <inline-formula><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>. Note that we have not made any assumption about the dependence or independence of these effects in the spatial domain (see <xref rid="s0060" ref-type="sec">Covariance between voxels</xref>).</p>
        <p>We have now <italic>Q</italic> unknown pattern components and <italic>N</italic> unknown noise components that we wish to estimate. Direct estimation is impossible, because we have only <italic>N</italic> observed patterns as data. The novel approach we adopt is to consider the pattern components to be randomly distributed across voxels and to estimate not the pattern components directly but the energy and similarity (variances and covariances) associated with those patterns. Given these variances and covariances, we can then obtain the random-effects estimates of the patterns.</p>
        <p>Thus, we assume that across the <italic>P</italic> voxels, the columns of <bold>U</bold> are distributed normally with mean <bold>a</bold> and variance–covariance matrix <bold>G</bold>.<disp-formula id="fo0010"><label>(2)</label><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>Because each pattern component has its own mean value, the estimates of <bold>a</bold> do not depend on <bold>G</bold>. Thus we can estimate <bold>a</bold> using the pseudo-inverse of <bold>Z</bold> as <inline-formula><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>/</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> and simply subtract <inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>Z</bold><bold>a</bold></textual-form><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">a</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula> from each <inline-formula><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula>. Without a loss of generality, we can therefore assume that the mean-subtracted column vectors <inline-formula><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula> have a normal distribution with mean <bold>0</bold> and variance–covariance:<disp-formula id="fo0015"><label>(3)</label><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula></p>
        <p>This is a random-effects model, in which we have converted the problem of estimating the unknown pattern components into the problem of estimating the unknown variance–covariance matrix <inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>G</bold> ∈ <italic>ℜ</italic><sup><italic>Q</italic>×<italic>Q</italic></sup></textual-form><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℜ</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo>×</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> that underlies the expression of the Q pattern components. The leading diagonal terms of <bold>G</bold> parameterize the overall energy or variance over voxels associated with each component, while the off-diagonal terms encode the similarity among components. Once we have obtained an estimate of <bold>G</bold>, the random pattern components can be estimated using the best-linear-unbiased predictor:<disp-formula id="fo0020"><label>(4)</label><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">U</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="s0020">
        <title>Estimation of <bold>G</bold></title>
        <p>If <bold>G</bold> is unconstrained, our model is the standard random-effects model. In such cases, an Expectation–Maximization (<xref rid="bb0075" ref-type="bibr">Laird et al., 1987</xref>) or Newton–Raphson (<xref rid="bb0080" ref-type="bibr">Lindstrom and Bates, 1988</xref>) algorithm can be used to compute maximum-likelihood or restricted-maximum likelihood estimators for the variance parameters. As we will see in the following, many applications demand certain constraints on <bold>G</bold> that embody structural assumptions about the underlying pattern components. For example, one may want to constrain the variances for all levels of one factor to be the same, or one may want to impose the constraint that some components are uncorrelated (see examples below). Thus, ideally, we should be able to specify an arbitrary set of linear constraints on <bold>G</bold>.</p>
        <p>In estimating the elements of the constrained <bold>G</bold>-matrix, we need to ensure that <bold>G</bold> is a true covariance matrix; i.e. it is positive definite. Here, we solve this problem by expressing <bold>G</bold> as <inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>A</bold><bold>A</bold><sup><italic>T</italic></sup></textual-form><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, and by imposing the linear constraints on <bold>A</bold>, rather than on <bold>G</bold>. This is achieved by constructing <bold>A</bold> as a linear combination of basis matrices. The full EM-algorithm to estimate <bold>A</bold> is presented in the <xref rid="s0085" ref-type="sec">Appendix A</xref>.</p>
      </sec>
    </sec>
    <sec id="s0025">
      <title>Example of a one-factorial design</title>
      <sec id="s0030">
        <title>Effect of noise on similarity analysis</title>
        <p>To give an illustrative example, let us consider a one-factorial experiment using 3 stimuli (<xref rid="f0010" ref-type="fig">Fig. 2</xref>A). The researcher may want to know, which of three pairs of stimuli are represented in a similar way; whether the similarity structure can be captured by one underlying dimension (that the region encodes), and how the similarity structure changes across regions.</p>
        <p>In this one-factorial design with 3 levels, we can think of <inline-formula><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:msubsup></mml:math></inline-formula>,<inline-formula><mml:math id="M13" altimg="si13.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mn>3</mml:mn><mml:mi>r</mml:mi></mml:msubsup></mml:math></inline-formula> as the three (unknown) pattern components encoding each level. Because the three conditions may be represented with different strengths, they may have different variances, with a high variance indicating a stronger response. Furthermore, each pair of pattern components may share a positive or negative covariance; i.e. they may be similar to each other or be partly inverse images of each other. These similarities correspond to the covariances <italic>γ</italic><sub><italic>i</italic>, <italic>j</italic></sub> between two pattern components. Thus in this simple case, our model would take the form:<disp-formula id="fo0025"><label>(5)</label><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>≜</mml:mo><mml:mo>var</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>The similarity of two pattern components is reflected in their true correlation:<disp-formula id="fo0030"><label>(6)</label><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>Because we do not have access to the true (population) correlations, a typical approach is to calculate the mean response patterns <inline-formula><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:math></inline-formula> for each stimulus and look at the sample correlations among them. Because each measurement is corrupted by noise, these sample correlations will be closer to zero than the true correlations. To illustrate this, we simulated an example in which the true patterns for stimuli 1 and 2 are represented independently of each other (<italic>ρ</italic><sub>1, 2</sub> = 0), patterns 1 and 3 are negatively correlated (<italic>ρ</italic><sub>1, 3</sub> = − 0.2), and patterns 2 and 3 are positively correlated (<italic>ρ</italic><sub>2, 3</sub> = 0.8). We simulated <italic>n = 5</italic> trials per condition for <italic>P</italic> = 100 voxels, setting the variance of the patterns to <italic>σ</italic><sub><italic>i</italic></sub><sup>2</sup> = 1, and varying the noise variance <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> between 0.5 and 10. As expected, the sample correlations become smaller (closer to zero) with increasing noise (<xref rid="f0010" ref-type="fig">Fig. 2</xref>B, gray line). Analytically, the expected value of the sample correlation between stimulus <italic>i</italic> and <italic>j</italic> under our model is:<disp-formula id="fo0035"><label>(7)</label><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>Thus, while the rank ordering of the sample correlations is still interpretable, the absolute size of the correlation coefficients is not. This constitutes a problem if one tries to compare the correlations across different regions or individuals. Using the pattern component model (<xref rid="f0010" ref-type="fig">Fig. 2</xref>A, Eq. <xref rid="fo0025" ref-type="disp-formula">(5)</xref>), we are able to estimate the variance–covariance matrix of the hidden pattern components directly. By plugging these estimates into Eq. <xref rid="fo0030" ref-type="disp-formula">(6)</xref>, we can then obtain corrected correlation coefficients. These provide the appropriate summary of relationships among the <italic>Q</italic> condition-specific pattern components, as opposed to the sample correlations, which are based on the fixed effects estimate (i.e. sample mean) for the underlying responses in each condition. The corrected estimates reflect the true size of the correlations for all stimulus pairs, independent of the level of noise (<xref rid="f0010" ref-type="fig">Fig. 2</xref>B, black dashed line). Correlation estimates from different regions can now be compared in a quantitative and meaningful way.</p>
        <p>Furthermore, similarity structures can be analyzed and visualized using multi-dimensional scaling (<xref rid="bb0015" ref-type="bibr">Borg and Groenen, 2005</xref>). Here, one defines a distance metric between each pair of stimuli (here 1 − <italic>ρ</italic>), and attempts to find a space in which the stimuli can be arranged in such a way that their spatial distance best reflects this similarity. In our example, the true similarity structure can be visualized using a single dimension, with stimulus 2 and 3 being grouped together (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>C, true structure). The similarity structure revealed by sample correlations, however, is very sensitive to the level of noise (<xref rid="f0010" ref-type="fig">Fig. 2</xref>C, sample correlations): with high noise, two dimensions are needed to represent the similarities, and the different stimuli appear to be equidistant from each other. Using our corrected estimates, the true one-dimensional structure is restored.</p>
      </sec>
      <sec id="s0035">
        <title>Correcting for a common activation pattern</title>
        <p>In many cases, the measured activation pattern of different stimuli may be highly correlated with each other, because they share a common nonspecific factor. For example, in a visual experiment all stimuli may be preceded by a cue or may be followed by a response, both of which would elicit a distributed activation. We can think about this activation as a pattern component that has variance <italic>σ</italic><sub>c</sub><sup>2</sup> over voxels and that is added to the measured activity pattern of each trial (<xref rid="f0015" ref-type="fig">Fig. 3</xref>A). When simulating data with <italic>σ</italic><sub>c</sub><sup>2</sup> = 4 (all other simulation parameters as before), we indeed see that the sample correlations between all pairs of stimuli become highly positive (light gray line, <xref rid="f0015" ref-type="fig">Fig. 3</xref>B). When attempting to compare correlations from different individuals or regions, this is problematic, as different regions may show this common pattern in varying degrees.</p>
        <p>To address this issue, one could introduce a control condition, which shares the nonspecific factors with all other conditions, but does not have any specific similarity with the conditions of interest. A typical approach would then be to subtract the mean activation pattern of the control condition from each of the condition-specific patterns and to calculate the sample correlation between these control-subtracted patterns. These correlations (dark gray dashed line, <xref rid="f0015" ref-type="fig">Fig. 3</xref>B) indeed correct for some of the positive correlation, bringing the correlation estimates closer to the true values. However, the measures are still biased. As the noise variance increases, the estimated correlations also increase. The reason for this behaviour is the following: The fixed-effects estimate of the common activation pattern (the mean pattern in the control condition) is itself corrupted by measurement noise. By subtracting the same random fluctuation from all the patterns, one introduces an artificial positive correlation between the ensuing residuals.</p>
        <p>Thus, to correct for the common activation, a random-effects estimate of the common pattern is needed. In our pattern component model, we can conceptualize this common factor as a pattern component that is shared by all stimuli (first row of <bold>U</bold>, <xref rid="f0015" ref-type="fig">Fig. 3</xref>A), and that has variance <italic>σ</italic><sub>c</sub><sup>2</sup>. We assume here that this common component is uncorrelated with the pattern components that distinguish between different stimuli:<disp-formula id="fo0040"><label>(8)</label><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>Thus, we define the stimulus-specific pattern components to be variations in activity that are orthogonal to the mean component, not stronger or weaker versions of the mean response. Our algorithm (see <xref rid="s0085" ref-type="sec">Appendix A</xref>) allows us to impose constraints on the variance–covariance matrix <bold>G</bold>, or more accurately, the square root (factor) <bold>A</bold> of this matrix. Thus, instead of explicitly estimating the common activation pattern and then subtracting it from the other patterns, we estimate the similarity structure of the stimuli directly, under the assumption that they share a common source of variance across voxels. The resulting estimates of the correlations correct for noise and the common activation pattern simultaneously (<xref rid="f0015" ref-type="fig">Fig. 3</xref>B). This correction makes it possible to compare the size of the correlations across different regions, even if these regions exhibit a common activation pattern to a different degree or have different levels of noise. Furthermore, the component model restores the true (one-dimensional) multidimensional similarity structure (<xref rid="f0015" ref-type="fig">Fig. 3</xref>C).</p>
        <p>It may not always be possible to find a control condition that contains the nonspecific components and is equally dissimilar to all stimuli of interest. In such a case we can also introduce a common activation pattern into the model without measuring it separately in a control condition. This, however, generates an implicit ambiguity, as a positive correlation between the measured patterns could be explained either by a positive covariance between the stimulus-specific pattern components, or by a high variance of the common pattern component. To resolve this ambiguity, we then need to anchor the similarity scores by assuming that one or multiple pairs of pattern components associated with the stimuli of interest are uncorrelated, thereby introducing the necessary constraint into the <bold>G</bold> matrix. In the following 2-factorial example we will provide an example of such an approach (see also Eq. <xref rid="fo0110" ref-type="disp-formula">(A7)</xref>).</p>
      </sec>
    </sec>
    <sec id="s0040">
      <title>Example using a 2-factorial design</title>
      <sec id="s0045">
        <title>Accessing similarities across conditions</title>
        <p>In this section, we further illustrate the use of our method for a more complex 2-factorial design, and show how the model can be used to test specific hypotheses about the structure of neural representations. In our example, a participant moved or received sensory stimulation to one of the four fingers of the right hand on each trial. Thus, Factor A was the experimental condition (movement vs. stimulation), while Factor B encoded which of the 4 fingers was involved. Overall, there were 8 experimental conditions (<xref rid="bb0130" ref-type="bibr">Wiestler et al., 2009</xref>), each repeated once in 7 imaging runs. In factorial designs like this, we can ask a number of questions: a) Does the region encode information about the finger in the movement and/or stimulation condition? b) Are the patterns evoked by movement of a given finger similar to the patterns evoked by stimulation of the same finger? c) Is this similarity greater in one region than another?</p>
        <p>To answer question (a), we could use a standard multivariate test (e.g. CCA, <xref rid="bb0040" ref-type="bibr">Friston et al., 1996</xref>) or a classification and cross-validation procedure (<xref rid="bb0110" ref-type="bibr">Pereira et al., 2009</xref>); in which we train a classifier on the data from 6 runs, and then test whether the classifier can successfully “predict” from the activation patterns of the 7th run which finger was moved or stimulated. Similar approaches could also be used to answer question (b). Here we could train the classifier on patterns from the movement condition and then test the classifier on the stimulation condition (<xref rid="bb0100" ref-type="bibr">Oosterhof et al., 2010b</xref>). Alternatively, one can use representational-similarity analyses and test if there is a higher correlation between movement and stimulation patterns for the same finger, compared to different fingers. However, to answer question (c) this approach will not suffice: As we have seen, sample correlations are influenced by noise and strength of common activation, which makes direct comparisons across regions impossible.</p>
        <p>To capture this more complex 2-factorial design in the pattern component model, let us first assume that all movement trials share a component (<italic>u</italic><sub><italic>α</italic>[1]</sub>), induced by the task. Similarly, there is an overall pattern component associated with sensory stimulation (<italic>u</italic><sub><italic>α</italic>[2]</sub>). These two components may also share a true covariance (<italic>γ</italic><sub><italic>α</italic></sub>) that reflects common task activity (i.e. seeing the instruction cue). Thus, together these two pattern components encode the intercept and the main effect of condition (movement vs. stimulation). To capture the second factor of the experimental design, we assume that each finger has a specific pattern component associated with it, one for each experimental condition (<italic>u</italic><sub><italic>β</italic>[<italic>c</italic>, 1, … 4]</sub> : <italic>c</italic> ∈ 1, 2). The variance of these components may be different for movement and stimulation conditions (<italic>σ</italic><sub><italic>β</italic>[1]</sub><sup>2</sup> vs. <italic>σ</italic><sub><italic>β</italic>[2]</sub><sup>2</sup>). Because the correlation between finger patterns within a single condition is captured in the strength of the pattern <italic>u</italic><sub><italic>α</italic>[<italic>c</italic>]</sub>, and for patterns of different fingers across condition by <italic>γ</italic><sub><italic>α</italic></sub>, these pattern components are uncorrelated. Only patterns for matching fingers share the additional covariance <italic>γ</italic><sub><italic>β</italic></sub>. It is these parameters that will tell us how much finger-specific variance or information is shared across conditions. In sum, our covariance component model is:<disp-formula id="fo0045"><label>(9)</label><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mtext>var</mml:mtext><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>...</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>...</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>Under this model, the expected value of the sample correlation between the measured patterns for the same finger for the movement and stimulation condition is:<disp-formula id="fo0050"><label>(10)</label><mml:math id="M21" altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mtext>cor</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>Whereas the sample correlation between non-matching fingers would be<disp-formula id="fo0055"><label>(11)</label><mml:math id="M22" altimg="si22.gif" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mtext>cor</mml:mtext><mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>As we can see, these sample correlations are influenced by many factors other than the true similarity <italic>γ</italic><sub><italic>β</italic></sub>. We simulated data with the parameters <italic>σ</italic><sub><italic>β</italic>[1]</sub><sup>2</sup> = <italic>σ</italic><sub><italic>β</italic>[2]</sub><sup>2</sup> = 1, <italic>σ</italic><sub><italic>α</italic>[1]</sub><sup>2</sup> = <italic>σ</italic><sub><italic>α</italic>[2]</sub><sup>2</sup> = 2, <italic>γ</italic><sub><italic>β</italic></sub> = 0.5, and varied the two parameters <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> ∈ {0.5,..., 8} and <italic>γ</italic><sub><italic>α</italic></sub>/<italic>σ</italic><sub><italic>α</italic></sub><sup>2</sup> ∈ {0,..., 0.9}. The sample correlation between matching fingers (<xref rid="f0020" ref-type="fig">Fig. 4</xref>A) was influenced by both of these factors: as the noise-level increased, the correlation dropped. Furthermore, as the true nonspecific correlation (<italic>γ</italic><sub><italic>α</italic></sub>/<italic>σ</italic><sub><italic>α</italic></sub><sup>2</sup>) between the activations increased, so did the sample correlations. This makes it very difficult to compare sample correlations across regions or groups of participants.</p>
        <p>An alternative strategy is to compare the correlations between movement and stimulation of the same finger to the correlations between different fingers. This analysis removes the dependency on <italic>γ</italic><sub><italic>α</italic></sub> (<xref rid="f0020" ref-type="fig">Fig. 4</xref>B). However, the difference between correlations underestimates the true correlation between finger patterns and still depends on the noise level.</p>
        <p>Third, as considered in the one-factorial design, one could estimate the nonspecific condition effect by calculating the mean patterns for all trials of one condition. One could then subtract this pattern from all finger-specific patterns of the same condition and then examine the correlations between the residual patterns. This represents an ad-hoc attempt to decompose the patterns into common and specific components. However, this fixed-effects approach does not recognize that the sample mean of all patterns (common mean) also contains noise. In <xref rid="s0035" ref-type="sec">Correcting for a common activation pattern</xref> we had seen how the subtraction of a pattern estimated from an independent control condition induces an artificial positive correlation between patterns. In this case we would subtract the mean over the conditions and thereby induce an artificial negative correlation between the residuals. The correlation between patterns of different conditions therefore decreases with increasing noise (<xref rid="f0020" ref-type="fig">Fig. 4</xref>C), which again makes it impossible to compare correlations from different regions.</p>
        <p>Thus, as we have seen before, there is no simple ‘fix’ for sample correlations that would enable them to be compared meaningfully. Our model solves this problem by estimating explicitly the different covariance components in Eq. <xref rid="fo0045" ref-type="disp-formula">(9)</xref>. By doing this, we obtain the corrected correlation <italic>γ</italic><sub><italic>β</italic></sub>/<italic>σ</italic><sub><italic>β</italic>[1]</sub><italic>σ</italic><sub><italic>β</italic>[2]</sub>. This estimate (see <xref rid="f0020" ref-type="fig">Fig. 4</xref>D) is stable across variations in the amplitude of noise or the nonspecific component. As such, this corrected correlation provides a robust measure of pattern similarity that can be compared meaningfully across different regions or participants.</p>
      </sec>
      <sec id="s0050">
        <title>The influence of voxel selection</title>
        <p>A further factor that influences the sample correlation is the composition of the region's voxels. While our model assumes that the pattern components will have an average variance across different voxels, it is very unlikely to pick voxels in which the patterns are represented homogenously. If the region that we pick contains informative voxels for half, and non-informative voxels for the other half, the correlation will be lower than when the region contains mostly informative voxels.</p>
        <p>We tested whether the pattern component model can deal with this problem. For this simulation, we used the two-factorial 2x4 (movement vs. stimulation) design described in the previous section. For one portion of the voxels we set the simulation values to <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> = 4, <italic>σ</italic><sub><italic>α</italic>[1]</sub><sup>2</sup> = <italic>σ</italic><sub><italic>α</italic>[2]</sub><sup>2</sup> = 2 and <italic>σ</italic><sub><italic>β</italic>[1]</sub><sup>2</sup> = <italic>σ</italic><sub><italic>β</italic>[2]</sub><sup>2</sup> = 1, <italic>γ</italic><sub><italic>α</italic></sub> = 0, and <italic>γ</italic><sub><italic>β</italic></sub> = 0.5. For a subset of voxels (varying between 0% and 75%), we set <italic>σ</italic><sub><italic>β</italic></sub><sup>2</sup> to zero; i.e., these voxels did not contain information about the finger involved.</p>
        <p>As can be seen from <xref rid="f0025" ref-type="fig">Fig. 5</xref>, increasing the number of non-informative voxels in the region of interest has the same effect as increasing noise: The mean-corrected sample correlation or the difference between sample correlations declines with the number of informative voxels. In contrast, the correlation estimate from the covariance component model <italic>r</italic><sub><italic>β</italic></sub> = <italic>γ</italic><sub><italic>β</italic></sub>/<italic>σ</italic><sub><italic>β</italic>1</sub><italic>σ</italic><sub><italic>β</italic>2</sub> retains its unbiased behaviour. This is because both the estimates for <italic>σ</italic><sub><italic>β</italic></sub><sup>2</sup> and <italic>γ</italic><sub><italic>β</italic></sub> decline simultaneously with the number of informative voxels.</p>
      </sec>
      <sec id="s0055">
        <title>Similarity of representations across conditions: real data example</title>
        <p>Having established the robustness of our approach, we now turn to a real data example. The design of the experiment (<xref rid="bb0130" ref-type="bibr">Wiestler et al., 2009</xref>) is described in the previous section. The main focus of this experiment was to compare the similarity of sensory and motor representations of fingers in the cerebellum (lobule V) and the neocortex (primary somatosensory cortex, S1, and primary motor cortex, M1). <xref rid="f0030" ref-type="fig">Fig. 6</xref> shows the results of a traditional representational similarity analysis. Here, we calculated the sample correlations between the mean patterns for each finger (digits 1, 2, 3, and 5) and condition (sense vs. move) to obtain an 8 × 8 correlation matrix (<xref rid="f0030" ref-type="fig">Fig. 6</xref>A). The patterns for moving a specific finger correlated with the pattern for stimulation of the same finger (1). To determine whether this similarity was finger-specific, or whether it was caused by a nonspecific similarity between motor and sensory patterns, we compared these correlations to those calculated across conditions for the six possible pairings of different fingers (2). An interesting effect was found in the between-region comparison (<xref rid="f0030" ref-type="fig">Fig. 6</xref>B): in the neocortex the correlation between patterns of movement and sensory stimulation for the same finger was higher than for different fingers, while in the cerebellum no such difference was found. This result, however, needs to be considered with caution, because, as seen above, differences between sample correlations from different regions cannot be compared. Because the correlations in the cerebellum were roughly half the size compared to those in the neocortex, it seems likely that the variance of the noise component was higher here.</p>
        <p>Before calculating corrected correlations using our method, we need to consider a further detail: When we looked at the correlations between different fingers within the sensory and movement conditions (<xref rid="f0030" ref-type="fig">Fig. 6</xref>C), we found these correlations to be very high in both regions. In the experiment we blocked the conditions to simplify instructions to the participant. In the first half of each run, participants performed trials in one condition, followed by the other condition in the second half, interrupted by a relatively short resting period. Because of this, the estimation errors of the regression coefficients will be correlated within each run and condition, while they should be uncorrelated across runs or conditions. Indeed, when we calculated the average sample correlations between the patterns of each run (different fingers, same condition), we found that these correlations were substantially higher than the same correlations calculated across runs (<xref rid="f0030" ref-type="fig">Fig. 6</xref>D). This finding shows that correlation between patterns can also be increased by noise due to conditional dependencies among estimators from a single run, and underscores the importance of performing cross-validation across different imaging runs.</p>
        <p>The decomposition method offers an elegant way to control for all these possible influences on the size of the correlation coefficients in a single modelling framework. In addition to noise (<italic>ε</italic>), condition (<italic>u</italic><sub><italic>α</italic>[1]</sub>,<italic>u</italic><sub><italic>α</italic>[2]</sub>), and finger (<italic>u</italic><sub><italic>β</italic>[1]</sub>,<italic>u</italic><sub><italic>β</italic>[2]</sub>) effects (Eq. <xref rid="fo0040" ref-type="disp-formula">(8)</xref>), we also added a run effect. This pattern component was common to all trials of one condition within the same run, but uncorrelated across runs. Thus, we allowed separate pattern components for each run (indexed by <italic>i</italic>, <italic>u</italic><sub><italic>δ</italic>[1, <italic>i</italic>]</sub>,<italic>u</italic><sub><italic>δ</italic>[2, <italic>i</italic>]</sub>), and estimated separate variances for the two conditions (<italic>σ</italic><sub><italic>δ</italic>[1]</sub><sup>2</sup> vs. <italic>σ</italic><sub><italic>δ</italic>[2]</sub><sup>2</sup>) and their covariance (<italic>γ</italic><sub><italic>δ</italic></sub>) within a run.</p>
        <p><xref rid="f0035" ref-type="fig">Fig. 7</xref> shows the decomposition into the different components. The noise variance (<xref rid="f0035" ref-type="fig">Fig. 7</xref>A) was indeed substantially higher in the cerebellum compared to the neocortex (by a factor of 2.5), which emphasizes the importance of accounting for noise when comparing correlations. The run effect (<xref rid="f0035" ref-type="fig">Fig. 7</xref>B), caused by correlated estimation errors, showed a similar difference between cerebellum and neocortex, consistent with the idea that the covariance was induced by noise in the estimation of the common resting baseline. The correlation coefficient (<italic>γ</italic><sub><italic>δ</italic></sub>/<italic>σ</italic><sub><italic>δ</italic>[1]</sub><italic>σ</italic><sub><italic>δ</italic>[2]</sub>, <xref rid="f0035" ref-type="fig">Fig. 7</xref>C) also showed that the run effect was uncorrelated across conditions. This makes sense as the two conditions were acquired in two different halves of each run, making their estimation nearly independent.</p>
        <p>Having accounted for the noise components, we can now investigate the condition effect (<italic>σ</italic><sub><italic>α</italic></sub>, <xref rid="f0035" ref-type="fig">Fig. 7</xref>D, E), which is common to all fingers. These pattern components were much stronger in the movement condition, consistent with the observation that the BOLD signal changes much more during the movement compared to the stimulation condition. In contrast, the variance of the components that were unique to each finger (<italic>σ</italic><sub><italic>β</italic></sub>, <xref rid="f0035" ref-type="fig">Fig. 7</xref>F) was similar across conditions and regions.</p>
        <p>Of key interest, however, are the corrected correlation coefficients (similarity indices) between the motor and sensory patterns for the same finger (<italic>γ</italic><sub><italic>δ</italic></sub>/<italic>σ</italic><sub><italic>δ</italic>[1]</sub><italic>σ</italic><sub><italic>δ</italic>[2]</sub>, <xref rid="f0035" ref-type="fig">Fig. 7</xref>G). These indices are significantly different between cerebellum and neocortical regions (paired <italic>t</italic>-test for <italic>N</italic> = 7 participants, <italic>t</italic>(6) = −4.09, <italic>p</italic> = 0.006), arguing strongly that the difference in correlation structure observed in <xref rid="f0030" ref-type="fig">Fig. 6</xref>B was caused by a difference in the neural representation in these regions, and not by an effect of noise, voxel selection, or covariance in estimation (<xref rid="bb0130" ref-type="bibr">Wiestler et al., 2009</xref>).</p>
      </sec>
    </sec>
    <sec id="s0060">
      <title>Covariance between voxels</title>
      <p>So far, we have looked at the covariance structure of the data over trials, ignoring the possible spatial dependence of voxels. Even in unsmoothed fMRI data, however, spatial correlations clearly exist, and may contain valuable information about the spatial structure of the underlying representations. Although the full integration of spatial covariances into the model is beyond the scope of this paper, we sketch out here how such correlations would be incorporated. We will then test, with simulated data, how spatial correlations influence our estimates of variance–covariance structure of the hidden pattern components. Finally, we will suggest a simple method to estimate the spatial smoothness of each of the pattern components, allowing some insight into their spatial structure.</p>
      <p>To include spatial smoothness into our model, we need to specify the correlation structure of the matrix <bold>U</bold> not only between the hidden patterns, but also between voxels or features. We can do this by specifying the variance of a row of <bold>U</bold> to be <inline-formula><mml:math id="M23" altimg="si23.gif" overflow="scroll"><mml:mrow><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:mi>∑</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where ∑ is a <italic>PxP</italic> covariance matrix that determines the distribution of the pattern component across voxels, and <italic>g</italic><sub><italic>i</italic>, <italic>i</italic></sub> is the <italic>i</italic>-th element of the diagonal of <bold>G</bold>, indicating the variance of this component. To avoid redundancy, we assume the mean of the diagonal elements of <italic>Σ</italic>is 1.</p>
      <p>Now we have to deal both with covariance across trials and covariance across voxels at the same time. To be able to write the full covariance structure, we need to rearrange our data matrix <bold>Y</bold> into a <italic>NxP</italic> vector by stacking the rows (via the vec() operator). Similarly we stack the rows of <bold>U</bold>, such that we obtain an <italic>NxQ</italic> vector. The new variance–covariance matrix <inline-formula><mml:math id="M24" altimg="si24.gif" overflow="scroll"><mml:mstyle mathvariant="bold"><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:math></inline-formula> (now a (NxQ)x(NxQ) matrix) can be written using the Kronecker tensor product: <inline-formula><mml:math id="M25" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mtext>vec</mml:mtext><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">U</mml:mi></mml:mstyle></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>⊗</mml:mo><mml:mi>∑</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p>We now can allow each pattern component to have its own spatial covariance structure. For example, we may hypothesize that the activation pattern elicited by the overall task of moving a finger (<inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>u</bold><sub><italic>α</italic></sub></textual-form><mml:math id="M26" altimg="si26.gif" overflow="scroll"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>α</mml:mi></mml:msub></mml:math></alternatives></inline-formula> in above example) is relatively smooth, while the patterns specific to the individual fingers (<inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>u</bold><sub><italic>β</italic></sub></textual-form><mml:math id="M27" altimg="si27.gif" overflow="scroll"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>β</mml:mi></mml:msub></mml:math></alternatives></inline-formula>) maybe more fractionated. Thus, we can partition the rows of <bold>U</bold> into <italic>J</italic> sets, each of which is associated with its own spatial covariance kernel <italic>Σ</italic><sub><italic>j</italic></sub>. Here, we assume that these subsets correspond to different diagonal blocks of <bold>G</bold> (<bold>G</bold><sub><bold>1</bold></sub><bold>, G</bold><sub><bold>2</bold></sub>, …). Under this formalism, the covariance matrix becomes:<disp-formula id="fo0060"><label>(12)</label><mml:math id="M28" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:mtext>var</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mtext>vec</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mstyle><mml:mi mathvariant="bold">U</mml:mi></mml:mstyle><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>∑</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>∑</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
      <p>Finally we posit that the noise, which is independent over the trials, has its own spatial covariance matrix ∑<sub><italic>ε</italic></sub>.<disp-formula id="fo0065"><label>(13)</label><alternatives><textual-form specific-use="jats-markup">var(vec(<bold>E</bold>)) = <italic>I</italic><sub><italic>N</italic></sub> ⊗ <italic>∑</italic><sub><italic>ε</italic></sub></textual-form><mml:math id="M29" altimg="si29.gif" overflow="scroll"><mml:mrow><mml:mtext>var</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mtext>vec</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mstyle><mml:mi mathvariant="bold">E</mml:mi></mml:mstyle><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>∑</mml:mi><mml:mi>ε</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
      <sec id="s0065">
        <title>Influence on the estimation of <bold>G</bold></title>
        <p>If different spatial smoothness for different components are a reality – and all our experience so far indicates that this is the case – then we have to first worry about how this would influence the estimation of the elements of <bold>G</bold>. We expected a priori that smoothness should not bias the estimation of variance, but only its precision (the degrees of freedom of the variance estimator). To test this assumption we simulated data using the 4 × 2 design described in <xref rid="s0045" ref-type="sec">Accessing similarities across conditions</xref>. We then introduced a spatial smoothness between neighbouring voxels that decayed exponentially with the square of the distance <italic>δ</italic> between two voxels.<disp-formula id="fo0070"><label>(14)</label><mml:math id="M30" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:mtext>corr</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mi>x</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where <italic>s</italic><sub><italic>i</italic></sub> indicates the standard deviation of the spatial autocorrelation function. If <italic>s</italic> is small, neighbouring voxels will be relatively independent. The smoothness can also be expressed as the FWHM of the Gaussian smoothing kernel that – applied to spatially independent data – would give rise to the same spatial autocorrelation function. The SD of this kernel is <inline-formula><mml:math id="M31" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msqrt><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> , and its FWHM can be calculated as:<disp-formula id="fo0075"><label>(15)</label><mml:math id="M32" altimg="si32.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mtext>FWHM</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mtext>log</mml:mtext><mml:mfenced open="(" close=")"><mml:mn>2</mml:mn></mml:mfenced></mml:mrow></mml:msqrt><mml:mspace width="0.12em"/><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p>We used different spatial kernels for the overall condition effect (<italic>α</italic>), the effect of finger (<italic>β</italic>) and the noise patterns (<italic>ε</italic>). We simulated a sphere of 160 voxels (3.5 voxels radius) with the parameter values <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> = 3, <italic>σ</italic><sub><italic>α</italic>1</sub><sup>2</sup> = <italic>σ</italic><sub><italic>α</italic>2</sub><sup>2</sup> = 1 and <italic>σ</italic><sub><italic>β</italic>1</sub><sup>2</sup> = <italic>σ</italic><sub><italic>β</italic>2</sub><sup>2</sup> = 1, <italic>γ</italic><sub><italic>α</italic></sub> = 0.3, <italic>γ</italic><sub><italic>β</italic></sub> = 0.5, and varied the spatial kernels, with <italic>s</italic><sub><italic>β</italic></sub> ∈ {0, 1, 2}, and <italic>s</italic><sub><italic>ε</italic></sub><sup>2</sup> ∈ {0, 1, 2}, while leaving <italic>s</italic><sub><italic>α</italic></sub> = 1.</p>
        <p>The results of this simulation are shown in <xref rid="f0040" ref-type="fig">Fig. 8</xref>. The estimates of the variances (left column) and the correlation (middle column) for the two factors and for the error term remain relatively stable. Only the variance estimates for the weakest effect (<italic>σ</italic><sub><italic>β</italic></sub><sup>2</sup>) are biased downward, when the FWHM (<italic>s</italic> = 2 corresponds to a FWHM of 3.33 voxels) approaches the radius of the search region. However, for a search region with a diameter of 7 voxels and FWHM below 3 voxels, the estimates remain reassuringly stable.</p>
      </sec>
      <sec id="s0070">
        <title>Estimating the width of covariance kernels</title>
        <p>Would covariance partitioning allow us to estimate the width of the covariance kernel for the experimental factors in question? Such an estimate would relate to the size of the neural clusters that show similar BOLD responses for the component in question. For example, nonspecific activations related to the task may be relatively smooth and cover large regions, while the pattern components distinguishing individual fingers may be much more fine-grained. So can we recover this spatial information for each component?</p>
        <p>The Kronecker form of the generative model (Eqs. <xref rid="fo0060 fo0065" ref-type="disp-formula">(12) and (13)</xref>) makes its inversion very slow. Alternatively one can employ an approximate two-step procedure, by first estimating the variance–covariance structure among components, ignoring any spatial dependence, and then obtaining a simple estimate for the spatial covariances, based on the current estimates of the hidden patterns, <bold>U</bold> (from Eq. <xref rid="fo0020" ref-type="disp-formula">(4)</xref>). To do this, we calculated the sample autocorrelation function (over voxels) using the appropriate rows of <bold>U</bold> (within all levels of a particular factor). To summarize these empirical estimates, we then determined <italic>s</italic> by fitting an exponential kernel (Eq. <xref rid="fo0070" ref-type="disp-formula">(14)</xref>).</p>
        <p>The resulting estimates are shown in the third column for <xref rid="f0040" ref-type="fig">Fig. 8</xref> for the simulated data above. Whereas the estimates for <italic>s</italic><sub><italic>α</italic></sub> and <italic>s</italic><sub><italic>ε</italic></sub> are relatively near to the true values (indicated by lines), the estimates for the weakest effect, <italic>β</italic>, are somewhat biased by the other values. First, for true values of <italic>s</italic><sub><italic>β</italic></sub> of 0 and 2, the estimates are biased towards the value of <italic>s</italic><sub><italic>α</italic></sub><sup>2</sup>(1). Furthermore, the spatial smoothness of the noise effect also influences the estimates.</p>
        <p>These biases reflect the fact that simply estimating the sample autocorrelation function provides suboptimal estimates. However, optimum estimators are rather difficult to obtain. We would need to specify our Gaussian process model in terms of vectorised responses (as above), because the covariance structure cannot be factorized into spatial and non-spatial (experimental) factors. This somewhat destroys the efficiency and utility of Gaussian process modelling of multivariate responses. Our simulation, however, demonstrates that even with our approximate method, we can obtain an estimate of the spatial smoothness for each component.</p>
      </sec>
      <sec id="s0075">
        <title>Estimating the width of covariance kernels: real data example</title>
        <p>To illustrate the utility of this method, we applied it to the data described in <xref rid="s0055" ref-type="sec">Similarity of representations across conditions: real data example</xref>. For primary sensory and motor cortex, and for the hand area in lobule V, we decomposed the covariance kernels separately for the condition, finger, run and noise effect. Based on the final estimate of <bold>U</bold>, we calculated the autocorrelation function for over 11 spatial bins, ranging from 0.1–2.5 mm (directly neighbouring), 2.5–3.6 mm (diagonally neighbouring), up to a total distance of 23.8 mm. To summarize the autocorrelation functions, we fitted a squared-exponential kernel (Eq. <xref rid="fo0070" ref-type="disp-formula">(14)</xref>) to the sample autocorrelation functions.</p>
        <p>The autocorrelation functions for the noise component (<xref rid="f0045" ref-type="fig">Fig. 9</xref>A) and for the run component (<xref rid="f0045" ref-type="fig">Fig. 9</xref>B) were very similar, with an average FWHM of 2.17 mm for the cerebellum and 2.9 mm for neocortical regions, <italic>t</italic>(6) = 6.43, <italic>p</italic> = .001. The similarity of the spatial structure of these two components agrees with the hypothesis that both result from similar noise processes. The results also show that noise has a spatial smoothness roughly one voxel (2 mm).</p>
        <p>In contrast, the effects for condition (<xref rid="f0045" ref-type="fig">Fig. 9</xref>C) and finger (<xref rid="f0045" ref-type="fig">Fig. 9</xref>D) have a significantly greater smoothness, both for lobules V, <italic>t</italic>(6) = 3.87, <italic>p</italic> = .008, as well as for the two cortical areas; both <italic>t</italic>(6) &gt; 4.46, <italic>p</italic> = .004. This indicates that the spatial scales of different pattern components can be different and that our method can (albeit imperfectly) detect these differences.</p>
        <p>Interestingly, we found a difference in the estimated size of the finger representation: We estimated the FWHM for S1 to be 5.1 mm, and for M1 to be 4.1 mm a significant difference, <italic>t</italic>(6) = 4.34, <italic>p</italic> = .027. Note that in the other components, no differences were found between these two regions. In the cerebellum, the representation was smaller again with a FWHM of 2.3 mm, <italic>t</italic>(6) = 5.83 <italic>p</italic> = .001. Thus, these results are consistent with the known characteristics of somatosensory representations in the neocortex and cerebellum (see <xref rid="bb0130" ref-type="bibr">Wiestler et al., 2009</xref>).</p>
      </sec>
    </sec>
    <sec id="s0080">
      <title>Outlook and conclusion</title>
      <p>The current algorithm and formulation furnishes estimates of the true similarity of patterns of distributed responses for subsequent analysis. We have focused here on correlation coefficients as similarity measures. The same covariance estimates could also be used to provide corrected estimates for the Euclidian distance between patterns. Technically, the innovative step presented in this paper is to parameterize <bold>G</bold> as <bold>AA</bold><sup><bold>T</bold></sup>, which renders the problem linear in the hyper-parameters (see also <xref rid="bb0135" ref-type="bibr">Wipf and Nagarajan, 2009</xref>).</p>
      <p>We anticipate that this approach could be extended in two directions. First, our model could be used to compare different covariance models using the marginal likelihood <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<bold>Y</bold>|<italic>m</italic>)</textual-form><mml:math id="M33" altimg="si33.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>. For this we would have to impose priors on the free parameters, effectively changing the EM-scheme into Variational Bayes. Imposing priors may also address a problem of stability in the current formulation, in that the variance of the normalized correlation coefficients (Eq. <xref rid="fo0030" ref-type="disp-formula">(6)</xref> becomes large, as the variance of the pattern <italic>σ</italic><sup>2</sup> becomes small. In the current approach, the user needs to ensure that the variances are sufficiently large, and use a simpler model when the region does not encode the factor in question. The use of priors (and marginal likelihoods or model evidence) would enable us to use automatic relevance detection to automatically drop terms from the model that do not help to explain the data.</p>
      <p>A second extension is to include and explicitly estimate the spatial parameters of the underlying patterns. This would unify this approach with a multivariate Bayesian approach to pattern analysis, in which only the correlation structure between voxels, but not between trials or conditions, is parameterized (<xref rid="bb0045" ref-type="bibr">Friston et al., 2008</xref>).</p>
      <p>In its current implementation, our algorithm provides a concise way of estimating the similarity of multivariate patterns, and enables researchers to compare these measures directly between different regions and brains.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bb0005">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Berry</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Heggernes</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Simonet</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <chapter-title>The minimum degree heuristic and the minimal triangulation process</chapter-title>
          <source>Graph-Theoretic Concepts in Computer Science</source>
          <year>2003</year>
          <publisher-name>Springer</publisher-name>
          <fpage>58</fpage>
          <lpage>70</lpage>
        </element-citation>
      </ref>
      <ref id="bb0010">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Berry</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Krueger</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Simonet</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Maximal label search algorithms to compute perfect and minimal elimination orderings</article-title>
          <source>SIAM J. Discrete Math.</source>
          <volume>23</volume>
          <year>2009</year>
          <fpage>428</fpage>
          <lpage>446</lpage>
        </element-citation>
      </ref>
      <ref id="bb0015">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Borg</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Groenen</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <chapter-title>Modern Multidimensional Scaling: Theory and Applications</chapter-title>
          <edition>2nd ed.</edition>
          <year>2005</year>
          <publisher-name>Springer-Verlag</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0020">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friman</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Cedefamn</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lundberg</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Borga</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Knutsson</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Detection of neural activity in functional MRI using canonical correlation analysis</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>45</volume>
          <year>2001</year>
          <fpage>323</fpage>
          <lpage>330</lpage>
          <pub-id pub-id-type="pmid">11180440</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0025">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <chapter-title>SPM package: spm_reml_sc</chapter-title>
          <year>2008</year>
          <publisher-name>London</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0030">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Modalities, modes, and models in functional neuroimaging</article-title>
          <source>Science</source>
          <volume>326</volume>
          <year>2009</year>
          <fpage>399</fpage>
          <lpage>403</lpage>
          <pub-id pub-id-type="pmid">19833961</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0035">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Grasby</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of fMRI time-series revisited</article-title>
          <source>Neuroimage</source>
          <volume>2</volume>
          <year>1995</year>
          <fpage>45</fpage>
          <lpage>53</lpage>
          <pub-id pub-id-type="pmid">9343589</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0040">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
          </person-group>
          <article-title>A multivariate analysis of PET activation studies</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>4</volume>
          <year>1996</year>
          <fpage>140</fpage>
          <lpage>151</lpage>
          <pub-id pub-id-type="pmid">20408193</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0045">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Mourao-Miranda</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hulme</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian decoding of brain images</article-title>
          <source>Neuroimage</source>
          <volume>39</volume>
          <year>2008</year>
          <fpage>181</fpage>
          <lpage>205</lpage>
          <pub-id pub-id-type="pmid">17919928</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0050">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
            <name>
              <surname>Gobbini</surname>
              <given-names>M.I.</given-names>
            </name>
            <name>
              <surname>Furey</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Ishai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schouten</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Pietrini</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>
          <source>Science</source>
          <volume>293</volume>
          <year>2001</year>
          <fpage>2425</fpage>
          <lpage>2430</lpage>
          <pub-id pub-id-type="pmid">11577229</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex</article-title>
          <source>Nat. Neurosci.</source>
          <volume>8</volume>
          <year>2005</year>
          <fpage>686</fpage>
          <lpage>691</lpage>
          <pub-id pub-id-type="pmid">15852013</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0060">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Predicting the stream of consciousness from activity in human visual cortex</article-title>
          <source>Curr. Biol.</source>
          <volume>15</volume>
          <year>2005</year>
          <fpage>1301</fpage>
          <lpage>1307</lpage>
          <pub-id pub-id-type="pmid">16051174</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0065">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kriegeskorte</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bandettini</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Information-based functional brain mapping</article-title>
          <source>Proc. Natl Acad. Sci. USA</source>
          <volume>103</volume>
          <year>2006</year>
          <fpage>3863</fpage>
          <lpage>3868</lpage>
          <pub-id pub-id-type="pmid">16537458</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0070">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kriegeskorte</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mur</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bandettini</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>
          <source>Front Syst. Neurosci.</source>
          <volume>2</volume>
          <year>2008</year>
          <fpage>4</fpage>
          <pub-id pub-id-type="pmid">19104670</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0075">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laird</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lange</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Stram</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Maximum liklihood computations with repeated measures: application of the EM algorithm</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>82</volume>
          <year>1987</year>
          <fpage>97</fpage>
          <lpage>105</lpage>
        </element-citation>
      </ref>
      <ref id="bb0080">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lindstrom</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Bates</surname>
              <given-names>M.B.</given-names>
            </name>
          </person-group>
          <article-title>Newton–Raphson and EM algorithms for linear mixed-effects models for repeated-measures data</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>83</volume>
          <year>1988</year>
          <fpage>1014</fpage>
          <lpage>1022</lpage>
        </element-citation>
      </ref>
      <ref id="bb0145">
        <mixed-citation publication-type="other">McLachlan, G.J., Krishnan, T., 1997. The EM Algorithm and Extensions. Wiley, New York.</mixed-citation>
      </ref>
      <ref id="bb0085">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Misaki</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bandettini</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Kriegeskorte</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>
          <source>Neuroimage</source>
          <volume>53</volume>
          <year>2010</year>
          <fpage>103</fpage>
          <lpage>118</lpage>
          <pub-id pub-id-type="pmid">20580933</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0090">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Norman</surname>
              <given-names>K.A.</given-names>
            </name>
            <name>
              <surname>Polyn</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Detre</surname>
              <given-names>G.J.</given-names>
            </name>
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
          </person-group>
          <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>
          <source>Trends Cogn. Sci.</source>
          <volume>10</volume>
          <year>2006</year>
          <fpage>424</fpage>
          <lpage>430</lpage>
          <pub-id pub-id-type="pmid">16899397</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0095">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Oosterhof</surname>
              <given-names>N.N.</given-names>
            </name>
            <name>
              <surname>Wiestler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Downing</surname>
              <given-names>P.E.</given-names>
            </name>
            <name>
              <surname>Diedrichsen</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>A comparison of volume-based and surface-based multi-voxel pattern analysis</chapter-title>
          <year>2010</year>
          <publisher-name>Neuroimage</publisher-name>
          <comment>(Electronic publication ahead of print)</comment>
        </element-citation>
      </ref>
      <ref id="bb0100">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Oosterhof</surname>
              <given-names>N.N.</given-names>
            </name>
            <name>
              <surname>Wiggett</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Diedrichsen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Tipper</surname>
              <given-names>S.P.</given-names>
            </name>
            <name>
              <surname>Downing</surname>
              <given-names>P.E.</given-names>
            </name>
          </person-group>
          <chapter-title>Surface-based information mapping reveals crossmodal vision-action representations in human parietal and occipitotemporal cortex</chapter-title>
          <year>2010</year>
          <publisher-name>J. Neurophysiol.</publisher-name>
          <comment>(Electronic publication ahead of print)</comment>
        </element-citation>
      </ref>
      <ref id="bb0105">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Parter</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>The use of linear graphs in Gauss elimination</article-title>
          <source>SIAM Rev.</source>
          <volume>3</volume>
          <year>1961</year>
          <fpage>119</fpage>
          <lpage>130</lpage>
        </element-citation>
      </ref>
      <ref id="bb0110">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pereira</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Mitchell</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Botvinick</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title>
          <source>Neuroimage</source>
          <volume>45</volume>
          <year>2009</year>
          <fpage>S199</fpage>
          <lpage>S209</lpage>
          <pub-id pub-id-type="pmid">19070668</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0115">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pinheiro</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Bates</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>Unconstrained parametrizations for variance–covariance matrices</article-title>
          <source>Comput. Stat.</source>
          <volume>6</volume>
          <year>1995</year>
          <fpage>289</fpage>
          <lpage>296</lpage>
        </element-citation>
      </ref>
      <ref id="bb0120">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rasmussen</surname>
              <given-names>C.E.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>C.K.I.</given-names>
            </name>
          </person-group>
          <chapter-title>Gaussian Processes for Machine Learning</chapter-title>
          <year>2006</year>
          <publisher-name>The MIT Press</publisher-name>
          <publisher-loc>Cambridge, Massachusetts</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0125">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rose</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Triangulated graphs and the elimination process</article-title>
          <source>J. Math. Anal. Appl.</source>
          <volume>23</volume>
          <year>1970</year>
          <fpage>597</fpage>
          <lpage>609</lpage>
        </element-citation>
      </ref>
      <ref id="bb0130">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wiestler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>McGonigle</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Diedrichsen</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>Sensory and motor representations of single digits in the human cerebellum</chapter-title>
          <year>2009</year>
          <publisher-name>Society for Neuroscience</publisher-name>
          <publisher-loc>Chicago</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wipf</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Nagarajan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A unified Bayesian framework for MEG/EEG source imaging</article-title>
          <source>Neuroimage</source>
          <volume>44</volume>
          <year>2009</year>
          <fpage>947</fpage>
          <lpage>966</lpage>
          <pub-id pub-id-type="pmid">18602278</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0140">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>C.H.</given-names>
            </name>
            <name>
              <surname>Aston</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Petre</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Duncan</surname>
              <given-names>G.H.</given-names>
            </name>
            <name>
              <surname>Morales</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>A general statistical analysis for fMRI data</article-title>
          <source>Neuroimage</source>
          <volume>15</volume>
          <year>2002</year>
          <fpage>1</fpage>
          <lpage>15</lpage>
          <pub-id pub-id-type="pmid">11771969</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="s0085">
      <label>Appendix A</label>
      <title>EM-Algorithm to estimate covariance matrices with linear constraints on its factors</title>
      <sec id="s0090">
        <title>The model</title>
        <p>The algorithm presented here provides inference on covariance component models, in which linear constraints are placed on factors of the variance–covariance matrix. Each of the <italic>N</italic> observations (referring to time points or trials) <inline-formula><mml:math id="M34" altimg="si34.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:math></inline-formula> is a <italic>Px1</italic> vector. The data therefore comprise an <italic>NxP</italic> matrix (see <xref rid="f0005" ref-type="fig">Fig. 1</xref>). We model observed covariances as a mixture of <italic>Q</italic> hidden pattern components encoded in a <italic>QxP</italic> pattern matrix <bold>U</bold>. The <italic>P</italic> columns (<inline-formula><mml:math id="M35" altimg="si35.gif" overflow="scroll"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula>) of <bold>U</bold> are randomly distributed over voxels:<disp-formula id="fo0080"><label>(A1)</label><mml:math id="M36" altimg="si36.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>This is a standard random effects model. If the variance–covariance matrix <bold>G</bold> is completely unconstrained, the EM algorithm proposed by <xref rid="bb0075" ref-type="bibr">Laird et al. (1987)</xref> would provide an efficient solution for estimation. However, in many cases we would like to estimate the variances or covariances under certain structural assumptions. For example, we may not be interested in the variance attributed to each stimulus, but may be interested in the average variance that encodes a certain factor. That is, we would like to assume that the variance of all levels within that factor is the same. This could be done by expressing <bold>G</bold> as a linear mixture of <italic>K</italic> components, each weighted by an unknown parameter <italic>θ</italic><sub><italic>k</italic></sub><disp-formula id="fo0085"><label>(A2)</label><mml:math id="M37" altimg="si37.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p>
        <p>Such parameterizations can be estimated using a Newton-Raphson algorithm (<xref rid="bb0025" ref-type="bibr">Friston, 2008</xref>). The main problem in such a scheme, however, is to ensure that <bold>G</bold> remains positive-definite. If the components of <bold>G</bold> are diagonal, a positive-definite <bold>G</bold> can be enforced by estimating the log of <italic>θ</italic>, thereby ensuring <italic>θ</italic> &gt; 0. For situations in which we also wish to estimate covariances, such a scheme sometimes fails, because it does not enforce the Cauchy-Schwarz inequality <inline-formula><mml:math id="M38" altimg="si38.gif" overflow="scroll"><mml:mrow><mml:mfenced open="|" close="|"><mml:msub><mml:mi>γ</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mfenced><mml:mo>&lt;</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. One can try to address this by rewriting <bold>Z</bold> and <bold>G</bold>, such that the covariances between conditions are captured by the summation over separate, independent factors. Because <bold>G</bold> now again has a diagonal form, positive-definiteness can easily be ensured. This formulation, however, restricts the covariance estimate by <italic>γ</italic> &gt; 0 and <italic>γ</italic> &lt; min(<italic>σ</italic><sub>1</sub><sup>2</sup>, <italic>σ</italic><sub>2</sub><sup>2</sup>), rather than enforcing the more flexible Cauchy–Schwarz inequality.</p>
        <p>To solve this problem, we impose constraints on the square root (factor) of the covariance matrix and estimate its parameters.<disp-formula id="fo0090"><label>(A3)</label><mml:math id="M39" altimg="si39.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The (matrix) squaring ensures that <bold>G</bold> is positive semi-definite for any <bold>A</bold>, and the addition of positive <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> then ensures the overall covariance <inline-formula><mml:math id="M40" altimg="si40.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula> is positive definite.</p>
      </sec>
      <sec id="s0095">
        <title>Linear constraints</title>
        <p>Eq. <xref rid="fo0090" ref-type="disp-formula">(A3)</xref> allows us to enforce independence constraints (elements of <bold>G</bold> that need to be 0) and equality constraints (elements of <bold>G</bold> that need to be equal). In general, many structurally equivalent parameterizations of A for each desired structure of <bold>G</bold> are possible (<xref rid="bb0115" ref-type="bibr">Pinheiro and Bates, 1995</xref>). In the following we will give a number of examples for different types of constraints. In the simple case of an unconstrained example for a 2 × 2 covariance matrix, we could use the following elements of <bold>A</bold>:<disp-formula id="fo0095"><label>(A4)</label><mml:math id="M41" altimg="si41.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>→</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>If we want to enforce equality of the diagonal elements (variances), one less element for <bold>A</bold> is used.<disp-formula id="fo0100"><label>(A5)</label><mml:math id="M42" altimg="si42.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>→</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>If <bold>G</bold> has a block-diagonal structure, then all elements of <bold>A</bold> (<inline-formula><alternatives><textual-form specific-use="jats-markup"><bold>A</bold><sub><italic>k</italic></sub></textual-form><mml:math id="M43" altimg="si43.gif" overflow="scroll"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>) also need to share the same block-diagonal structure. For example the variance–covariance matrix in Eq. <xref rid="fo0045" ref-type="disp-formula">(9)</xref> can be rearranged in a block-diagonal form by swapping rows and columns. We can then enforce equality constraints across different blocks by having joint elements for each of the blocks.<disp-formula id="fo0105"><label>(A6)</label><mml:math id="M44" altimg="si44.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>→</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>However, it should be noted that it is not possible to enforce equal covariances across different blocks, while allowing different variances. Similarly, it is also not possible to enforce equal variances, while allowing different covariances. Thus, the variance–covariance structures of the block over which we want to enforce equality constraints need to be identical.</p>
        <p>One last example considers an unconstrained estimation of the elements of <bold>G</bold>, when one of the pairs of stimuli is uncorrelated. This situation may arise when we want to estimate the similarity structure between a set of stimuli and would like to remove the common task activation by introducing a common pattern. To disambiguate the common activation and correlation of individual patterns, the correlation of one pair of stimuli (here between 1 and 3) needs to be held constant.<disp-formula id="fo0110"><label>(A7)</label><mml:math id="M45" altimg="si45.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">G</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>To enforce such a constraint, we would have <italic>k</italic> = 5 basis matrices, in which the position of the corresponding parameter in the lower triangular part of <bold>G</bold> is set to 1. A linear combination of these basis matrices then forms <bold>G</bold>'s Cholesky factor.</p>
      </sec>
      <sec id="s0100">
        <title>Limitation on the possible structure of independence constraints</title>
        <p>In general, independence constraints between pattern components are encoded in the sparsity patterns of <bold>G</bold>. It is important to note that not all sparsity patterns of <bold>G</bold> can be translated into corresponding sparsity patterns of the Cholesky factors. For example, the solution employed for Eq. <xref rid="fo0110" ref-type="disp-formula">(A7)</xref> would not work if we try to set <italic>γ</italic><sub>2, 3</sub> to zero and allow <italic>γ</italic><sub>1, 3</sub> to be non-zero. In this case, a fill-in occurs; the Cholesky factor has six non-zero elements.</p>
        <p>Thus, for some independence structures, we need to re-order the rows and columns. For most cases, MATLAB's chol command for sparse matrices can be used with multiple return arguments, such that it permutes the rows and columns to obtain an appropriate decomposition. However, some structures produce an unavoidable fill-in, even allowing for permutations. One of the simplest examples is given by the sparsity pattern <italic>S</italic> = toeplitz([1 1 0 1]). Regardless of the values of the 8 unique nonzero elements, the Cholesky factorization has 9 values, meaning that this square root parameterization would have some redundancy.</p>
        <p>There is an interesting connection here to graph theory: <xref rid="bb0105" ref-type="bibr">Parter (1961)</xref> established a link between Gaussian elimination (from which Cholesky factorization emerges for symmetric positive definite matrices) and eliminating vertices from a graph with adjacency matrix given by the nonzero non-diagonal elements of the original matrix. Fill-in corresponds to edges that must be added to make the neighbourhood of a candidate vertex into a clique before removing that vertex and its edges. <xref rid="bb0125" ref-type="bibr">Rose (1970)</xref>, characterized graphs allowing a perfect (zero fill-in) elimination ordering of their vertices as “chordal,” i.e. having no cycles of length 4 or more without a chord joining a pair of non-consecutive vertices. The problematic sparsity pattern S given above corresponds to the simplest such graph: a square, in which one may form a cycle from element 1 to 2 to 3 to 4 and back to 1. Adding a chord across either diagonal of the square, corresponding to matrix elements (1, 3) or (2, 4), allows a perfect elimination ordering to be found.</p>
        <p>For most desired similarity structures of <bold>G</bold>, however, a matching similarity structure in <bold>A</bold> can be found. For some examples, one needs to rearrange rows and columns to avoid redundancy in the parameterization. For chordal graphs a perfect ordering can be found using maximum cardinality search (<xref rid="bb0010" ref-type="bibr">Berry et al., 2009</xref>). For the rare similarity structures that correspond to non-chordal graphs, there will be some redundancy in their Cholesky factors. Minimizing this redundancy is NP-hard, but approximate solutions can be efficiently found using a minimum degree heuristic (<xref rid="bb0005" ref-type="bibr">Berry et al., 2003</xref>), as available in MATLAB's chol command.</p>
      </sec>
      <sec id="s0105">
        <title>Estimating the variance–covariance structure</title>
        <p>After defining possible constraints, we want to estimate <italic>θ</italic> by optimizing the likelihood <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<bold>Y</bold>|<italic>θ</italic>)</textual-form><mml:math id="M46" altimg="si46.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula>. This optimization problem can be transformed into a regression problem, by introducing a new set of hidden variables <bold>v</bold> that have i.i.d. multivariate normal distribution, with identity covariance. In the following all <bold>y</bold>, <bold>v</bold>, <bold>u</bold>, and <italic>ε</italic> are Nx1 column vectors for each voxel, we will drop the superscript <italic>c</italic>.<disp-formula id="fo0115"><label>(A8)</label><mml:math id="M47" altimg="si47.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">u</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow/></mml:msubsup></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow/></mml:msubsup></mml:mfenced><mml:msup><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>We can now replace <bold>ZA</bold> with a new variable <bold>C</bold> and effectively replace the constraints in Eq. <xref rid="fo0090" ref-type="disp-formula">(A3)</xref> with constraints on <bold>C</bold>.<disp-formula id="fo0120"><label>(A9)</label><mml:math id="M48" altimg="si48.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:msub><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>≜</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mstyle><mml:mi mathvariant="bold">Z</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>k</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ε</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msub><mml:mi>θ</mml:mi><mml:mi>ε</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>This model is equivalent to stochastic factor analysis with constraints imposed on the loading matrix <bold>C</bold>. The complete log-likelihood of both the data <bold>Y</bold> and the hidden variables <bold>V</bold> together is:<disp-formula id="fo0125"><label>(A10)</label><mml:math id="M49" altimg="si49.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mfenced open="|" close="|"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mfenced open="|" close="|"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mfenced open="|" close="|"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mrow/></mml:msubsup><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>To estimate <italic>θ</italic> and <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> we would have to integrate out the hidden parameters <bold>v</bold>, <inline-formula><mml:math id="M50" altimg="si50.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mrow><mml:mo>∫</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi><mml:mo mathvariant="bold">,</mml:mo></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mfenced><mml:mi>d</mml:mi><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, and maximize this quantity. Because no closed form for this integral exists, we use the Expectation-Maximization algorithm (<xref rid="bb0145" ref-type="bibr">McLachlan, 1997</xref>) to instead maximize a lower bound on the log-likelihood, the free energy <italic>F</italic>.<disp-formula id="fo0130"><label>(A11)</label><mml:math id="M51" altimg="si51.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>log</mml:mtext><mml:mstyle><mml:mrow><mml:mo>∫</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:msup><mml:mrow/><mml:mi>ε</mml:mi></mml:msup><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>≥</mml:mo><mml:mstyle><mml:mrow><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mfenced><mml:mtext>log</mml:mtext><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:msub><mml:mo>p</mml:mo><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mfenced></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mfenced open="〈" close="〉"><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mfenced open="〈" close="〉"><mml:mrow><mml:mtext>log</mml:mtext><mml:mi>q</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">Y</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Where 〈〉<sub><italic>q</italic></sub> is the expected value under the proposal distribution. It can be shown that if <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>q</italic>(<bold>v</bold>)</textual-form><mml:math id="M52" altimg="si52.gif" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mfenced open="(" close=")"><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle></mml:mfenced></mml:mrow></mml:math></alternatives></inline-formula> is the posterior distribution over <bold>V</bold> given the parameters, the bound becomes tight and <italic>F</italic> becomes the log-likelihood that we are attempting to optimize.</p>
        <p>Thus, in the E-step we calculate the posterior distribution of <bold>V</bold>, given the current estimate of <italic>θ</italic> and <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup>. Because the noise is normally distributed, the posterior distribution over <bold>V</bold> also has multivariate normal distribution, meaning we only have to calculate the posterior mean and variance. On the first E-step we start with an initial guess on the parameters. Each iteration u then follows as:<disp-formula id="fo0135"><mml:math id="M53" altimg="si53.gif" overflow="scroll"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mfenced open="(" close=")"><mml:mi>u</mml:mi></mml:mfenced></mml:msubsup><mml:msub><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="fo0140"><label>(A12)</label><mml:math id="M54" altimg="si54.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mfenced open="(" close=")"><mml:mi>u</mml:mi></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced open="〈" close="〉"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mtext>var</mml:mtext><mml:msup><mml:mfenced open="(" close=")"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mtext>var</mml:mtext><mml:msup><mml:mfenced open="(" close=")"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced open="〈" close="〉"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>var</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="〈" close="〉"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:msup><mml:mfenced open="〈" close="〉"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>In the M-step, we update <italic>θ</italic> and <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup> by maximizing Eq. <xref rid="fo0130" ref-type="disp-formula">(A11)</xref>. To do this, we only consider the part of <italic>F</italic> that depends on the parameters. By taking the expectation of Eq. <xref rid="fo0125" ref-type="disp-formula">(A10)</xref> in respect to the distribution <italic>q</italic>(<italic>v</italic>), we get<disp-formula id="fo0145"><label>(A13)</label><mml:math id="M55" altimg="si55.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mfenced open="|" close="|"><mml:mrow><mml:mstyle><mml:mi mathvariant="bold">I</mml:mi></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mfenced open="〈" close="〉"><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>−</mml:mo></mml:mrow></mml:mstyle><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mo>...</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mfenced open="〈" close="〉"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mfenced open="〈" close="〉"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mfenced open="〈" close="〉"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>From Eq. <xref rid="fo0145" ref-type="disp-formula">(A13)</xref> we can read off the sufficient statistics that we need to calculate from the data and the hidden parameters <bold>V</bold>, such that we can take derivatives of Eq. <xref rid="fo0145" ref-type="disp-formula">(A13)</xref> with respect to <italic>θ</italic>:<disp-formula id="fo0150"><label>(A14)</label><mml:math id="M56" altimg="si56.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>n</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>n</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mfenced open="〈" close="〉"><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mfenced><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>p</mml:mi><mml:mrow/></mml:munderover><mml:msub><mml:mfenced open="〈" close="〉"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>q</mml:mi></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Given these sufficient statistics, we can now find the best solution for <italic>θ</italic> using linear regression. For the unconstrained case, the maximum likelihood estimators are<disp-formula id="fo0155"><label>(A15)</label><mml:math id="M57" altimg="si57.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>p</mml:mi><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">y</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msup><mml:mfenced open="〈" close="〉"><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mfenced><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mfenced><mml:msup><mml:mfenced open="(" close=")"><mml:mstyle><mml:munderover><mml:mo>∑</mml:mo><mml:mi>p</mml:mi><mml:mrow/></mml:munderover><mml:mfenced open="〈" close="〉"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">v</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mstyle></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>3</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mfenced open="(" close=")"><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="fo0160"><mml:math id="M58" altimg="si58.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msup><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>Because of the linear constraint (Eq. <xref rid="fo0090" ref-type="disp-formula">(A3)</xref>), however, we need to take the derivative of <italic>F</italic> (Eq. <xref rid="fo0145" ref-type="disp-formula">(A13)</xref>) with respect to all the parameters:<disp-formula id="fo0165"><label>(A16)</label><mml:math id="M59" altimg="si59.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mfenced open="〈" close="〉"><mml:mrow><mml:mtext>log</mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="[" close="]"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mfenced open="〈" close="〉"><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle></mml:mfenced><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="[" close="]"><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>k</mml:mi><mml:mrow/></mml:msubsup><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mstyle><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>3</mml:mn></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mfenced></mml:mfenced><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ε</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>By setting these derivatives to zero, we obtain the following system of linear equations:<disp-formula id="fo0170"><label>(A17)</label><mml:math id="M60" altimg="si60.gif" overflow="scroll"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>3</mml:mn></mml:msub><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mrow/></mml:mtd><mml:mtd columnalign="center"><mml:mo>⋱</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mi>θ</mml:mi><mml:msub><mml:mrow><mml:malignmark/></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>θ</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mn>1</mml:mn><mml:mrow/></mml:msubsup><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">C</mml:mi></mml:mstyle><mml:mi>K</mml:mi><mml:mrow/></mml:msubsup><mml:msubsup><mml:mstyle><mml:mi mathvariant="bold">S</mml:mi></mml:mstyle><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
        <p>Finally, we can simply invert this equation to obtain the maximum likelihood estimate of the parameters given the sufficient statistics of <bold>V</bold>. By iterating the E and M steps, the expected log-likelihood is guaranteed to increase with every step, thereby increasing the lower bound on the likelihood.</p>
      </sec>
      <sec id="s0110">
        <title>Implementation</title>
        <p>The algorithm is implemented in the Matlab function mvpattern_covcomp.m, which can be found at “<ext-link ext-link-type="uri" xlink:href="http://www.icn.ucl.ac.uk/motorcontrol/imaging/multivariate_analysis.htm">http://www.icn.ucl.ac.uk/motorcontrol/imaging/multivariate_analysis.htm</ext-link>” together with example code that reproduces all simulations reported in this paper.</p>
        <p>To speed the convergence, the algorithm uses the Aitken acceleration method (McLachlan and Krishnan, 1997). After 3 normal EM iterations, the scheme looks at the first and second derivative of the convergence and tries a large jump to the predicted best value. Because an increase in likelihood is not assured after such jumps, the increase needs to be tested and the jump rejected if the likelihood decreased. The method speeds convergence substantially. For the example of the one-factorial design (2.2) convergences was reached on average after 28 iterations or 15 ms (Quad-core Apple Pro, 2.26GHz), on the example of the two-factorial design (3.1), convergence was achieved after 86 iterations and 90 ms. For the latter example, the scheme is about a factor of 3 faster than a standard Fisher-scoring scheme (<xref rid="bb0025" ref-type="bibr">Friston, 2008</xref>).</p>
      </sec>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>The work was supported by the Wellcome Trust, grants from the National Science Foundation (NSF, BSC 0726685) and the Biotechnology and Biological Sciences Research Council (BBSRC, BB/E009174/1). We thank Pieter Medendorp and Nick Oosterhof for helpful discussion.</p>
    </ack>
    <fn-group>
      <fn id="fn0005">
        <label>1</label>
        <p>While the approach presented here assumes temporally independent error terms, an extension to times-series analysis with auto-correlated errors is possible by incorporating a structured covariance matrix in Eqs. <xref rid="fo0165 fo0170" ref-type="disp-formula">(A16) and (A17)</xref>.</p>
      </fn>
    </fn-group>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig. 1</label>
      <caption>
        <p>Covariance component model. The data (<italic>Y</italic>) comprise the activations over <italic>P</italic> voxels and <italic>N</italic> trials. The observed patterns (<bold>y</bold><sub><italic>n</italic></sub><sup><italic>r</italic></sup>) are generated from a set of <italic>Q</italic> unknown or hidden pattern components <bold>u</bold><sub><italic>q</italic></sub><sup><italic>r</italic></sup> and noise <italic>E</italic>. The hidden patterns <bold>u</bold><sub><italic>q</italic></sub><sup><italic>r</italic></sup> are modelled as random effects over the voxels, such that the columns <bold>u</bold><sub><italic>p</italic></sub><sup><italic>c</italic></sup> are distributed normally with variance–covariance matrix <bold>G</bold>. This matrix encodes the similarity structure of the different patterns.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig. 2</label>
      <caption>
        <p>Example of a one-factorial design with 3 stimuli shows the influence of noise on sample correlations. (A) The five measures for each of the 3 conditions consist of the corresponding true pattern component U and noise E. (B) Dependence of sample correlations (gray line) and of the estimates from the component model (dashed line) on the noise variability (<italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup>). Correlations between stimulus 1 and 2, stimulus 1 and 3, and stimulus 2 and 3 are shown. The true value is indicated by a line. (C) Multi-dimensional scaling of similarity structure, based on 1-r as a distance metric. The true similarity structure can be represented as one dimension.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig. 3</label>
      <caption>
        <p>Example of one-factorial design of three conditions that share a common activation pattern. (A) The data consist of 5 measures of the control condition, which provides a measure of the common activation pattern (first row in matrix U), followed by 5 measurements for the 3 conditions each. (B) Due to the common activation, the sample correlations (light gray line) between mean patterns are much higher than the true correlations (line). Prior subtraction of the mean pattern of the control condition (dark gray dashed line) lowers the estimates, but still overestimates the correlation and underestimates the differences. Using the pattern component model (black dashed line), valid estimates can be obtained. (C) Multidimensional scaling based on the estimated correlation coefficients shows distortions of similarity structure for sample correlations between mean patterns.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig. 4</label>
      <caption>
        <p>Estimates for the representational similarity between sensory and motor representations of the same finger (true value <italic>r</italic> = 0.5), as a function of the level of noise (<italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup>) and the covariance of the patterns common to the movement and stimulation conditions (<italic>γ</italic><sub><italic>α</italic></sub>). (A) The sample correlation calculated on the mean activation patterns for identical fingers across conditions is strongly influenced by noise and common activation. (B) By subtracting the correlation across conditions for different fingers, the influence of the common activation is eliminated. However, the correlation is underestimated and biased (downwards) by noise. (C) The correlation between patterns for the same fingers, after subtracting the mean pattern for the respective condition accounts partly for the effect of common activation, but is severely biased by noise. (D) The corrected estimate from the covariance-component model is unbiased over a large range of parameter settings.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig. 5</label>
      <caption>
        <p>Correlation estimate (true value <italic>r</italic> = 0.5) changes with increasing numbers of non-informative voxels. This makes it impossible to compare correlations across different regions. The graph shows the difference between sample correlation for same and different finger (dark gray dashed line) and the correlation between the patterns after subtracting the corresponding condition mean (light gray). The corrected estimate from the pattern-component model (black dashed) remains valid, even if 75% of the voxels in the studied regions are uninformative.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig. 6</label>
      <caption>
        <p>Traditional representational similarity analysis using sample correlations from a real data set. (A) Correlation-matrix for primary somatosensory cortex (S1) for two conditions (sense and move) and four levels (fingers 1, 2, 3 and 5). Bright squares indicate high correlations, dark squares zero or slight negative correlations. (B) In S1, the average correlation between movement and sensory patterns for the same finger (1) was elevated compared to those between different fingers (2). This was also the case for M1, but not for lobule V of the cerebellum. (C) The correlations between patterns for different fingers within the same condition were higher and more pronounced in the movement (4) than in the sensory (3) condition, suggesting different strength of the common activation patterns. (D) These correlations were much higher for patterns estimated within the same run than for different runs, indicating a strong covariance in the estimation errors. Error bars indicate between-subject (<italic>N</italic> = 7) standard error.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig. 7</label>
      <caption>
        <p>Decomposition of the correlations shown in <xref rid="f0030" ref-type="fig">Fig. 6</xref> into pattern components. (A) The estimated variance of the noise component (<italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup>) was 2.5 times stronger for the cerebellar lobule V than for the primary motor cortex (M1) and primary sensory cortex (S1). (B) The effect of run (<italic>σ</italic><sub><italic>δ</italic></sub><sup>2</sup>) was strong for both sensory (gray) and movement (white) condition and scaled in the same way as the noise component. (C) The effect was uncorrelated across conditions within the same run. (D) The variance of the common condition component (<italic>σ</italic><sub><italic>α</italic></sub><sup>2</sup>) was stronger for the movement (white) than for the sensory (gray) condition. (E) The components for the two conditions were slightly correlated. (F) The variance of the finger component (<italic>σ</italic><sub><italic>β</italic></sub><sup>2</sup>) was roughly matched across regions. (G) Covariance of the finger components across the two conditions confirms that there is a difference in the organization of sensory and motor maps between neocortex and cerebellum. Error bars indicate between-subject (<italic>N</italic> = 7) standard error.</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig. 8</label>
      <caption>
        <p>Stability of the estimators with respect to spatial smoothness of the pattern component of Factor B (SD of autocorrelation function,<italic>s</italic><sub><italic>β</italic></sub>, <italic>x</italic>-axis) and the noise component (<italic>s</italic><sub><italic>ε</italic></sub>, different lines). The first column shows the variance estimates, and second correlation estimates for the experimental factors A and B. The last column shows the estimates for the SD parameter of the spatial autocorrelation function (see text for details).</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
    <fig id="f0045">
      <label>Fig. 9</label>
      <caption>
        <p>Estimates of the spatial auto-correlation from 4 different pattern components for primary sensory cortex (S1), primary motor cortex (M1) and lobule V of the cerebellum (V). The noise components (A) and run component (B) show rapidly decaying spatial autocorrelation functions, with slightly wider correlations in cortical than in cerebellar regions. (C) The main effect of condition shows wider correlation kernels, indicating that larger groups of voxels increase or decrease consistently in movement and stimulation condition. (D) The effect of finger indicates slightly larger finger patches in S1 than in M1, with the representations in lobule V being smaller than the effective resolution.</p>
      </caption>
      <graphic xlink:href="gr9"/>
    </fig>
  </floats-group>
</article>