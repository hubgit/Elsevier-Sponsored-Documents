<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Pattern Recognit Lett</journal-id>
      <journal-title-group>
        <journal-title>Pattern Recognition Letters</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0167-8655</issn>
      <publisher>
        <publisher-name>Elsevier Science</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3223567</article-id>
      <article-id pub-id-type="pmid">22199412</article-id>
      <article-id pub-id-type="publisher-id">PATREC5120</article-id>
      <article-id pub-id-type="doi">10.1016/j.patrec.2011.05.005</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Hierarchical spatio-temporal extraction of models for moving rigid parts</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Artner</surname>
            <given-names>Nicole M.</given-names>
          </name>
          <email>artner@prip.tuwien.ac.at</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ion</surname>
            <given-names>Adrian</given-names>
          </name>
          <email>ion@ins.uni-bonn.de</email>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Kropatsch</surname>
            <given-names>Walter G.</given-names>
          </name>
          <email>krw@prip.tuwien.ac.at</email>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>AIT – Austrian Institute of Technology, Donau-City-Straße 1, Vienna 1220, Austria</aff>
      <aff id="aff2"><label>b</label>University of Bonn, Institute for Numerical Simulation, Wegelerstraße 4, Bonn 53115, Germany</aff>
      <aff id="aff3"><label>c</label>Vienna University of Technology, Institute of Computer Graphics and Algorithms, Pattern Recognition and Image Processing Group (PRIP), Favoritenstraße 9/186-3, Vienna 1040, Austria</aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author at: AIT – Austrian Institute of Technology, Donau-City-Straße 1, Vienna 1220, Austria. <email>artner@prip.tuwien.ac.at</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>01</day>
        <month>12</month>
        <year>2011</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>01</day>
        <month>12</month>
        <year>2011</year>
      </pub-date>
      <volume>32</volume>
      <issue>16-2</issue>
      <fpage>2239</fpage>
      <lpage>2249</lpage>
      <permissions>
        <copyright-statement>© 2011 Elsevier B.V.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Elsevier B.V.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract abstract-type="graphical">
        <title>Highlights</title>
        <p>► Extract part-based model based on trajectories of features. ► Model consists of rigid parts connected via points of articulation. ► Analyze movement in triangulated graph. ► Filter triangles which are not relevant. ► Group triangles which undergo similar motion.</p>
      </abstract>
      <abstract>
        <p>This paper presents a method to extract a part-based model of an observed scene from a video sequence. Independent motion is a strong cue that two points belong to different “rigid” entities. Conversely, <italic>things that move together throughout the whole video belong together</italic> and define a “rigid” object or part. Successfully tracked features indicate trajectories of salient points in the scene. A triangulated graph connects the salient points and encodes their local neighborhood in the first frame. The length variation of the triangle edges is used to label them as <italic>relevant</italic> (on an object) or <italic>separating</italic> (connecting different objects). A following grouping process uses the motion of the triangles marked as relevant as a cue to identify the “rigid” parts of the foreground or the background. The choice of the motion-based grouping criterion depends on the type of motion: in the image plane or out of the image plane. The result is a hierarchical description (graph pyramid) of the scene, where each vertex in the top level of the pyramid represents a “rigid” part of the foreground or the background, and encloses to the salient features used to describe it. Promising experimental results show the potential of the approach.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Rigid parts</kwd>
        <kwd>Articulated objects</kwd>
        <kwd>Model extraction</kwd>
        <kwd>Graph pyramid</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <label>1</label>
      <title>Introduction</title>
      <p>Visual tracking of articulated objects and their rigid parts is an important and still challenging task in computer vision (see for example the surveys <xref rid="b0155 b0070 b0115 b0005" ref-type="bibr">Yilmaz et al., 2006; Gavrila, 1999; Moeslund et al., 2006; Aggarwal and Cai, 1999</xref>). Example applications are the analysis of human motion for action recognition, motion based diagnosis and identification, motion capture for 3D animation and human computer interfaces.</p>
      <p>To be able to detect and associate instances of the object of interest in consecutive frames, tracking methods use a model of the <italic>target</italic> (the object to be tracked). This model is at the minimum a rectangle-shaped close-up of the object (called a template) or a color histogram, but can be as sophisticated as an online-trained classifier (<xref rid="b0075" ref-type="bibr">Godec et al., 2010</xref>), or a hierarchical description of the objects’ parts and their salient features (<xref rid="b0025" ref-type="bibr">Artner et al., 2011</xref>).</p>
      <p>Sources for the target model and the objects’ position in the initial frame are: user input, various segmentation and/or object recognition methods (e.g. <xref rid="b0065" ref-type="bibr">Felzenszwalb and Huttenlocher, 2005</xref>), or an initialization sequence (this work). In the latter case, the sequence is usually made specifically for this purpose, it emphasizes all relevant properties of the target object and does not pose a too high challenge to the visual aspect of the extraction of these properties (e.g. no unnecessary occlusion).</p>
      <p>This paper presents a method to extract a model for the “rigid” parts of articulated objects using the motion information in an “initialization video”. It follows the intuition that salient features on the same “rigid” parts will move together:<list list-type="simple"><list-item><label>1.</label><p>The input of the framework consists of point correspondences of salient features (e.g. corner points) over time (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>). These correspondences result in a set of trajectories, which give information about the motion in a scene.</p></list-item><list-item><label>2.</label><p>A triangulated graph is built based on the positions of the features in the first frame. It encodes the spatial relationships (edges) between the features (vertices) and its deformation over time is the basis for all processes and decisions (see <xref rid="f0015" ref-type="fig">Fig. 3</xref>).</p></list-item><list-item><label>3.</label><p>The motion in the triangulation is analyzed by determining the motion of the triangles in or out of the image plane (see Section <xref rid="s0030" ref-type="sec">3</xref>).</p></list-item><list-item><label>4.</label><p>In a spatio-temporal filtering the features describing “rigid” parts of foreground and background are selected. The result is a labeling of each triangle in the graph as <italic>relevant</italic> or <italic>separating</italic> depending on the variation of the edge lengths (see Section <xref rid="s0050" ref-type="sec">4.1</xref> and <xref rid="f0035" ref-type="fig">Fig. 7</xref>).</p></list-item><list-item><label>5.</label><p>All <italic>relevant</italic> triangles are the input for the hierarchical grouping process, which is realized by building a graph pyramid, where the base level is a graph encoding the adjacency of the triangles (see Section <xref rid="s0025" ref-type="sec">2</xref>). The grouping depends on the similarity of the motion of the triangles over time (see Section <xref rid="s0055" ref-type="sec">4.2</xref> and <xref rid="f0045" ref-type="fig">Fig. 9</xref>).</p></list-item><list-item><label>6.</label><p>The output of this process is a hierarchical description of the “rigid” parts in the scene, where in the optimal case each vertex in the top level of the pyramid represents one “rigid” part (see <xref rid="f0045" ref-type="fig">Fig. 9</xref>).</p></list-item></list></p>
      <p>Like the methods in (<xref rid="b0150 b0055" ref-type="bibr">Yan et al., 2008; Costeira and Kanade, 1998</xref>), the presented approach requires features that haven been tracked throughout the whole video sequence. We are interested in obtaining a model that describes the parts using the most salient features. The above requirement can be seen as a preprocessing step, which filters out the non-salient features.</p>
      <sec id="s0010">
        <label>1.1</label>
        <title>Related work</title>
        <p>The work in this paper is related to the concept of <italic>video object segmentation</italic> (VOS), where the task is to separate foreground from background in an image sequence. Notice however the difference: VOS methods try to group pixels as robustly as possible in a possibly highly cluttered scene, whereas in our case the emphasis is on extracting the relevant model properties (salient features and “rigid” parts) from a less cluttered scene. VOS methods can be divided into two categories (<xref rid="b0040" ref-type="bibr">Celasun et al., 2001</xref>):</p>
        <p><italic>Two</italic>-<italic>frame motion</italic>/<italic>object segmentation</italic>: <xref rid="b0015" ref-type="bibr">Altunbasak et al. (1998)</xref> a combination of pixel-based and region-based segmentation methods. Their goal is to obtain the best possible segmentation results on a variety of image sequences. <xref rid="b0035" ref-type="bibr">Castagno et al. (1998)</xref> describe a system for interactive video segmentation. An important key feature of the system is the distinction between two levels of segmentation: regions and object segmentation. <xref rid="b0045" ref-type="bibr">Chen et al. (2006)</xref> propose an approach to segment highly articulated objects by employing weak-prior random forests. The random forests are used to derive the prior probabilities of the object configuration for an input frame. Then these priors are applied to guide the grouping of over-segmented regions. The work of <xref rid="b0010" ref-type="bibr">Alatan et al. (1998)</xref> presents the activities of the COST 211<sup><italic>ter</italic></sup> group dedicated toward image and video sequence analysis and segmentation. This work is an important technological aspect for the success of emerging object-based MPEG-4 and MPEG-7 multimedia applications.</p>
        <p><italic>Multi</italic>-<italic>frame spatio-temporal segmentation</italic>/<italic>tracking</italic>: <xref rid="b0040" ref-type="bibr">Celasun et al. (2001)</xref> present VOS based on 2D meshes. <xref rid="b0130" ref-type="bibr">Tekalp et al. (1998)</xref> describe 2D mesh-based modeling of video objects as a compact representation of motion and shape for interactive video manipulation, compression, and indexing. <xref rid="b0110" ref-type="bibr">Li et al. (2001)</xref> propose to use affine motion models to estimate the motion of homogeneous regions.</p>
        <p>There are VOS methods explicitly dealing with the segmentation of articulated objects (e.g. <xref rid="b0045" ref-type="bibr">Chen et al., 2006</xref>), but the result of these approaches is still only a separation of foreground and background.</p>
        <p><italic>Motion segmentation</italic>: In comparison to VOS, motion segmentation approaches work on the basis of trajectories of features and not on the pixel level.</p>
        <p><xref rid="b0105" ref-type="bibr">Lauer and Schnörr (2010)</xref> present an approach to automatically segment multiple motions from tracked features points by spectral embedding and clustering of linear subspaces. <xref rid="b0125" ref-type="bibr">Nordberg and Zografos (2010)</xref> also work on motion segmentation and propose an approach using the geometry of 6 points in 2D images to infer motion consistency with regard to rigid 3D motion. As in the work of <xref rid="b0105" ref-type="bibr">Lauer and Schnörr (2010)</xref>, they are able to segment the motion of an arbitrary number of moving objects. Nevertheless, articulated objects like humans are segmented as one object and not split into their moving rigid parts.</p>
        <p><italic>Automatic articulated model extraction</italic>: The most similar works to the presented approach lie in the field of automatic model extraction.</p>
        <p><xref rid="b0150" ref-type="bibr">Yan et al. (2008)</xref> use factorization to analyze the trajectories of tracked features and cluster them into subspaces corresponding to parts. This method can cope with affine motion and can extract both articulation axes and joints, but their parts have to be fully rigid or defined by a linear combination of a subset of the features. <xref rid="b0145 b0140" ref-type="bibr">Walther and Würtz (2009, 2008)</xref> a method to learn a 2D pictorial model of observed humans for pose estimation. Their approach is based on the 2D trajectories of features, which are grouped to limbs using spectral clustering. The extracted limbs are refined by employing multi-label image segmentation methods. They also extract body kinematics by finding joint connections between the segmented limbs. <xref rid="b0060" ref-type="bibr">Drouin et al. (2008)</xref> propose an approach which incrementally identifies object parts in videos. The main contribution of their approach is the <italic>Modeler</italic>, which allows to track several candidates for the model of the foreground object in parallel. As the approach in (<xref rid="b0145" ref-type="bibr">Walther and Würtz, 2009</xref>) their work is limited to movements in the 2D image plane. In comparison to our approach, this work requires the initialization of the foreground object.</p>
      </sec>
      <sec id="s0015">
        <label>1.2</label>
        <title>Contributions</title>
        <p>Our approach goes beyond the related works by following:<list list-type="simple"><list-item><label>•</label><p>Analysis of trajectories of features and their behavior on a higher abstraction level – in a triangulation. The related works group pixels or features independently without considering spatial proximity and relationships.</p></list-item><list-item><label>•</label><p>A generic grouping framework, which allows the usage of different grouping criteria depending on the motion of the objects in the video. This flexibility allows to adjust and optimize the presented framework for any application.</p></list-item><list-item><label>•</label><p>Build a graph pyramid based on the adjacency graph of the triangulation, guided by the motion of features. There is only a small number of works (e.g. <xref rid="b0050" ref-type="bibr">Conte et al., 2005</xref>), where graph pyramids are employed on spatio-temporal data and to the best of our knowledge there is no work employing graph pyramids to extract models based on the motion of features in a triangulation.</p></list-item><list-item><label>•</label><p>Besides providing a grouping of features into “rigid” parts like the related work, the presented approach additionally supplies a hierarchical description of the “rigid” parts, which can be used as a model in coarse-to-fine tracking scenarios.</p></list-item></list></p>
        <p>This paper extends the work in (<xref rid="b0020" ref-type="bibr">Artner et al., 2009</xref>) for out of plane motion and shows additional experimental results.</p>
      </sec>
      <sec id="s0020">
        <label>1.3</label>
        <title>Overview</title>
        <p>The paper is organized as follows: in Section <xref rid="s0025" ref-type="sec">2</xref> graph pyramids which are employed for the grouping process are briefly recalled. Section <xref rid="s0030" ref-type="sec">3</xref> explains how the motion of features in and out of the image plane is described and analyzed. Section <xref rid="s0045" ref-type="sec">4</xref> presents the generic framework, which allows to identify the “rigid” parts of articulated objects. Section <xref rid="s0060" ref-type="sec">5</xref> shows experiments and in Section <xref rid="s0085" ref-type="sec">6</xref> conclusions are given.</p>
      </sec>
    </sec>
    <sec id="s0025">
      <label>2</label>
      <title>Recall: irregular graph pyramids</title>
      <p>A <italic>region adjacency graph</italic> (RAG), encodes the adjacency of regions in a partition. A vertex is associated to each region, vertices of neighboring regions are connected by an edge. Classical RAGs do not contain any self-loops or parallel edges. An <italic>extended region adjacency graph</italic> (eRAG) is a RAG that contains the so-called <italic>pseudo edges</italic>, which are self-loops and parallel edges used to encode neighborhood relations to a cell completely enclosed by one or more other cells (<xref rid="b0090" ref-type="bibr">Kropatsch, 1995</xref>). The <italic>dual</italic> graph of an eRAG <italic>G</italic> is called <italic>boundary graph</italic> and is denoted by <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> (<italic>G</italic> is said to be the <italic>primal</italic> graph of <inline-formula><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>). The edges of <inline-formula><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> represent the boundaries of the regions encoded by <italic>G</italic>, and the vertices of <inline-formula><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> represent points where boundary segments meet. <italic>G</italic> and <inline-formula><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> are planar graphs. There is a one-to-one correspondence between the edges of <italic>G</italic> and the edges of <inline-formula><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>, which also induces a one-to-one correspondence between the vertices of <italic>G</italic> and the 2D cells (denoted by <italic>faces</italic><xref rid="fn1" ref-type="fn">1</xref>) of <inline-formula><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>. The dual of <inline-formula><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> is again <italic>G</italic>. The following operations are equivalent: edge contraction in <italic>G</italic> with edge removal in <inline-formula><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>, and edge removal in <italic>G</italic> with edge contraction in <inline-formula><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>.</p>
      <p>A (dual) <italic>irregular graph pyramid</italic> (<xref rid="b0090 b0095" ref-type="bibr">Kropatsch, 1995; Kropatsch et al., 2005</xref>) is a stack of successively reduced planar graphs <inline-formula><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. Each level <inline-formula><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext><mml:mspace width="0.35em"/><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>k</mml:mi><mml:mo>⩽</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> is obtained by first contracting edges in <italic>G</italic><sub><italic>k</italic>−1</sub> (removal in <inline-formula><mml:math id="M13" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), if their end vertices have the same label (regions should be merged), and then removing edges in <italic>G</italic><sub><italic>k</italic>−1</sub> (contraction in <inline-formula><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) to simplify the structure. The contracted and removed edges are said to be <italic>contracted</italic> or <italic>removed</italic> in <inline-formula><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. In each <italic>G</italic><sub><italic>k</italic>−1</sub> and <inline-formula><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, contracted edges form trees called <italic>contraction kernels</italic>. One vertex of each contraction kernel is called a <italic>surviving vertex</italic> and is considered to have been “survived” to <inline-formula><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. The vertices of a contraction kernel in level <italic>k</italic> − 1 form the <italic>reduction window W</italic>(<italic>v</italic>) of the respective surviving vertex <italic>v</italic> in level <italic>k</italic>. The <italic>receptive field F</italic>(<italic>v</italic>) of <italic>v</italic> is the (connected) set of vertices from level 0 that have been “merged“ to <italic>v</italic> over levels 0, … , <italic>k</italic>.</p>
      <p>For the sake of simplicity, the rest of the paper will only use the adjacency graph <italic>G</italic>, but for correctly encoding the topology, both <italic>G</italic> and <inline-formula><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> have to be maintained. <xref rid="f0005" ref-type="fig">Fig. 1</xref> shows an example triangulation and pyramid.</p>
    </sec>
    <sec id="s0030">
      <label>3</label>
      <title>Analysis of motion</title>
      <p>The grouping of the triangles in the scene into “rigid” parts relies on the intuitive idea that features which move together belong to the same “rigid” part. This section explains how motion is described and analyzed in the presented framework. The description and analysis of motion is based on the trajectories of the independently tracked features (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>). It is used later to filter the input for the hierarchical grouping process (see Section <xref rid="s0050" ref-type="sec">4.1</xref>) and for the grouping itself (see Section <xref rid="s0055" ref-type="sec">4.2</xref>).</p>
      <p>The presented framework is generic with respect to the type of the motion representation and therefore to the criterion in the hierarchical grouping process.</p>
      <p>In comparison to the related work, the presented framework analyzes the motion of features in a higher abstraction level – a triangulation. The vertices <italic>V</italic> of the triangulated graph <italic>G</italic> represent the tracked features (e.g. corner points with their positions). A Delaunay triangulation (<xref rid="b0135" ref-type="bibr">Tuceryan and Chorzempa, 1991</xref>) is used in the first frame to insert the edges <italic>E</italic>, connect the features and represent their spatial relationships (see <xref rid="f0015" ref-type="fig">Fig. 3</xref>).</p>
      <p>The advantage of analyzing the motion on triangles is the reduction of the ambiguity of motion in the 2D image plane (see <xref rid="f0020" ref-type="fig">Fig. 4</xref> and Section <xref rid="s0035" ref-type="sec">3.1</xref>) and the availability of necessary information for determining the affine transformation matrices for motion out of the image plane (see Section <xref rid="s0040" ref-type="sec">3.2</xref>).</p>
      <sec id="s0035">
        <label>3.1</label>
        <title>Motion in the image plane</title>
        <p>If the movement of the objects in the scene is limited to the image plane, the motion of triangles can be described using their “translation invariant” <italic>orientation variation</italic> over time.<statement id="n0005"><label>Definition 1</label><p>The <italic>orientation variation O</italic><sub><italic>e</italic></sub> <italic>of an edge e</italic> over time <italic>t</italic> =  2, … , <italic>n</italic>, where <italic>n</italic> is the number of frames, is a 1D signal that encodes at each frame <italic>t</italic> the accumulated orientation change relative to the orientation at frame 1. More formally, for edge <italic>e</italic> and frame <italic>t</italic>,<disp-formula id="e0040"><alternatives><textual-form specific-use="jats-markup"><italic>O</italic><sub><italic>e</italic></sub>(<italic>t</italic>) = <italic>O</italic><sub><italic>e</italic></sub>(<italic>t</italic> - 1) + θ(<italic>t</italic>),</textual-form><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></alternatives></disp-formula>where <italic>θ</italic>(<italic>t</italic>) is the relative change in orientation (signed angle) of the edge <italic>e</italic> between frames <italic>t</italic> and <italic>t</italic> − 1.</p></statement></p>
        <p>We assume that the rotation of an edge between two consecutive frames is less than 180° and therefore we do not deal with the problem of “circularity” (+180° = −180°).</p>
        <p>If any of the two end points of an edge is fixed as the reference point and the other end point is turned around the reference point once this will give a value of 360° degrees and turning twice in the same direction will give 720°, not 0°. The direction of rotation is encoded by the sign: counter clockwise (CCW) is positive, and clockwise (CW) is negative. <xref rid="f0025" ref-type="fig">Fig. 5</xref> shows an example for the orientation variation of an edge: if turning 45° CCW, then again 45° CCW, and afterwards 90° CW, the computed variations will be 0°, 45° = 0° + 45°, 90° = 45° + 45°, 0° = 90°−90° (see <xref rid="f0030" ref-type="fig">Fig. 6</xref>).<statement id="n0010"><label>Definition 2</label><p>The <italic>orientation variation O</italic><sub><italic>r</italic></sub> <italic>of a triangle r</italic> is the 1D signal obtained by taking the average of the 1D signals of the three edges of the triangle:<disp-formula id="e0045"><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>e</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 0, 1, 2 are the edges of the triangle.</p></statement><statement id="n0015"><label>Definition 3</label><p>The <italic>similarity X</italic><sub><italic>O</italic></sub> <italic>between two in-plane orientation variation signals O</italic><sub>1</sub>, <italic>O</italic><sub>2</sub> is calculated as follows:<disp-formula id="e0050"><mml:math id="M21" altimg="si21.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>O</italic><sub>1</sub>(<italic>t</italic>) and <italic>O</italic><sub>2</sub>(<italic>t</italic>) are the 1D signals corresponding to two triangles or two groups of triangles.</p></statement></p>
        <p>For an explanation of the 1D signal of a group of triangles refer to Section <xref rid="s0055" ref-type="sec">4.2</xref>.</p>
      </sec>
      <sec id="s0040">
        <label>3.2</label>
        <title>Motion out of the image plane</title>
        <p>For objects moving out of the image plane the vertices of the triangles in the image are feature points corresponding to points projected from the 3D world to the image. The motion of the triangles is described using 2D affine transformation matrices.</p>
        <p>An affine transformation can be described with the help of homogeneous coordinates by a matrix <italic>M</italic> = 3 × 3:<disp-formula id="e0005"><label>(1)</label><alternatives><textual-form specific-use="jats-markup"><italic>P</italic><sup>′</sup> = <italic>M</italic>*<italic>P</italic>,</textual-form><mml:math id="M22" altimg="si22.gif" overflow="scroll"><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mtext>,</mml:mtext></mml:math></alternatives></disp-formula>where <italic>P</italic> = 3 × 1, <italic>P</italic>′ = 3 × 1 are the homogeneous coordinates of the point(s) before and after applying the transformation. <italic>M</italic> can be written as:<disp-formula id="e0010"><label>(2)</label><mml:math id="M23" altimg="si23.gif" overflow="scroll"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="[" close="]"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>A</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mi>B</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn><mml:mspace width="0.35em"/><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>A</italic> describes the linear transformation (rotation, scaling or shear) and <italic>B</italic> is the translation (shift).</p>
        <p>An affine transformation matrix <italic>M</italic> is uniquely determined by the correspondences of three non-collinear 2D points. The entries in the transformation matrix can be calculated by solving the resulting 6 linear equations (<xref rid="b0085" ref-type="bibr">Klein, 1939</xref>).</p>
        <p>As in the planar case (Section <xref rid="s0035" ref-type="sec">3.1</xref>), we factor out the translation of the triangles when describing their motion, and focus on rotation as it provides a strong cue for (non-)rigid motion.</p>
        <p>Hence, the presented approach uses the coordinates given by the three vectors <inline-formula><mml:math id="M24" altimg="si24.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:mspace width="0.35em"/><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M25" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> determined at two time instances, where {<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, <italic>v</italic><sub>3</sub>} are the three vertices of a triangle <italic>r</italic>.<statement id="n0020"><label>Definition 4</label><p>The (<italic>affine</italic>) <italic>transformation T</italic><sub><italic>r</italic></sub> <italic>of a triangle r</italic> over time <italic>t</italic> = 2, … , <italic>n</italic> is the signal that encodes for each frame <italic>t</italic> the (affine) transformation matrix that maps the vectors <inline-formula><mml:math id="M26" altimg="si26.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mtext>,</mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mtext>,</mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> of a triangle from the first frame to frame <italic>t</italic>:<disp-formula id="e0015"><label>(3)</label><alternatives><textual-form specific-use="jats-markup"><italic>P</italic>(<italic>t</italic>) = <italic>T</italic><sub><italic>r</italic></sub>(<italic>t</italic>)*<italic>P</italic>(1),</textual-form><mml:math id="M27" altimg="si27.gif" overflow="scroll"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></alternatives></disp-formula>where <italic>P</italic>(1) and <italic>P</italic>(<italic>t</italic>) are 3 × 3 matrices having as rows <inline-formula><mml:math id="M28" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="}"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mtext>,</mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover><mml:mtext>,</mml:mtext><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>→</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> in homogeneous coordinates at time 1 and <italic>t</italic>.</p></statement></p>
        <p>The affine transformation matrix <italic>T</italic><sub><italic>r</italic></sub>(<italic>t</italic>), which has the form in Eq. <xref rid="e0010" ref-type="disp-formula">(2)</xref>, is determined by solving a linear system of equations (<xref rid="b0085" ref-type="bibr">Klein, 1939</xref>).</p>
        <p>To determine a criterion for the hierarchical grouping process, a similarity measure for the transformation signals is needed. As there is no similarity measure to compare transformation matrices directly, we propose to apply the transformation matrices and compare the results (see <xref rid="f0030" ref-type="fig">Fig. 6</xref>).</p>
        <p>The idea is to transform the points <italic>p</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, … , <italic>k</italic> lying on an unit circle, which is centered at the origin (0, 0), by the matrices <italic>T</italic><sub>1</sub> and <italic>T</italic><sub>2</sub> of two triangles and measure the similarity of the transformation matrices in the resulting positions <inline-formula><mml:math id="M29" altimg="si29.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M30" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        <p>Measuring the similarity by calculating the Euclidean distances between the two sets <inline-formula><mml:math id="M31" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M32" altimg="si32.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> has two disadvantages: (1) the Euclidean distance does not characterize well (linearly) in-plane rotation and (2) the observed effects of rotation in and out of the image plane on a triangle are defined on a different domain: rotation in the image plane maps linearly to angles, rotation out of the image plane is (non-linearly) observed as scaling.</p>
        <p>Therefore, we propose to analyze the similarity using polar coordinates, which are better suited to describe in and out of plane rotation respectively by their radii and angles. <xref rid="f0025" ref-type="fig">Fig. 5</xref> illustrates the concept by approximating the unit circle with a <italic>k</italic> = 5 sided regular polygon, where this approximation is also used for the experiments in Section <xref rid="s0060" ref-type="sec">5</xref>.<statement id="n0025"><label>Definition 5</label><p>The <italic>similarity X</italic><sub><italic>T</italic></sub> <italic>of two transformation signals T</italic><sub>1</sub>, <italic>T</italic><sub>2</sub> is calculated by:<disp-formula id="e0055"><mml:math id="M33" altimg="si33.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>T</italic><sub>1</sub>, <italic>T</italic><sub>2</sub> are the transformation matrices of two triangles or groups of triangles and <italic>d</italic><sub><italic>A</italic></sub>, <italic>d</italic><sub><italic>R</italic></sub> are the mean differences of the two sets <inline-formula><mml:math id="M34" altimg="si34.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mtext>,</mml:mtext><mml:mspace width="0.35em"/><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> regarding the angles and radii calculated for each frame <italic>t</italic>.</p></statement></p>
      </sec>
    </sec>
    <sec id="s0045">
      <label>4</label>
      <title>The generic grouping framework</title>
      <p>In this section the proposed generic framework is presented, where the aim is to identify the “rigid” parts in a scene (e.g. decomposing a human into head, torso, and limbs).</p>
      <p>The input of the framework is the spatio-temporal information about the tracked features, which includes the position of each feature over time (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>), the motion of the triangles, and the resulting deformation of the triangulated graph (see <xref rid="f0015" ref-type="fig">Fig. 3</xref>).</p>
      <p>First a spatio-temporal filtering selects the input for the hierarchical grouping process (see <xref rid="f0035" ref-type="fig">Fig. 7</xref>). Then each group of triangles belonging to a rigid part is generalized into a single vertex at the top level of an irregular graph pyramid (see <xref rid="f0045" ref-type="fig">Fig. 9</xref>).</p>
      <sec id="s0050">
        <label>4.1</label>
        <title>Spatio-temporal filtering</title>
        <p>The aim of the spatio-temporal filtering is to select <italic>relevant</italic> triangles for the input of the hierarchical grouping process. A triangle is <italic>relevant</italic> for the grouping process if the length of its edges remains nearly stable over time, which indicates its affiliation to a “rigid” part.</p>
        <p>The <italic>edge length</italic> of every edge <italic>e</italic> = (<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>) ∈ <italic>E</italic> at time <italic>t</italic> is the Euclidean distance between the positions of the two vertices <italic>e</italic>(<italic>t</italic>) = ∥<italic>v</italic><sub>1</sub>(<italic>t</italic>) − <italic>v</italic><sub>2</sub>(<italic>t</italic>)∥. Triangles which do not belong to a “rigid” part are outliers and mostly only stand out for a short period of time. Therefore, we propose to consider the extrema of an edge for the spatio-temporal filtering.<statement id="n0030"><label>Definition 6</label><p>The <italic>maximum variation of edge length</italic> is the difference between the minimum and the maximum length of the edge in the video, more formally<disp-formula id="e0060"><mml:math id="M35" altimg="si35.gif" overflow="scroll"><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>⩽</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo>-</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>⩽</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">{</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mtext>.</mml:mtext></mml:math></disp-formula></p></statement></p>
        <p>A triangle is labeled as <italic>relevant</italic> if the edge length variations of all three edges <italic>e</italic><sub>1</sub>,<italic>e</italic><sub>2</sub>,<italic>e</italic><sub>3</sub> are beneath a certain value i.e. <italic>l</italic>(<italic>e</italic><sub><italic>i</italic></sub>) ⩽ <italic>ϵ</italic><sub><italic>r</italic></sub>,1 ⩽ <italic>i</italic> ⩽ 3. Otherwise, the triangle is labeled as <italic>separating</italic>.</p>
        <p>The value <italic>ϵ</italic><sub><italic>r</italic></sub> should be chosen s.t. triangles with a high deformation over time, which connect different “rigid” parts, are labeled as <italic>separating</italic> and triangles with points on the same “rigid” part will have edge length variations smaller than <italic>ϵ</italic><sub><italic>r</italic></sub>.</p>
        <p>The result of the spatio-temporal filtering is a triangulation, where each triangle is labeled <italic>relevant</italic> or <italic>separating</italic> (see <xref rid="f0035 f0040" ref-type="fig">Figs. 7 and 8</xref>).</p>
      </sec>
      <sec id="s0055">
        <label>4.2</label>
        <title>Hierarchical grouping process</title>
        <p>The task of this process is to group the <italic>relevant</italic> triangles which survived the spatio-temporal filtering into groups of triangles, each describing one “rigid” part (see <xref rid="f0045" ref-type="fig">Fig. 9</xref>).</p>
        <p>The grouping process is realized by building a irregular graph pyramid on the dual graph of the already existing triangulation. There are three reasons for the usage of an irregular graph pyramid: (1) using a hierarchy reduces the complexity of the grouping (global decisions become local ones), (2) the produced description can be used for a coarse-to-fine tracking approach and (3) in comparison to a regular graph pyramid the irregular one has the advantage that it is adaptive (shift and rotation invariant).<table-wrap id="t0015" position="float"><table frame="hsides" rules="groups"><thead><tr><th><bold>Algorithm 1:</bold><italic>BuildPyr</italic>(<italic>T</italic>): Group triangles into “rigid” parts</th></tr></thead><tbody><tr><td><bold>Input</bold>: <italic>relevant</italic> triangles <italic>T</italic> (see Section <xref rid="s0050" ref-type="sec">4.1</xref>)</td></tr><tr><td>1: <italic>G</italic><sub>0</sub> = (<italic>V</italic><sub>0</sub>, <italic>E</italic><sub>0</sub>)</td></tr><tr><td> /∗<italic>V</italic><sub>0</sub> = <italic>T</italic>, <italic>and</italic> (<italic>v</italic>, <italic>w</italic>) ∈ <italic>E</italic><sub>0</sub> ⇔ <italic>the corresponding triangles share an edge</italic>∗/</td></tr><tr><td>2: <italic>k</italic> = 0</td></tr><tr><td>3: <bold>repeat</bold></td></tr><tr><td>4:  /∗<italic>select edges to contract</italic>∗/</td></tr><tr><td> <italic>K</italic> = ∅</td></tr><tr><td> ∀<italic>v</italic> ∈ <italic>G</italic><sub><italic>k</italic></sub><bold>do</bold><inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>K</italic>←<italic>K</italic>∪argmin<sub>(<italic>v</italic>,<italic>w</italic>)∈<italic>G</italic><sub><italic>k</italic></sub></sub>{<italic>X</italic>(<italic>v</italic>,<italic>w</italic>)}</textual-form><mml:math id="M36" altimg="si36.gif" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi><mml:mo>←</mml:mo><mml:mi>K</mml:mi><mml:mo>∪</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mtext>,</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">{</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mtext>,</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></td></tr><tr><td>5: /∗filter edges based on internal/external difference∗/</td></tr><tr><td> ∀(<italic>v</italic>, <italic>w</italic>) ∈ <italic>K</italic>, <bold>if</bold><italic>X</italic>(<italic>v</italic>, <italic>w</italic>) &gt; <italic>I</italic>′(<italic>v</italic>, <italic>w</italic>) <bold>then</bold></td></tr><tr><td>  <italic>K</italic> ← <italic>K</italic>⧹{(<italic>v</italic>, <italic>w</italic>)}</td></tr><tr><td>6:  <bold>if</bold><italic>K</italic> ≠ ∅ <bold>then</bold> break <italic>K</italic> into trees of radius 1</td></tr><tr><td>7:  <bold>if</bold><italic>K</italic> ≠ ∅ <bold>then</bold><italic>G</italic><sub><italic>k</italic>+1</sub> ← contract edges <italic>K</italic> in <italic>G</italic><sub><italic>k</italic></sub> and simplify</td></tr><tr><td>8:  <italic>k</italic> ← <italic>k</italic> + 1</td></tr><tr><td>9:  <bold>until</bold><italic>K</italic> = ∅</td></tr><tr><td><bold>Output</bold>: Graph pyramid <italic>P</italic> = {<italic>G</italic><sub>0</sub>, … , <italic>G</italic><sub><italic>k</italic>−1</sub>}.</td></tr></tbody></table></table-wrap></p>
        <p>Algorithm 1 creates a graph pyramid in which each vertex <italic>v</italic> of the top level identifies a detected “rigid” part, with its average motion description (orientation variation or transformation matrix over time) stored in <italic>S</italic>(<italic>v</italic>). The receptive fields of these vertices identify the triangles that the respective part consists of.</p>
        <p>Note that the pyramid is not built on the triangulated graph, but on its dual. In the base level <italic>G</italic><sub>0</sub>, one vertex is associated to each <italic>relevant</italic> triangle. Two vertices are connected by an edge if the respective triangles share a common edge. Edges to be contracted are selected from the edges proposed by the Minimum Spanning Tree algorithm by Boruvka (<xref rid="b0120" ref-type="bibr">Nesetril et al., 2001</xref>) (Line 4).</p>
        <p>The <italic>external difference X</italic>(<italic>v</italic>, <italic>w</italic>) between two vertices (triangles) <italic>v</italic> and <italic>w</italic> depends on the representation for the motion of the triangles (see Section <xref rid="s0030" ref-type="sec">3</xref>) and is computed using one of the two formulas:<disp-formula id="e0020"><label>(4)</label><mml:math id="M37" altimg="si37.gif" overflow="scroll"><mml:mtable columnspacing="0em"><mml:mtr><mml:mtd columnalign="right"/><mml:mtd columnalign="left"><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mtext>,</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"/><mml:mtd columnalign="left"><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mtext>,</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>S</italic>(<italic>v</italic>) and <italic>S</italic>(<italic>w</italic>) are the signals associated to <italic>v</italic> respectively <italic>w</italic>. For the vertices <italic>v</italic> ∈ <italic>G</italic><sub>0</sub> of the base level, depending on the type of motion in the video, <italic>S</italic>(<italic>v</italic>) is either the orientation variation signal <italic>O</italic><sub><italic>r</italic></sub> or the transformation signal <italic>T</italic><sub><italic>t</italic></sub> of the corresponding triangle <italic>r</italic>. For a vertex in a higher level it is computed as:<disp-formula id="e0025"><label>(5)</label><mml:math id="M38" altimg="si38.gif" overflow="scroll"><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>·</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:math></disp-formula>where ∣<italic>F</italic>(<italic>v</italic>)∣ is the size of <italic>F</italic>(<italic>v</italic>) and can be propagated up in the pyramid. Note that in the case of transformation matrices the computation is done element-wise (scalar multiplication, element-wise addition, division as scalar multiplication).</p>
        <p>The <italic>internal difference</italic> of a vertex at level <italic>k</italic> &gt; 0 is:<disp-formula id="e0030"><label>(6)</label><alternatives><textual-form specific-use="jats-markup"><italic>I</italic>(<italic>v</italic>) = max(max{<italic>I</italic>(<italic>u</italic>)},max{<italic>X</italic>(<italic>w</italic><sub><italic>i</italic></sub>,<italic>w</italic><sub><italic>j</italic></sub>)}),</textual-form><mml:math id="M39" altimg="si39.gif" overflow="scroll"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mtext>,</mml:mtext><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:math></alternatives></disp-formula>where <italic>u</italic> ∈ <italic>W</italic>(<italic>v</italic>) and <italic>w</italic><sub><italic>i</italic></sub>, <italic>w</italic><sub><italic>j</italic></sub> ∈ <italic>W</italic>(<italic>v</italic>) s.t. <italic>w</italic><sub><italic>i</italic></sub>, <italic>w</italic><sub><italic>j</italic></sub> are connected by an edge. For the vertices in the base level <italic>I</italic>(<italic>v</italic>) = 0. The value <italic>I</italic>′(<italic>v</italic>, <italic>w</italic>) is defined as:<disp-formula id="e0035"><label>(7)</label><mml:math id="M40" altimg="si40.gif" overflow="scroll"><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mtext>,</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>β</italic> is a parameter of the method that allows regions to start forming in the base level where <italic>I</italic>(<italic>v</italic>) = 0 for all vertices. The selected smallest edges (Line 4 of Algorithm 1) are accepted for contraction up to a weight of <italic>β</italic>. When going higher in the pyramid the size of the receptive fields increases and the contribution of <italic>β</italic> to the condition in Line 5 of Algorithm 1 rapidly decreases. As a result <italic>β</italic> sets an upper bound on the internal difference (deformation) of the produced parts.</p>
        <p>Line 6 of Algorithm 1 keeps the contraction operations local (optimal for parallel processing) and avoids contracting the whole graph in a single level. It does this by excluding edges from <italic>K</italic> to obtain trees of radius 1 for the current contraction. The excluded edges will be selected again in the next level. In (<xref rid="b0100" ref-type="bibr">Kropatsch et al., 2007</xref>) three methods, MIES, MIS, and D3P (used in our experiments) for breaking large contraction kernels are described, and it is also shown that their difference in the context of segmentation is minimal.</p>
        <p>Note that the described grouping method is similar to the image segmentation method in (<xref rid="b0080" ref-type="bibr">Haxhimusa and Kropatsch, 2004</xref>), which also builds a graph pyramid and uses concepts of internal/external contrast. Important differences are (presented approach vs. <xref rid="b0080" ref-type="bibr">Haxhimusa and Kropatsch, 2004</xref>):<list list-type="simple"><list-item><label>1.</label><p>Edge weights are recomputed at every level to reflect the differences between the updated ‘models’ for the <italic>whole regions</italic> vs. always selecting a subset of the weights from the level below, which reflects the difference between vertices that are neighbors in the base level (contrast <italic>along the boundary</italic>).</p></list-item><list-item><label>2.</label><p>The features are signals of orientation variation/transformation vs. color values.</p></list-item><list-item><label>3.</label><p>The method starts from a (possibly disconnected) graph vs. from a neighborhood graph using 4 connectivity.</p></list-item></list></p>
        <p>The difference at 1 has the effect that a long chain of regions that differs by a constant small difference, will not be merged to create a single region (e.g. a smooth gradient over the whole image). This is very important when grouping movements with locally constant difference like for example the skin covered body.</p>
      </sec>
    </sec>
    <sec id="s0060">
      <label>5</label>
      <title>Experiments</title>
      <p>The experiments in this section are divided by the type of motion in the input videos: motion in (see Section <xref rid="s0070" ref-type="sec">5.2</xref>) and out of the 2D image plane (see Section <xref rid="s0075" ref-type="sec">5.3</xref>).</p>
      <sec id="s0065">
        <label>5.1</label>
        <title>Parameters of the framework</title>
        <p>In the following the parameters of the presented framework are recalled:<list list-type="simple"><list-item><label><italic>ϵ</italic><sub><italic>r</italic></sub></label><p>This threshold decides if a triangle is labeled <italic>relevant</italic> or <italic>separating</italic>. It should be set considering the noise in the sequence, the tracking errors, and the local deformations in the parts (e.g. skin, cloth, material of man-made object).</p></list-item><list-item><label><italic>β</italic></label><p>Sets an upper bound for the internal difference in the grouping process in the graph pyramid, which decides if edges are contracted (triangles are grouped together). This parameter is important in the lower levels of the pyramid (especially in the first level) and its influence decreases in higher levels (see Eq. <xref rid="e0035" ref-type="disp-formula">(7)</xref>).</p></list-item><list-item><label><italic>w</italic></label><p>Weights the influence of the differences in radii <italic>d</italic><sub><italic>R</italic></sub> and angles <italic>d</italic><sub><italic>A</italic></sub> on the similarity of the transformation matrices. It is only relevant for out of the plane motion.</p></list-item></list></p>
      </sec>
      <sec id="s0070">
        <label>5.2</label>
        <title>Motion in the image plane</title>
        <p>The videos human 1 (640 <italic>times</italic> 480, 860 frames) and human 2 (640 <italic>times</italic> 480, 1178 frames) are self-produced and show humans undergoing articulated motion in the image plane. In each video the Kanade–Lucas–Tomasi tracker (<xref rid="b0030" ref-type="bibr">Birchfeld, 2008</xref>) is used to track corner points to supply the necessary motion information for the hierarchical grouping process.</p>
        <p>Both video sequences, human 1 and 2, show a person undergoing articulated motion in the image plane. <xref rid="t0005" ref-type="table">Table 1</xref> lists the parameters used with the two videos and the results. In <xref rid="f0050" ref-type="fig">Fig. 10</xref> the results of the spatio-temporal filtering are visualized. <xref rid="f0055" ref-type="fig">Fig. 11</xref> shows the grouping result, where each color in the triangulation represents one “rigid” part. <xref rid="f0060 f0065" ref-type="fig">Figs. 12 and 13</xref> present the grouping results in separate images.</p>
        <p>The result for <italic>experiment human</italic> 1 is ideal, meaning that each “rigid” part and the background are one vertex in the top level of the graph pyramid. For <italic>experiment human</italic> 2 the right lower arm is represented by two top vertices. Additionally, one “rigid” part of the hair and one of the left upper arm with the background are produced. The reasons for this are: (1) the relative orientation change (angle) between two “rigid” parts is smaller than the local differences due to locally non-rigid deformation (e.g. skin) or the imprecisions of the tracker and (2) the labeling into <italic>relevant</italic> and <italic>separating</italic> has to allow certain variation (see Section <xref rid="s0050" ref-type="sec">4.1</xref>). The torso is connected with the base of the chin in both sequences because during tracking the features at the base of the chin slide when the head is tilted and remain in the same position in the image, creating a <italic>relevant</italic> triangle.</p>
      </sec>
      <sec id="s0075">
        <label>5.3</label>
        <title>Motion out of the image plane</title>
        <p>The input data, videos and trajectories, for this experiments are from the benchmark used by <xref rid="b0150" ref-type="bibr">Yan et al. (2008)</xref>. All videos show articulated objects undergoing movements out of the 2D image plane (toy truck: 720 × 480, 31 frames; two cranes: 360 × 240, 61 frames; dancing: 720 × 480, 40 frames). Besides the different type of motion, these videos are significantly shorter than the self-produced sequences in Section <xref rid="s0070" ref-type="sec">5.2</xref>. Therefore, there is less motion information and the presented approach is more prone to outliers. <xref rid="t0010" ref-type="table">Table 2</xref> summarizes the parameters used for the experiments.</p>
        <p><xref rid="f0070" ref-type="fig">Fig. 14</xref> shows the <italic>relevant</italic> triangles, which are the input for the grouping process. <xref rid="f0075" ref-type="fig">Fig. 15</xref> is an overview of the grouping results for all three videos, where the identified “rigid” parts are labeled with different colors.</p>
        <p>The <italic>experiment toy truck</italic> successfully results in two “rigid” parts as in (<xref rid="b0150" ref-type="bibr">Yan et al., 2008</xref>). <xref rid="f0075" ref-type="fig">Fig. 15</xref>(a) shows the labeled triangulation including the outliers (red triangles) and <xref rid="f0080" ref-type="fig">Fig. 16</xref> the two main parts of the truck represented by their triangulation.</p>
        <p>The presented approach correctly decomposes <italic>experiment two cranes</italic> in three main parts, where the crane on the left is one “rigid” part as it does not undergo articulated motion and the crane on the right is separated in two parts as its arm (boom) with the bucket moves separately (see <xref rid="f0075" ref-type="fig">Fig. 15</xref>(b) and <xref rid="f0085" ref-type="fig">Fig. 17</xref>).</p>
        <p><xref rid="f0075" ref-type="fig">Fig. 15</xref>(c) and (d), <xref rid="f0090 f0095" ref-type="fig">Figs. 18 and 19</xref> present two results for <italic>experiment human dancing</italic>. Our approach is not able to decompose the left arm into two parts as the articulated movement is not distinctive enough. <xref rid="b0150" ref-type="bibr">Yan et al. (2008)</xref> are able to separate the object in six parts. The best result with our approach is shown in <xref rid="f0090" ref-type="fig">Fig. 18</xref> <italic>human dancing</italic> 1, where the object is separated in four parts <xref rid="f0075" ref-type="fig">Fig. 15</xref>(c)). <xref rid="f0095" ref-type="fig">Fig. 19</xref> shows a result with five parts <italic>human dancing</italic> 2, but we prefer the first result considering the symmetry of the human body.</p>
      </sec>
      <sec id="s0080">
        <label>5.4</label>
        <title>Discussion</title>
        <p>The presented method extracts a model for the “rigid” parts of a scene in an unsupervised manner. It uses no prior knowledge (no training, no knowledge about the scene and its objects), it can deal with motion in and out of the 2D image plane and it can be applied to videos with any arbitrary articulated or rigid object (i.e. human, finger, animal, basket ball…).</p>
        <p>The spatio-temporal filtering (Section <xref rid="s0050" ref-type="sec">4.1</xref>) will correctly identify the triangles belonging to “rigid” parts, if the movement of the parts relative to each other (distance variation) is larger than the local distance variation between neighboring features of the same part.</p>
        <p>The quality of the results depends on the quality of the input data, the trajectories of the features. If the trajectories are reliable and at least three features exist for each “rigid” part, the proposed framework is capable of decomposing the object. Only parts undergoing significant articulated movement different from the other parts in the object can be identified (e.g. if an arm never bends it will not be decomposed in two parts, but one part). The hierarchical grouping process is able to identify the “rigid” parts if the difference in the movement of triangles belonging to different parts is larger than the local differences due to locally non-rigid deformation (e.g. skin) or to imprecisions of the computed feature positions.</p>
      </sec>
    </sec>
    <sec id="s0085">
      <label>6</label>
      <title>Conclusion</title>
      <p>This paper presented a graph-based approach to extract a model of the “rigid” parts of articulated objects. The input consists of a video and the trajectories of corresponding salient features. A triangulated graph is used to represent the features and their spatial relationships over time. First a spatio-temporal filtering is performed which labels the triangles in the graph as <italic>relevant</italic> or <italic>separating</italic>. The <italic>relevant</italic> triangles are given as input to a hierarchical grouping process which identifies the “rigid” parts in the scene considering the motion of the triangles. Depending on the motion, a suitable representation is the orientation variation of the triangles or their transformation matrices. The hierarchical grouping is realized by building a graph pyramid, where the grouping criterion decides which triangles are grouped together and the vertices in the top level represent “rigid” parts. Promising experimental results show the potential of the approach.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="b0005">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aggarwal</surname>
              <given-names>J.K.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <article-title>Human motion analysis: A review</article-title>
          <source>CVIU</source>
          <volume>73</volume>
          <issue>3</issue>
          <year>1999</year>
          <fpage>428</fpage>
          <lpage>440</lpage>
        </element-citation>
      </ref>
      <ref id="b0010">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Alatan</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Onural</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wollborn</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mech</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tuncel</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sikora</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Image sequence analysis for emerging interactive multimedia services</article-title>
          <source>Circuits Systems Video Technol.</source>
          <volume>8</volume>
          <issue>7</issue>
          <year>1998</year>
          <fpage>802</fpage>
          <lpage>813</lpage>
        </element-citation>
      </ref>
      <ref id="b0015">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Altunbasak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Eren</surname>
              <given-names>P.E.</given-names>
            </name>
            <name>
              <surname>Tekalp</surname>
              <given-names>A.M.</given-names>
            </name>
          </person-group>
          <article-title>Region-based parametric motion segmentation using color information</article-title>
          <source>Graphical Models Image Process.</source>
          <volume>60</volume>
          <issue>1</issue>
          <year>1998</year>
          <fpage>13</fpage>
          <lpage>23</lpage>
        </element-citation>
      </ref>
      <ref id="b0020">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Artner</surname>
              <given-names>N.M.</given-names>
            </name>
            <name>
              <surname>Ion</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
          </person-group>
          <chapter-title>Rigid part decomposition in a graph pyramid</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Eduardo Bayro-Corrochano</surname>
              <given-names>J.O.E.</given-names>
            </name>
          </person-group>
          <source>The 14th Iberoamerican Congress on Pattern Recognition</source>
          <series>LNCS</series>
          <volume>vol. 5856</volume>
          <year>2009</year>
          <publisher-name>Springer</publisher-name>
          <fpage>758</fpage>
          <lpage>765</lpage>
        </element-citation>
      </ref>
      <ref id="b0025">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Artner</surname>
              <given-names>N.M.</given-names>
            </name>
            <name>
              <surname>Ion</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
          </person-group>
          <article-title>Multi-scale 2d tracking of articulated objects using hierarchical spring systems</article-title>
          <source>Pattern Recognition</source>
          <volume>44</volume>
          <issue>4</issue>
          <year>2011</year>
          <fpage>800</fpage>
          <lpage>810</lpage>
        </element-citation>
      </ref>
      <ref id="b0030">
        <mixed-citation publication-type="other">Birchfeld, S., 2008. Klt: An implementation of the kanade-lucas-tomasi feature tracker. <ext-link ext-link-type="uri" xlink:href="http://www.ces.clemson.edu/stb/klt/">http://www.ces.clemson.edu/stb/klt/</ext-link> (04.08).</mixed-citation>
      </ref>
      <ref id="b0035">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Castagno</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ebrahimi</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Kunt</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Video segmentation based on multiple features for interactive multimedia applications</article-title>
          <source>Circuits Systems Video Technol.</source>
          <volume>8</volume>
          <issue>5</issue>
          <year>1998</year>
          <fpage>562</fpage>
          <lpage>571</lpage>
        </element-citation>
      </ref>
      <ref id="b0040">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Celasun</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Tekalp</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Gokcetekin</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Harmanci</surname>
              <given-names>D.M.</given-names>
            </name>
          </person-group>
          <article-title>2-d mesh-based video object segmentation and tracking with occlusion resolution</article-title>
          <source>Signal Process. Image Comm.</source>
          <volume>16</volume>
          <issue>10</issue>
          <year>2001</year>
          <fpage>949</fpage>
          <lpage>962</lpage>
        </element-citation>
      </ref>
      <ref id="b0045">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>H.-T.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.-L.</given-names>
            </name>
            <name>
              <surname>Fuh</surname>
              <given-names>C.-S.</given-names>
            </name>
          </person-group>
          <chapter-title>Segmenting highly articulated video objects with weak-prior random forests</chapter-title>
          <source>ECCV</source>
          <year>2006</year>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Graz, Austria</publisher-loc>
          <fpage>373</fpage>
          <lpage>385</lpage>
        </element-citation>
      </ref>
      <ref id="b0050">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Conte</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Foggia</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jolion</surname>
              <given-names>J.-M.</given-names>
            </name>
            <name>
              <surname>Vento</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <chapter-title>Graph-Based Representations in Pattern Recognition</chapter-title>
          <year>2005</year>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin/ Heidelberg</publisher-loc>
          <comment>(Chapter: A Graph-Based, Multi-Resolution Algorithm for Tracking), pp. 193–202</comment>
        </element-citation>
      </ref>
      <ref id="b0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Costeira</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Kanade</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>A multibody factorization method for independently moving objects</article-title>
          <source>Internat. J. Comput. Vision</source>
          <volume>29</volume>
          <issue>3</issue>
          <year>1998</year>
          <fpage>159</fpage>
          <lpage>179</lpage>
        </element-citation>
      </ref>
      <ref id="b0060">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Drouin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hébert</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Parizeau</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Incremental discovery of object parts in video sequences</article-title>
          <source>Comput. Vision Image Understanding</source>
          <volume>110</volume>
          <year>2008</year>
          <fpage>60</fpage>
          <lpage>74</lpage>
        </element-citation>
      </ref>
      <ref id="b0065">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Felzenszwalb</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Huttenlocher</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Pictorial structures for object recognition</article-title>
          <source>IJCV</source>
          <volume>61</volume>
          <issue>1</issue>
          <year>2005</year>
          <fpage>55</fpage>
          <lpage>79</lpage>
        </element-citation>
      </ref>
      <ref id="b0070">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gavrila</surname>
              <given-names>D.M.</given-names>
            </name>
          </person-group>
          <article-title>The visual analysis of human movement: A survey</article-title>
          <source>CVIU</source>
          <volume>73</volume>
          <issue>1</issue>
          <year>1999</year>
          <fpage>82</fpage>
          <lpage>980</lpage>
        </element-citation>
      </ref>
      <ref id="b0075">
        <mixed-citation publication-type="other">Godec, M., Leistner, C., Saffari, A., Bischof, H., 2010. On-line random naive bayes for tracking. In: ICPR, IEEE, pp. 3545–3548.</mixed-citation>
      </ref>
      <ref id="b0080">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Haxhimusa</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
          </person-group>
          <chapter-title>Segmentation graph hierarchies</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Fred</surname>
              <given-names>A.L.N.</given-names>
            </name>
            <name>
              <surname>Caelli</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Duin</surname>
              <given-names>R.P.W.</given-names>
            </name>
            <name>
              <surname>Campilho</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>de Ridder</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <source>SSPR/SPR</source>
          <series>Lecture Notes in Computer Science</series>
          <volume>vol. 3138</volume>
          <year>2004</year>
          <publisher-name>Springer</publisher-name>
          <fpage>343</fpage>
          <lpage>351</lpage>
        </element-citation>
      </ref>
      <ref id="b0085">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Klein</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <chapter-title>Elementary Mathematics from an Advanced Standpoint: Geometry</chapter-title>
          <year>1939</year>
          <publisher-name>MacMillan</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </element-citation>
      </ref>
      <ref id="b0090">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
          </person-group>
          <article-title>Building irregular pyramids by dual graph contraction</article-title>
          <source>Vision, Image Signal Process.</source>
          <volume>142</volume>
          <issue>6</issue>
          <year>1995</year>
          <fpage>366</fpage>
          <lpage>374</lpage>
        </element-citation>
      </ref>
      <ref id="b0095">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
            <name>
              <surname>Haxhimusa</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Pizlo</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Langs</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Vision pyramids that do not grow too high</article-title>
          <source>PRL</source>
          <volume>26</volume>
          <issue>3</issue>
          <year>2005</year>
          <fpage>319</fpage>
          <lpage>337</lpage>
        </element-citation>
      </ref>
      <ref id="b0100">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kropatsch</surname>
              <given-names>W.G.</given-names>
            </name>
            <name>
              <surname>Haxhimusa</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ion</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <source>Applied Graph Theory in Computer Vision and Pattern Recognition</source>
          <series>Studies in Computational Intelligence</series>
          <volume>vol. 52</volume>
          <year>2007</year>
          <publisher-name>Springer</publisher-name>
          <comment>(Chapter: Multiresolution Image Segmentations in Graph Pyramids), pp. 3–42</comment>
        </element-citation>
      </ref>
      <ref id="b0105">
        <mixed-citation publication-type="other">Lauer, F., Schnörr, C., 2010. Spectral clustering of linear subspaces for motion segmentation. In: ICCV, IEEE, pp. 678–685.</mixed-citation>
      </ref>
      <ref id="b0110">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tye</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Ong</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ko</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Object segmentation with affine motion similarity measure</article-title>
          <source>Multimedia Expo</source>
          <year>2001</year>
          <fpage>841</fpage>
          <lpage>844</lpage>
        </element-citation>
      </ref>
      <ref id="b0115">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moeslund</surname>
              <given-names>T.B.</given-names>
            </name>
            <name>
              <surname>Hilton</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Krger</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>A survey of advances in vision-based human motion capture and analysis</article-title>
          <source>CVIU</source>
          <volume>104</volume>
          <issue>2–3</issue>
          <year>2006</year>
          <fpage>90</fpage>
          <lpage>126</lpage>
        </element-citation>
      </ref>
      <ref id="b0120">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nesetril</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Milková</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Nesetrilová</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Otakar boruvka on minimum spanning tree problem translation of both the 1926 papers, comments, history</article-title>
          <source>Discrete Math.</source>
          <volume>233</volume>
          <issue>1–3</issue>
          <year>2001</year>
          <fpage>3</fpage>
          <lpage>36</lpage>
        </element-citation>
      </ref>
      <ref id="b0125">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Nordberg</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zografos</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <chapter-title>Multibody motion segmentation using the geometry of 6 points in 2d images</chapter-title>
          <source>ICPR</source>
          <year>2010</year>
          <publisher-name>IEEE</publisher-name>
          <publisher-loc>Istanbul</publisher-loc>
          <fpage>1783</fpage>
          <lpage>1787</lpage>
        </element-citation>
      </ref>
      <ref id="b0130">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tekalp</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Van Beek</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Toklu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Gunsel</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Two-dimensional mesh-based visual-object representation for interactive synthetic/natural digital video</article-title>
          <source>Proc. IEEE</source>
          <volume>86</volume>
          <issue>6</issue>
          <year>1998</year>
          <fpage>1029</fpage>
          <lpage>1051</lpage>
        </element-citation>
      </ref>
      <ref id="b0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tuceryan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chorzempa</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Relative sensitivity of a family of closest-point graphs in computer vision applications</article-title>
          <source>Pattern Recognition</source>
          <volume>24</volume>
          <issue>5</issue>
          <year>1991</year>
          <fpage>361</fpage>
          <lpage>373</lpage>
        </element-citation>
      </ref>
      <ref id="b0140">
        <mixed-citation publication-type="other">Walther, T., Würtz, R.P., 2008. Learning to look at humans – what are the parts of a moving body? In: Articulated Motion and Deformable Objects, pp. 22–31.</mixed-citation>
      </ref>
      <ref id="b0145">
        <mixed-citation publication-type="other">Walther, T., Würtz, R.P., 2009. Unsupervised learning of human body parts from video footage. In: 2nd Workshop on Non-Rigid Shape Analysis and Deformable Image Alignment, pp. 336–343.</mixed-citation>
      </ref>
      <ref id="b0150">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pollefeys</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>A factorization-based approach for articulated nonrigid shape, motion and kinematic chain recovery from video</article-title>
          <source>IEEE Trans. Pattern Anal. Machine Intell.</source>
          <volume>30</volume>
          <issue>5</issue>
          <year>2008</year>
          <fpage>865</fpage>
          <lpage>877</lpage>
        </element-citation>
      </ref>
      <ref id="b0155">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yilmaz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Javed</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Object tracking: A survey</article-title>
          <source>ACM Comput. Surv.</source>
          <volume>38</volume>
          <issue>4</issue>
          <year>2006</year>
        </element-citation>
      </ref>
    </ref-list>
    <ack>
      <title>Acknowledgments</title>
      <p>This work was partially supported by the Austrian Science Fund under Grants P18716-N13 and S9103-N13. Adrian Ion was supported in part by the European Commission, under project MCEXT-025481.</p>
    </ack>
    <fn-group>
      <fn id="fn1">
        <label>1</label>
        <p>Not to be confused with the vertices of the dual of a RAG (sometimes also denoted by the term <italic>faces</italic>).</p>
      </fn>
    </fn-group>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig. 1</label>
      <caption>
        <p>Example graph pyramid for a triangulation. (a) Triangulation. (b) Associated adjacency graph, a vertex for each triangle, edges are added for triangles sharing a side. (c) Graph pyramid: contracted edges are marked with an arrow.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig. 2</label>
      <caption>
        <p>Input of framework: trajectories of independently tracked features.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig. 3</label>
      <caption>
        <p>Triangulated graph is built in first frame (dark box) and deformed over time (bright box).</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig. 4</label>
      <caption>
        <p>Advantage of analyzing the motion of features in a triangulation in comparison to single trajectories/positions. (a) Trajectories of points on the same “rigid” part can differ a lot. (b) The orientation change of triangles on a “rigid” part are very similar.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig. 5</label>
      <caption>
        <p>Orientation variation of an edge as a 1D signal.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig. 6</label>
      <caption>
        <p>Determining the similarity between two transformation matrices by applying them on a polygon inscribed in an unit circle (black polygon). Results of transformation matrices (a) <italic>T</italic><sub>1</sub> and (b) <italic>T</italic><sub>2</sub> are compared with the help of polar coordinates: radii (<italic>R</italic><sub>1</sub>, <italic>R</italic><sub>2</sub>) and angles (<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>).</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig. 7</label>
      <caption>
        <p>Spatio-temporal filtering decides which triangles are input into the grouping process (dark box) based on the deformation of the graph over time (bright box).</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig. 8</label>
      <caption>
        <p>Spatio-temporal filtering of triangles. Triangles on “rigid” parts are white and triangles connecting “rigid” parts are gray (see Section <xref rid="s0050" ref-type="sec">4.1</xref> for details on labeling).</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
    <fig id="f0045">
      <label>Fig. 9</label>
      <caption>
        <p>Grouping process. Input: filtered triangles and their motion over time (bright box). The dark triangles rotate while the bright triangles stay still. Output: graph pyramid where each top vertex represents one “rigid” part of the scene (dark box).</p>
      </caption>
      <graphic xlink:href="gr9"/>
    </fig>
    <fig id="f0050">
      <label>Fig. 10</label>
      <caption>
        <p>Triangulation with labeling. White: relevant. Gray: separating. (a) <italic>Human</italic> 1. (b) <italic>Human</italic> 2.</p>
      </caption>
      <graphic xlink:href="gr10"/>
    </fig>
    <fig id="f0055">
      <label>Fig. 11</label>
      <caption>
        <p>Grouping result of <italic>human</italic> 1 (a) and <italic>human</italic> 2 (b). Each color represent one “rigid” part. The red triangles in (b) are outliers. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr11"/>
    </fig>
    <fig id="f0060">
      <label>Fig. 12</label>
      <caption>
        <p>Grouping result of <italic>human</italic> 1, where each image shows one identified part.</p>
      </caption>
      <graphic xlink:href="gr12"/>
    </fig>
    <fig id="f0065">
      <label>Fig. 13</label>
      <caption>
        <p>Grouping result of <italic>human</italic> 2, where each image shows one identified part.</p>
      </caption>
      <graphic xlink:href="gr13"/>
    </fig>
    <fig id="f0070">
      <label>Fig. 14</label>
      <caption>
        <p>Input for out of the plane experiments (triangles labeled <italic>relevant</italic>). (a) <italic>Toy truck</italic>. (b) <italic>Two cranes</italic>. (c) <italic>Human dancing</italic>.</p>
      </caption>
      <graphic xlink:href="gr14"/>
    </fig>
    <fig id="f0075">
      <label>Fig. 15</label>
      <caption>
        <p>Grouping results of experiments <italic>toy truck</italic> (a), <italic>two cranes</italic> (b), and <italic>human dancing</italic> 1 (c) and <italic>human dancing</italic> 2 (d). Each part is labeled with a color and all outliers are colored red. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr15"/>
    </fig>
    <fig id="f0080">
      <label>Fig. 16</label>
      <caption>
        <p>The two main parts of the grouping process of <italic>toy truck</italic>.</p>
      </caption>
      <graphic xlink:href="gr16"/>
    </fig>
    <fig id="f0085">
      <label>Fig. 17</label>
      <caption>
        <p>Grouping of <italic>two cranes</italic> into three parts.</p>
      </caption>
      <graphic xlink:href="gr17"/>
    </fig>
    <fig id="f0090">
      <label>Fig. 18</label>
      <caption>
        <p>Grouping of <italic>human dancing</italic> 1 in four parts.</p>
      </caption>
      <graphic xlink:href="gr18"/>
    </fig>
    <fig id="f0095">
      <label>Fig. 19</label>
      <caption>
        <p>Grouping of <italic>human dancing</italic> 2 in five parts.</p>
      </caption>
      <graphic xlink:href="gr19"/>
    </fig>
    <table-wrap id="t0005" position="float">
      <label>Table 1</label>
      <caption>
        <p>Parameters and results for sequences human 1 and 2 (see Section <xref rid="s0070" ref-type="sec">5.2</xref>). “Ground truth” is the correct number of parts in the scene and “result” lists the outcome of the presented approach, where the numbers in brackets are (outliers/all triangles).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Sequence</th>
            <th>
              <italic>ϵ</italic>
              <sub>
                <italic>r</italic>
              </sub>
            </th>
            <th>
              <italic>β</italic>
            </th>
            <th>Ground truth</th>
            <th>Result</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Human 1</td>
            <td>20</td>
            <td>50°</td>
            <td>7</td>
            <td>7(0/180)</td>
          </tr>
          <tr>
            <td>Human 2</td>
            <td>15</td>
            <td>40°</td>
            <td>7</td>
            <td>7(2/305)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0010" position="float">
      <label>Table 2</label>
      <caption>
        <p>Parameters and results with videos out of the image plane (see Section <xref rid="s0075" ref-type="sec">5.3</xref>). <italic>w</italic> is the weight of Eq. <xref rid="e0025" ref-type="disp-formula">(5)</xref>. “Ground truth” is the correct number of parts in the scene and “result” lists the outcome of the presented approach, where the numbers in brackets are (outliers/all triangles).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Sequence</th>
            <th>
              <italic>ϵ</italic>
              <sub>
                <italic>r</italic>
              </sub>
            </th>
            <th>
              <italic>β</italic>
            </th>
            <th>
              <italic>w</italic>
            </th>
            <th>Ground truth</th>
            <th>Result</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Toy truck</td>
            <td>20</td>
            <td align="char">1.3</td>
            <td align="char">1.0</td>
            <td>2</td>
            <td>2 (9/147)</td>
          </tr>
          <tr>
            <td>Two cranes</td>
            <td>20</td>
            <td align="char">0.4</td>
            <td align="char">0.9</td>
            <td>3</td>
            <td>3 (11/134)</td>
          </tr>
          <tr>
            <td>Human dancing 1</td>
            <td>20</td>
            <td align="char">0.4</td>
            <td align="char">0.6</td>
            <td>6 (4)</td>
            <td>4 (25/366)</td>
          </tr>
          <tr>
            <td>Human dancing 2</td>
            <td>20</td>
            <td align="char">0.5</td>
            <td align="char">1.0</td>
            <td>6 (4)</td>
            <td>5 (14/366)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>