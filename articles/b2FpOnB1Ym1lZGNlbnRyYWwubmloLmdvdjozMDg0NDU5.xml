<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3084459</article-id>
      <article-id pub-id-type="pmid">20348000</article-id>
      <article-id pub-id-type="publisher-id">YNIMG7177</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2010.03.058</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Kernel regression for fMRI pattern prediction</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Chu</surname>
            <given-names>Carlton</given-names>
          </name>
          <email>chuc2@mail.nih.gov</email>
          <xref rid="af0005" ref-type="aff">a</xref>
          <xref rid="af0015" ref-type="aff">c</xref>
          <xref rid="cr0005" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ni</surname>
            <given-names>Yizhao</given-names>
          </name>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>Geoffrey</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Saunders</surname>
            <given-names>Craig J.</given-names>
          </name>
          <xref rid="af0010" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>John</given-names>
          </name>
          <xref rid="af0005" ref-type="aff">a</xref>
        </contrib>
      </contrib-group>
      <aff id="af0005"><label>a</label>Wellcome Trust Centre for Neuroimaging, UCL Institute of Neurology, London, UK</aff>
      <aff id="af0010"><label>b</label>ISIS group, Electronics and Computer Science, University of Southampton, Southampton, UK</aff>
      <aff id="af0015"><label>c</label>Section on Functional Imaging Methods, Laboratory of Brain and Cognition, National Institute of Mental Health, NIH, USA</aff>
      <author-notes>
        <corresp id="cr0005"><label>⁎</label>Corresponding author. Section on Functional Imaging Methods, Room 1D80, Building 10, National Institute of Health, 9000 Rockville Pike, Bethesda, Maryland 20892, USA. Fax: +1 301 4021370. <email>chuc2@mail.nih.gov</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>15</day>
        <month>5</month>
        <year>2011</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>15</day>
        <month>5</month>
        <year>2011</year>
      </pub-date>
      <volume>56</volume>
      <issue>2-10</issue>
      <fpage>662</fpage>
      <lpage>673</lpage>
      <history>
        <date date-type="received">
          <day>18</day>
          <month>11</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>17</day>
          <month>2</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>3</month>
          <year>2010</year>
        </date>
      </history>
      <abstract>
        <p>This paper introduces two kernel-based regression schemes to decode or predict brain states from functional brain scans as part of the <italic>Pittsburgh Brain Activity Interpretation Competition</italic> (PBAIC) 2007, in which our team was awarded first place. Our procedure involved image realignment, spatial smoothing, detrending of low-frequency drifts, and application of multivariate linear and non-linear kernel regression methods: namely kernel ridge regression (KRR) and relevance vector regression (RVR). RVR is based on a Bayesian framework, which automatically determines a sparse solution through maximization of marginal likelihood. KRR is the dual-form formulation of ridge regression, which solves regression problems with high dimensional data in a computationally efficient way. Feature selection based on prior knowledge about human brain function was also used. Post-processing by constrained deconvolution and re-convolution was used to furnish the prediction. This paper also contains a detailed description of how prior knowledge was used to fine tune predictions of specific “feature ratings,” which we believe is one of the key factors in our prediction accuracy. The impact of pre-processing was also evaluated, demonstrating that different pre-processing may lead to significantly different accuracies. Although the original work was aimed at the PBAIC, many techniques described in this paper can be generally applied to any fMRI decoding works to increase the prediction accuracy.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Kernel methods</kwd>
        <kwd>Machine learning</kwd>
        <kwd>Kernel ridge regression (KRR)</kwd>
        <kwd>fMRI prediction</kwd>
        <kwd>Automatic relevance determination (ARD)</kwd>
        <kwd>Relevance vector machines (RVM)</kwd>
        <kwd>Regression</kwd>
        <kwd>Multivariate</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <title>Introduction</title>
      <p>The purpose of this paper is to describe a general kernel regression approach to predict sensory and cognitive states from imaging data. Conventionally, functional imaging studies focus mainly on finding regions showing variation under controlled experimental stimuli. The most well-known technique is <italic>Statistical Parametric Mapping</italic> (SPM)(<xref rid="bb0045" ref-type="bibr">Friston et al., 1995</xref>). Under the assumptions of a general linear model (GLM), the time series at each voxel are modeled by a linear combination of experimental conditions and confounds (e.g. low frequency drifts). The statistical tests are later applied to the weighting of each experimental regressor to infer where contrasts (i.e. linear mixtures) of experimental effects can predict brain activity at each voxel. In other words, the objective is to detect regions of activation in the brain during tasks. Three-dimensional statistical maps are generated, which can be explained by contrasts of experimental conditions. The location of activation patterns provides insight into brain function. Alternatively, researchers may utilize the known function of those brain regions to make inferences about the particular experimental conditions.</p>
      <p>In recent years, pattern recognition and machine learning methods have been used to predict or decode an experimental variable from high-dimensional imaging data. Not all methods are truly multivariate, as some still assume independence among voxels (<xref rid="bb0130" ref-type="bibr">Shinkareva et al., 2008</xref>). In general, these studies have well-controlled experimental stimuli, and the number of conditions are limited. The performance of the classification is determined by cross-validation, which involves partitioning the data into training and testing sets. The decoding machine, or classifier, is trained using the functional images and labels indicating the corresponding experimental condition. In the test phase, the classifier returns the predicted experimental conditions using test images as input. Because the true experimental conditions are known, the predictive accuracy can be calculated. Most of the published studies that applied pattern recognition to neuroimaging data involved block stimuli with categorical conditions, such as observing different categories of image stimuli or performing different tasks (<xref rid="bb0065 bb0070 bb0110 bb0105" ref-type="bibr">Haynes and Rees, 2006; Haynes et al., 2007; Mourao-Miranda et al., 2006, 2007</xref>).</p>
      <p>For classification problems, a distinction can be made between generative and discriminative models. A generative model would describe the entire probability distribution of each of the classes of data. An alternative is to use a discriminative model, which only needs to model the conditional probability of the class memberships (<xref rid="bb0155" ref-type="bibr">Ulusoy and Bishop, 2005</xref>). Generative models are usually not the most accurate approach to use for predicting. They require more hidden variables, so marginalization over higher dimensional probability densities is needed. Empirical evidence shows that discriminative pattern recognition models usually outperform generative models in terms of their predictive accuracy (<xref rid="bb0010" ref-type="bibr">Bishop, 2006</xref>) (although one could argue there is a fine distinction between a generative model of discriminative features and a discriminative model).</p>
      <p>Discriminative models also allow different forms of question to be posed. For example, it becomes possible to estimate whether task C activates a network that is more similar to that activated by task A, or that activated by task B. By accurately characterizing the pattern of difference between A and B, it becomes possible to formulate questions in terms of this difference. More accurate characterizations of differences may also lead to tests with greater sensitivity. This has been demonstrated in studies that applied pattern recognition approaches to particular brain regions (<xref rid="bb0025 bb0060" ref-type="bibr">Eger et al., 2008; Haynes and Rees, 2005</xref>). Such work has allowed differences to be detected that could not be found by mass-univariate approaches.</p>
      <p>Most neuroimagers treat estimates of model parameters as the important findings, because these parameterize their model or question. Such studies generally involve simplified models, as these allow findings to be more easily visualised and explained.<xref rid="fn0005" ref-type="fn">1</xref> It is acknowledged that the models may depend on unlikely assumptions, but the benefits of adopting them should be evident from the literature. For example, mass-univariate statistical testing in SPM has proven to be a very powerful tool for visualising regional differences, despite the fact that it usually ignores the possibility of connections among different brain regions.</p>
      <p>Hypothesis testing usually involves a comparison between two models (null and alternate), where the aim is to reject the null hypothesis if the alternative hypothesis models the data better. More recently, model comparison approaches have been introduced into the neuroimaging field (<xref rid="bb0040" ref-type="bibr">Friston et al., 2008</xref>), whereby a number of models are compared to identify those that best model the probability density of the data. In other words, the aim is to search for the most accurate model, where the measure of accuracy essentially concerns how well it would predict new data. The <italic>Pittsburgh Brain Activity Interpretation Competition</italic> (PBAIC) 2007 (<ext-link ext-link-type="uri" xlink:href="http://www.lrdc.pitt.edu/ebc/2007/competition.html">http://www.lrdc.pitt.edu/ebc/2007/competition.html</ext-link>) allowed a comparison among a diverse range models for encoding the patterns of bold signal elicited in fMRI data by various tasks. As in the model comparison problem, it allowed the most accurate approach to be selected from a range of candidates. The benefits of PBAIC rest on being able to compare and contrast various models, of the same data, from different groups. In what follows we describe the models that we employed. The uniqueness of PBAIC 2007 was that it was formulated as a regression problem, rather than as one of classification. This is in contrast to most previous studies.</p>
    </sec>
    <sec id="s0010">
      <title>Data acquisition and experimental design</title>
      <p>This section will provide a summary of version 7 of the competition guidebook. For full details please refer to the guidebook, which can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.lrdc.pitt.edu/ebc/2007/materials.html">http://www.lrdc.pitt.edu/ebc/2007/materials.html</ext-link>. Briefly speaking, three subjects played a virtual reality game in a 3 T scanner (Siemens Allegra at Pittsburgh University). Each run was 20 min, and contained 704 scans with TR = 1.75 s. The game was a typical 3D first-person shooter (FPS) game, in which the subject navigated from the first person perspective. During the game, subjects were asked to fulfil certain tasks such as collecting weapons or fruits or taking pictures. There were intermittent periods of rests, during which the screen would turn gray with a white fixation cross, and the control from the joystick would also be paused. The task in the game also changed every few minutes from a pseudo instructor calling with a cell phone. When the subject was receiving the instruction, the control would be paused, but the subject could still see the scene. Objective feature ratings such as viewing faces, viewing dogs, and speed of motion were recorded automatically during scanning by joystick and eye trackers. However, subjective ratings were later obtained from subjects while viewing the record of their game play. To avoid confusion, “feature rating” here is analogous to the experimental conditions in standard fMRI experiments. It is also referred to as “rating” throughout this paper and in the competition guidebook. All ratings were rescaled in the 0–1 range. For the readers’ convenience, we present descriptions of the 13 mandatory feature ratings from the competition guidebook in <xref rid="t0005" ref-type="table">Table 1</xref>, with our interpretations added. Sample pictures and movies of game play can also be found in the competition webpage. It is also possible to obtain the full video of game plays for all three players by contacting ebc@pitt.edu.</p>
      <p>The scans of all three sessions were released, along with the objective and subjective ratings of only the first and second sessions (VR1, VR2). The aim of the competition was for entrants to predict the ratings from the third session (VR3). In order to score each team, the prediction accuracy of each rating was calculated from the Pearson's correlation between the predicted rating and the original rating convolved by a “canonical hemodynamic function.” In other words, the goal of the competition was to predict the convolved ratings, rather than the original ratings. There were a total of 13 mandatory ratings and 10 optional ratings for each team to predict. The final score of each team was calculated by averaging the <italic>Z</italic>-scores transformed from the correlation scores from each rating and each subject using Fisher's transform (<xref rid="bb0030" ref-type="bibr">Fisher, 1915</xref>). The motivation for using <italic>Z</italic>-scores came from the fact that correlations are very non-linear in terms of evidence against a null hypothesis of no association. Consequently, Fisher's transform makes un-remarkable correlations squash together, and 'remarkable' correlations extreme. This metric would also encourage teams to further improve their methods for those ratings that may have had the potential to achieve high predictive accuracies.</p>
    </sec>
    <sec sec-type="methods" id="s0015">
      <title>Methods</title>
      <p>A general processing pipeline is described with three major parts: (i) pre-processing, (ii) machine learning, and (iii) post-processing. However, not all the predictions of feature ratings followed this procedure; the process had some modifications to improve accuracy using knowledge about the nature of the ratings. These extra operations are explained at the end of the section.</p>
      <sec id="s0020">
        <title>Pre-processing</title>
        <p>The data were pre-processed using SPM5 (Wellcome Trust Centre for Neuroimaging, London, UK). All the scans were first realigned and resampled to remove the effects of subject motion. From a pattern recognition perspective, the variability arising from rigid body motion lies on a six-dimensional manifold embedded within the original 64 × 64 × 34 = 139264 dimensional space. Removing motion effects can be seen as a form of dimensionality reduction, which increases the similarity between scans.</p>
        <p>To further reduce dimensionality, those voxels that were, <italic>a priori</italic>, considered non-informative were removed. Blood oxygenation level dependent (BOLD) signal change is generally believed to occur mainly in gray matter, as its major cause should be the local neuronal activity (<xref rid="bb0090" ref-type="bibr">Logothetis et al., 2001</xref>). Masks defining gray matter were generated for each subject by segmenting one of the fMRI scans (<xref rid="bb0005" ref-type="bibr">Ashburner and Friston, 2005</xref>) using methods implemented in SPM5. Voxels that did not contain gray matter were set to zero in all volumes of the time series (see <xref rid="f0005" ref-type="fig">Fig. 1</xref>). One practical reason for masking out non-gray matter tissue was to accelerate the speed of kernel generation. By masking out other tissues, only 20% of the whole image is used. It may also have been possible to co-register the anatomical image with the fMRI, and identify gray matter from this. Nevertheless, functional images tend to suffer from spatial distortions, especially in the frontal region due to the air in the frontal sinus, so it may not have been possible to accurately overlay gray matter masks derived from the anatomical scans.</p>
        <p>For fMRI, signal changes due to brain activity tend to be slightly lower frequency over space than much of the noise. From a Wiener filtering perspective, the signal to noise ratio can be increase by spatially smoothing the scans. Empirically, we found that accuracy could be increased by convolving the scans with a 6 mm full width at half maximum (FWHM) Guassian kernel. Another reason for applying spatial smoothing was to suppress interpolation error due to image realignment of fMRI time series (<xref rid="bb0055" ref-type="bibr">Grootoonk et al., 2000</xref>).</p>
        <p>Low frequency drift has often been reported in fMRI time series. This drift has been attributed to physiological noise or subject motion, but few studies have been done to test this assumption (<xref rid="bb0135" ref-type="bibr">Smith et al., 1999</xref>). The drift models currently dominating fMRI analysis are linear subspaces spanned by a set of polynomial or discrete cosine transform (DCT) basis functions (<xref rid="bb0035 bb0145" ref-type="bibr">Friman et al., 2004; Tanabe et al., 2002</xref>). In our preliminary experiment, we observed that there was still a large amount of low frequency (0.0–0.0015 Hz) drift in the linearly detrended dataset provided by the PBAIC committee. Hence, we utilized DCT bases to eliminate additional low frequency drifts, as this is the default technique employed by the SPM software. Mathematically, for each voxel, the time series <bold>v</bold> = {<italic>v</italic><sub><italic>n</italic></sub>}<sub><italic>n</italic> = 0</sub><sup><italic>N</italic></sup><sup>−</sup><sup>1</sup> is collected from <italic>N</italic> time points and transformed into a frequency sequence <bold>f</bold> = {<italic>f</italic><sub><italic>l</italic></sub>}<sub><italic>l</italic> = 0</sub><sup><italic>N</italic></sup><sup>−</sup><sup>1</sup><disp-formula id="fo0005"><label>(1)</label><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>cos</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="true">]</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>After pruning the low frequency drift (i.e. frequency components less than and equal to a particular number of minimum basis sets, say <italic>L</italic>), the detrended sequence <bold>v̅</bold> = {<italic>v</italic><sub><italic>n</italic></sub>}<sub><italic>n</italic>= 0</sub><sup><italic>N</italic></sup><sup>−</sup><sup>1</sup> is obtained by the inverse transform<disp-formula id="fo0010"><label>(2)</label><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>cos</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:mi>l</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">]</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>Note that the DCT can be represented as a matrix multiplication. Let <bold>G</bold> be the <italic>N x L</italic> matrix with <inline-formula><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:mo>cos</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic>L</italic> denotes the number of the minimum DCT basis which are meant to be removed. It can be shown that the detrending operation is<disp-formula id="fo0015"><label>(3)</label><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>G</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>v</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi><mml:mi>v</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula>where the matrix <bold>R</bold>=(<bold>I</bold>−<bold>GG</bold><sup>T</sup>) is commonly known as the residual forming matrix (<xref rid="bb0040" ref-type="bibr">Friston et al., 2008</xref>). The operation can be applied to each voxel to generate the detrended scans. However, it is also feasible to use the more computationally efficient approach of directly detrending the linear kernel generated from the fMRI scans. Further details about the kernel will be explained in a later section, but generally speaking, a kernel is a matrix of similarity measures between each pair of scans. Suppose we define the input features <bold>X</bold> as an <italic>N</italic> x <italic>D</italic> matrix, which contains <italic>N</italic> input scans <bold>X</bold> = [<bold>x</bold><sub>1</sub>,… <bold>x</bold><sub><italic>N</italic></sub>] <sup><bold>T</bold></sup> and each vector <bold>x</bold><sub><italic>i</italic></sub> contains <italic>D</italic> voxels. Then, a linear kernel is defined as <italic>K</italic>(<bold>x</bold><sub><italic>i</italic></sub>,<bold>x</bold><sub><italic>j</italic></sub>) = <bold>x</bold><sub><italic>i</italic></sub><sup><bold>T</bold></sup><bold>x</bold><sub><italic>j</italic></sub><bold>,</bold> and the <italic>N</italic> x <italic>N</italic> kernel matrix <bold>K</bold> = <bold>XX</bold><sup>T</sup> can be calculated.<disp-formula id="fo0020"><label>(4)</label><alternatives><textual-form specific-use="jats-markup"><italic>K</italic><sub>detrended</sub> =  &lt; <italic>X</italic><sup><italic>T</italic></sup><italic>R</italic>, <italic>X</italic><sup><italic>T</italic></sup><italic>R</italic> &gt;  = <italic>R</italic><sup><italic>T</italic></sup><italic>X</italic><italic>X</italic><sup><italic>T</italic></sup><italic>R</italic> = <italic>R</italic><sup><italic>T</italic></sup><italic>K</italic><italic>R</italic></textual-form><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mtext>detrended</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mo>&gt;</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>R</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi><mml:mi>R</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>Notice the enormous reduction of computation as in general <italic>D</italic>&gt;&gt;<italic>N</italic>. It is also possible to apply other forms of detrending such as polynomial or piece-wise linear in this manner, as long as the detrending can be modeled as a matrix operation. So in general the residual forming matrix has the form. <bold>R</bold> = (<bold>I</bold>−<bold>CC</bold><sup>+</sup>)<bold>,</bold> where <bold>C</bold> is any matrix of basis functions that model the drift. For example a quadratic basis set will be <inline-formula><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>C</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:msup><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>N</mml:mi></mml:mtd><mml:mtd><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></inline-formula><bold>,</bold> and <bold>C</bold><sup>+</sup>=(<bold>C</bold><sup>T</sup><bold>C</bold>)<sup>−</sup><sup>1</sup><bold>C</bold><sup>T</sup> is the pseudo-inverse of <bold>C.</bold> In our experiments, we used cross-validation to determine the optimal number of basis functions. Eight basis functions seemed to give robust results and is equivalent to a high pass filter with a cut-off of around 1/352 Hz. Detrending removes a large amount of variance from the fMRI kernel (see <xref rid="f0010" ref-type="fig">Fig. 2</xref>).</p>
      </sec>
      <sec id="s0025">
        <title>Machine learning</title>
        <p>Mathematically, we denote fMRI scans as {<bold>x</bold><sub><italic>i</italic></sub>}<sub><italic>i</italic>= 1</sub><sup><italic>N</italic></sup> which are embedded in a voxel feature space <bold>x</bold> ∈ ℜ<sup><italic>D</italic></sup><bold>.</bold> <italic>N</italic> is the total number of time points, and the index <italic>i</italic> is ordered along the scanning sequence. The targets are the continuous variables of each feature rating, {<bold>t</bold><sub><italic>i</italic></sub>}<sub><italic>i</italic>= 1</sub><sup><italic>N</italic></sup><bold>.</bold> In the competition, there were 13 required ratings to predict, and 10 optional ratings. In our approach, each rating was treated independently.</p>
        <p>For kernel regression methods, instead of evaluating the parameters in the space of input features, the problem is transformed into a dual representation. In this representation, solutions are sought in the kernel space, and the complexity is bounded by the number of training samples. This greatly reduces the computational complexity for high dimensional data (<italic>D</italic>&gt;&gt;<italic>N</italic>). In addition, with an appropriate kernel function, one can map the input space into a higher dimensional feature space (<xref rid="bb0125" ref-type="bibr">Shawe-Taylor and Cristianini, 2004</xref>). It is possible that the non-linear pattern in the original input space appears linear in this higher dimensional feature space. This is known as the kernel trick, and makes the linear regression or classification solution in the feature space equivalent to a non-linear solution in the original input space. Practically, some non-linear kernels can be directly computed from the original linear kernel. A commonly used example is the radial basis function (RBF)<disp-formula id="fo0025"><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mtext>RBF</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>exp</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msup><mml:mo stretchy="true">|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>exp</mml:mo><mml:mo>{</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Another example is the polynomial kernel<disp-formula id="fo0030"><alternatives><textual-form specific-use="jats-markup"><italic>K</italic><sub>poly</sub>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>) = (<italic>θ</italic> + <italic>x</italic><sub><italic>i</italic></sub><sup><italic>T</italic></sup><italic>x</italic><sub><italic>j</italic></sub>)<sup><italic>d</italic></sup> = (<italic>θ</italic> + <italic>K</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>))<sup><italic>d</italic></sup></textual-form><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mtext>poly</mml:mtext></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></disp-formula>where <italic>θ</italic>, <italic>d</italic>, <italic>γ</italic> are functional parameters, which are often learnt through cross-validation. Alternatively, it is also possible to set the parameters by maximising their marginal likelihood (<xref rid="bb0115" ref-type="bibr">Rasmussen, 2006</xref>). In practice, we found linear kernel to be most robust when predicting most ratings except “Arousal” and “Valence.”</p>
        <p>In the competition, we used two kernel regression methods: kernel ridge regression (KRR) and relevance vector regression (RVR).</p>
        <sec id="s0030">
          <title>Kernel ridge regression</title>
          <p>Kernel ridge regression is the dual representation of ridge regression, which is sometimes known as the linear least square regression with Tikhonov regularization. The parameters from the input space are determined by minimizing a regularized sum of squares error functions given by<disp-formula id="fo0035"><label>(5)</label><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula>where <italic>λ</italic> ≥ 0 is the regularization parameter, which is normally determined by cross-validation. Let <bold>X</bold> = [<bold>x</bold><sub>1</sub>,… <bold>x</bold><sub><italic>N</italic></sub>] <sup><bold>T</bold></sup>, and <bold>t</bold>=[<italic>t</italic><sub>1</sub>,…<italic>t</italic><sub><italic>N</italic></sub>]<sup>T</sup><bold>.</bold> If we take the derivative of the objective function with respect to the parameters <bold>w</bold>, we obtain the equations (<xref rid="bb0010 bb0125" ref-type="bibr">Bishop, 2006; Shawe-Taylor and Cristianini, 2004</xref>)<disp-formula id="fo0040"><label>(6)</label><alternatives><textual-form specific-use="jats-markup"><italic>X</italic><sup><italic>T</italic></sup><italic>X</italic><italic>w</italic> + <italic>λ</italic><italic>w</italic> = (<italic>X</italic><sup><italic>T</italic></sup><italic>X</italic> + <italic>λ</italic><italic>I</italic>)<italic>w</italic> = <italic>X</italic><sup><italic>Τ</italic></sup><italic>t</italic></textual-form><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi><mml:mi>w</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mi>Τ</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></disp-formula>where <bold>I</bold> is the <italic>D x D</italic> identity matrix. In this case we can obtain the solution<disp-formula id="fo0045"><label>(7)</label><alternatives><textual-form specific-use="jats-markup"><italic>w</italic> = (<italic>X</italic><sup><italic>T</italic></sup><italic>X</italic> + <italic>λ</italic><italic>I</italic>)<sup> − 1</sup><italic>X</italic><sup><italic>Τ</italic></sup><italic>t</italic></textual-form><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mi>Τ</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></disp-formula></p>
          <p>In our problem, because the input dimension is in the order of tens of thousands, directly computing the matrix inversion becomes very difficult. Alternatively, we can rewrite Eq. <xref rid="fo0040" ref-type="disp-formula">(6)</xref> in terms of <bold>w</bold> to obtain <bold>w</bold> = <italic>λ</italic><sup>−</sup><sup>1</sup><bold>X</bold><sup>T</sup>(<bold>t</bold> − <bold>Xw</bold>) = <bold>X</bold><sup>T</sup><bold>β</bold>. This shows that <bold>w</bold> can be written as a linear combination of the training samples, <bold>w</bold> = ∑<sub><italic>i</italic> = 1</sub><sup><italic>N</italic></sup><italic>β</italic><sub><italic>i</italic></sub><bold>x</bold><sub><bold>i</bold></sub>, with <bold>β</bold> = <italic>λ</italic><sup>−</sup><sup>1</sup>(<bold>t</bold> − <bold>Xw</bold>). By substituting <bold>w</bold> with this new dual representation, it can be shown<disp-formula id="fo0050"><label>(8)</label><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⇒</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⇒</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <bold>K=XX</bold><sup>T</sup> is the kernel matrix as mentioned in the previous section. This formulation makes the computation much easier, as <italic>K</italic> is only <italic>N</italic> × <italic>N</italic>. To predict the output rating of a particular fMRI scan, the similarity measures between this scan and all the training scans are required. The prediction can be obtained by<disp-formula id="fo0055"><label>(9)</label><mml:math id="M13" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mo>*</mml:mo></mml:mstyle></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>It is also possible to see ridge regression from a probabilistic perspective (<xref rid="bb0010 bb0075" ref-type="bibr">Bishop, 2006; Hsiang, 1975</xref>). Applying Bayes’ rule leads to the posterior distribution of the parameters: <italic>p</italic>(<bold>w</bold>|<bold>t</bold>) ∝ <italic>p</italic>(<bold>t</bold>|<bold>w</bold>)<italic>p</italic>(<bold>w</bold>). If <bold>w</bold> has a shrinkage prior with zero mean and variance <italic>α</italic><sup>−</sup><sup>1</sup>, then the <italic>maximum a posteriori</italic> (MAP) solution is given by setting the derivative with respect to <bold>w</bold> to zero, to obtain<disp-formula id="fo0060"><label>(10)</label><alternatives><textual-form specific-use="jats-markup"><italic>w</italic><sub>MAP</sub> = <italic>σ</italic><sup> − 2</sup>(<italic>σ</italic><sup> − 2</sup><italic>X</italic><sup><italic>T</italic></sup><italic>X</italic> + <italic>α</italic><italic>I</italic>)<sup> − 1</sup><italic>X</italic><sup><italic>T</italic></sup><italic>t</italic></textual-form><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mtext>MAP</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></disp-formula>where <italic>σ</italic><sup>2</sup> is the noise variance, assuming the noise is zero mean Gaussian. The MAP solution is equivalent to ridge regression where <italic>λ</italic> = <italic>ασ</italic><sup>2</sup>. This interpretation, with some modification, leads to the following method.</p>
        </sec>
        <sec id="s0035">
          <title>Relevance vector regression</title>
          <p>Relevance vector regression (RVR) (<xref rid="bb0150" ref-type="bibr">Tipping, 2001</xref>) is formulated in a Bayesian framework, while the general expression takes the form of a dual formulation and treats the kernel as a set of linear basis functions.<disp-formula id="fo0065"><label>(11)</label><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>ϕ</italic><sub><italic>i</italic>,<italic>j</italic></sub> is the element in the <italic>N</italic> × <italic>N</italic> + 1 ‘design’ matrix <bold>Φ</bold> = [1, <bold>K</bold>] with <bold>K</bold> denoting the kernel matrix and 1 denoting a column of ones.</p>
          <p>Similar to the Bayesian view of ridge regression, each of the weights, <bold>β</bold>, are assigned a unique zero mean Gaussian prior. This differs from ridge regression, where all the elements of the weight have the same variance, <italic>α</italic><sup>−</sup><sup>1</sup><bold>.</bold> The RVR models the prior of <bold>β</bold> with independent variance <inline-formula><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the solution involves optimizing the marginal likelihood (type-II maximum likelihood) with respect to the vector of hyper-parameters <bold>α</bold> and a noise variance <italic>σ</italic><sup>2</sup><disp-formula id="fo0070"><label>(12)</label><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>∫</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle><mml:mi mathvariant="bold">β</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>β</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="true">|</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Φ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>A</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Φ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msup><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>exp</mml:mo><mml:mo>{</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold">Φ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>A</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle><mml:mi mathvariant="bold">Φ</mml:mi></mml:mstyle><mml:mstyle><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>}</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <bold>A</bold> = diag(<italic>α</italic><sub>0</sub>,...,<italic>α</italic><sub><italic>N</italic></sub>) is a diagonal matrix. The objective of the optimization is to find the hyper-parameters, <bold>A</bold>, <italic>σ</italic><sup>2</sup>, which maximize the “evidence” of the data. This can be achieved by expectation maximization. We refer readers to <xref rid="bb0010" ref-type="bibr">Bishop (2006)</xref> and <xref rid="bb0150" ref-type="bibr">Tipping (2001)</xref> for further details of this iterative procedure. In the training, some of the <italic>α</italic> will grow very large because <italic>α</italic> is the inverse of the variance of the prior of the parameter. A large value of <italic>α</italic> implies a small variance, a priori. Because the prior is zero mean, a parameter having extremely small variance results in its posterior probability being sharply peaked at zero. This property will prune out irrelevant columns of the design matrix, and is known as automatic relevance determination (ARD) (<xref rid="bb0095" ref-type="bibr">MacKay, 1995</xref>). Because the algorithm results a sparse solution, it means that only some of the training scans are used for prediction. Those scans are called “relevance vectors” which are analogous to “support vectors” in the SVM framework.</p>
          <p>Predictions through RVR are given by<disp-formula id="fo0075"><label>(13)</label><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <bold>μ</bold> = <italic>σ</italic><sup>−</sup><sup>2</sup>(<italic>σ</italic><sup>−</sup><sup>2</sup><bold>Φ</bold><sup><bold>T</bold></sup><bold>Φ</bold> + <bold>A</bold>)<sup>−</sup><sup>1</sup><bold>Φ</bold><sup><bold>T</bold></sup><bold>t</bold> is the posterior mean of the parameter <bold>β</bold>. Formulation <xref rid="fo0075" ref-type="disp-formula">(13)</xref> is nearly the same as Eq. <xref rid="fo0055" ref-type="disp-formula">(9)</xref>, except in the RVR setup, a bias term was included.</p>
        </sec>
      </sec>
      <sec id="s0040">
        <title>Post-processing</title>
        <p>In most cases, the range of the raw feature ratings <bold>z</bold><sub>raw</sub>, prior to convolution with the hemodynamic response function (HRF), was between zero and one. To utilize this known information, a constrained de-convolution strategy was applied (<xref rid="bb0050" ref-type="bibr">Gitelman et al., 2003</xref>). The “canonical HRF,” which the competition used to convolve the raw ratings with, was generated. The convolution can be implemented as a matrix multiplication of the raw rating by a toeplitz matrix <bold>H</bold>,<bold>t</bold> = <bold>Hz</bold><sub>raw</sub>. The objective is to recover the raw rating <bold>z</bold><sub>raw</sub> fulfilling the constraints by minimizing the sum of square loss between the re-convolved solution <bold>Hz</bold><sub>raw</sub> and the predicted rating <bold>t</bold><sub>*</sub>. Quadratic programming (the same optimization used by support vector machines) was used to de-convolve the HRF from the predictions (<bold>t</bold><sub><bold>*</bold></sub>) by<disp-formula id="fo0080"><label>(14)</label><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub></mml:munder><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold"><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub><mml:mstyle mathvariant="bold"><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msub><mml:mstyle mathvariant="bold"><mml:mo stretchy="false">)</mml:mo></mml:mstyle><mml:mo>}</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub></mml:munder><mml:mo>{</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext><mml:msup><mml:mrow/><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup></mml:msubsup><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>H</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>z</mml:mi></mml:mstyle><mml:mtext>raw</mml:mtext></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></disp-formula>subject to the constraints 0 ≤ <bold>z</bold><sub>raw</sub> ≤ 1<bold>.</bold></p>
        <p>The new predicted rating is then <bold>t</bold> = <bold>Hz</bold><sub>raw</sub>. After the re-convolution, the rating was further smoothed by a Gaussian kernel with a FWHM of three time points. This FWHM was determined empirically (<xref rid="f0015" ref-type="fig">Fig. 3</xref>).</p>
        <sec id="s0045">
          <title>Feature selection with prior knowledge</title>
          <p><italic>Ugly Duckling Theorem</italic> (<xref rid="bb0020 bb0160" ref-type="bibr">Duda et al., 2001; Watanabe, 1970</xref>) tells us that prior knowledge is essential for quantifying the similarity between things, so knowledge about human brain function was used to further increase the signal to noise ratio and suppress those features that were believed, <italic>a priori</italic>, to be less informative. It is known from the functional brain mapping literature that some cognitive functions and sensory perceptions are regionally localized. Hence, masks were used to weight the kernels when predicting the two feature ratings: “dog barking” and “interior or exterior of the building.” It was believed that most of the fMRI pattern resulting from the barking sound would be localised in auditory cortex. Similarly, the major discrimination between the inside and outside of the buildings would be the illumination differences. Therefore, a mask of visual cortex could mask out a large amount of irrelevant signal.</p>
          <p>In order to generate the mask of functional regions for all three subjects, the cytoarchitectonic maps of visual and auditory in stereotaxic space were first downloaded from the McConnell Brain Imaging Center (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/cytoarchitectonics/">http://www.bic.mni.mcgill.ca/cytoarchitectonics/</ext-link>). Then the deformation field generated by the normalization routine in SPM5 was used, but rather than warping the individual to the MNI space, the cytoarchitectonic maps in MNI space were warped so that they overlay the individual subject's fMRI data. Finally, a threshold of 0.3 was used to convert the probability maps into binary masks (<xref rid="f0020" ref-type="fig">Fig. 4</xref>).</p>
        </sec>
      </sec>
      <sec id="s0050">
        <title>Post-processing for predicting “Instruction”</title>
        <p>Because the competition scoring was based on <italic>Z</italic>-scores, we found that increasing a correlation from 0.8 to 0.9 resulted in three times as much improvement in the final scores as raising a correlation from 0.2 to 0.3. The goal was therefore to focus attention on those ratings that could be predicted reasonably well, and improve them further.</p>
        <p>It was observed that the “Instructions” ratings had seven spikes, which all had similar shapes across all subjects and sessions. It became apparent that an ad hoc model fitting strategy could be used to further improve what were already high correlations. Firstly, kernel regression was applied to predict the rating, and then the prediction was convolved with the model shape, which was generated by averaging all the spikes in all sessions of all subjects. This is equivalent to match filtering, and the peak values in the convolved ratings indicate the location where the average shape fits best. After finding the estimated peak location, the average shape was inserted (<xref rid="f0025" ref-type="fig">Fig. 5</xref>). Without this procedure, the correlation of the predicted rating was 0.8, whereas by adopting it, the final correlation reached 0.988, which increased the <italic>Z</italic>-score from 1.0986 to 2.555.</p>
        <sec id="s0055">
          <title>Temporal shifting</title>
          <p>Unlike most conventional fMRI studies, which use controlled external stimuli, some of the ratings were self-paced. These included “hits” and “velocity,” which were believed to have different HRF delays from the canonical HRF. The stringent way should be to train with ratings convolved with differently specified HRFs, but there are at least five parameters to adjust for generating a HRF using double gamma functions. For reasons of generalization and robustness, we simply applied forward or backward shifts by discrete numbers of time points (scans). The predicted rating was later inversely shifted.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s0060">
      <title>Results and discussion</title>
      <p>Overall, no single method had the best performance across all ratings. <xref rid="f0030" ref-type="fig">Fig. 6</xref> shows the correlations achieved by our final submission, which was the combination of best results from the first and second submission. For the first submission, kernel ridge regression was used, whereas for the second submission, most ratings were predicted with RVR. Individual differences also appeared to play a large part in how well we were able to predict ratings, as the a subject who performed worse or better in one rating also performed consistently worse or better in other ratings. For example, subject 1 had the worst prediction accuracy (average <italic>Z</italic>-score 0.980), especially for emotional and subjective ratings such as “Arousal,” “Valence,” “Fearful/Anxious” and “Happy.” Subject 3 had the best overall prediction accuracy, with an average <italic>Z</italic>-score of 1.142. The variation of prediction accuracy for each rating across all subjects is quite consistent, i.e. subject 1 is often the worst; this implies that accuracy is influenced by subject-specific issues. This may relate to concentration, but was most likely due to motion in the scanner. By inspecting the movement parameters generated from the realignment procedure, subject 1 clearly showed more translation and rotation than subjects 2 and 3. Our ability to predict particular ratings was clearly higher for objective ratings such as instructions, velocity and faces, than it was for subjective ratings. This may be related to the reliability of the reported ratings (many of the subjective ratings were made at a separate occasion based on episodic recall of how they felt), and that this will improve if real-time measures such as skin conductance and heart rate or subjective ratings between each block were used instead. Among objective ratings, we were able to best predict those that involved attention or required a response on the part of the subject. Thus “Instructions” required the subject to attend and comprehend, while “velocity” and “hits” required a motor response from the participant. These were followed by anthropomorphic objects such as faces and bodies. Indeed, the highest of the optional ratings (lower graph in <xref rid="f0030" ref-type="fig">Fig. 6</xref>) involved humans, such as looking at bodies and discrimination of gender. Hit ratings were highest for people. We believe this is because humans have particular object expertise in humans and recruit additional cross-modal systems such as the mirror system in the representation beyond the primary sensory modalities.</p>
      <p>As the 1st place winner in 2007 PBAIC competition, our final competition score was 0.785 which was a substantially higher than other groups. Generally speaking, our team predicted all the objective ratings well within the top 5% of the maximum correlation for the entry, and we had the best prediction over the three subjects for “Hits,” “Search People,” “Search Weapons,” “Search Fruit,” “Faces,” “Fruits Vegetables,” and “Velocity” (<xref rid="f0035" ref-type="fig">Fig. 7</xref>) Some readers may be puzzled why our team achieved near perfect prediction for “Search People,” “Search Weapons,” and “Search Fruit.” It was actually based on an ad hoc procedure which exploited flaws in the competition design. Further details may be found in the <xref rid="s0085" ref-type="sec">Supplementary material</xref>. Although our methods predicted objective ratings well, it did not perform well for the subjective ratings, which were “Arousal” and “Valence.” It is probably because our team used the entire gray matter, and results from groups who did feature selection seemed to be more accurate for those two ratings. We used a linear kernel to predict most ratings, except “Arousal” and “Valence,” which were predicted by RBF kernels. Cross-validation showed that linear kernels performed relatively poorly for those two ratings compared with RBF kernels. Sometimes, linear kernels even yielded negative scores in cross-validation. We suspect it is because linear methods are only able to model a single mode of difference, whereas non-linear models can potentially model multiple modes of variability. This may indicate that these states may be represented in the brain by several alternative networks of activity, rather than a single consistent pattern of differential activity. In addition, the reason we favoured RBF kernels rather than polynomial kernels is that polynomial kernels require two parameters, which increase the complexity of the model. Based on Ockham's Razor (<xref rid="bb0020" ref-type="bibr">Duda et al., 2001</xref>), when both non-linear kernels achieve equivalent performance, we tended to select the one requiring fewer parameters.</p>
      <p>According to the competition committee, more than 40 teams submitted their final predictions. A list of participants can be found at <ext-link ext-link-type="uri" xlink:href="http://www.lrdc.pitt.edu/ebc/2007/2007.html">http://www.lrdc.pitt.edu/ebc/2007/2007.html</ext-link>. Some teams also submitted their methodological reports on the website. The most popular approaches were RVR, SVM, ridge regression, and neural networks. There were also some potentially very interesting techniques, such as elastic net regularization, fuzzy ARTMap, and functional data analysis.</p>
      <p>In addition to winning the competition, our team made some original contributions to the methodological development of neuroimaging. We were the first group to apply RVR to fMRI data in PBAIC 2006. This sound approach also caught the attention of group from University of Maastricht, as they also applied RVR in PBAIC 2007, and won the 2nd position. The other major contribution would be the “kernel detrending” (Eq. <xref rid="fo0020" ref-type="disp-formula">(4)</xref>), which is computationally efficient. Although, similar formulations were also mentioned in (<xref rid="bb0040" ref-type="bibr">Friston et al., 2008</xref>), we realized it can also be applied to the kernel formulation. Kernel detrending is a general process for kernel methods, so people who apply SVM or other kernel algorithms can also utilize this approach.</p>
      <sec id="s0065">
        <title>Relevance vector machine vs. kernel ridge regression</title>
        <p>On average, kernel ridge regression (KRR) performed slightly better than relevance vector regression (RVR), but the results are mostly within 10% of each other. In <xref rid="t0010" ref-type="table">Table 2</xref>, we compared KRR and RVR with five different feature ratings for subject 3, using a linear kernel. In addition, the sparseness of RVR (percentage of the training scans contributing to the prediction) is presented in <xref rid="t0015" ref-type="table">Table 3</xref>. As we observed, KRR performed slightly better for most ratings. It is possible that sparse representations may not fully utilize all the information in the training set; hence pooling all the training scans would probably estimate the variance component more accurately. However, from <xref rid="t0015" ref-type="table">Table 3</xref>, RVR required less than 25% of the training data to make predictions, with less than a 10% sacrifice of accuracy. For ratings that could be predicted well, such as “Velocity” and “Faces,” the differences between RVR and KRR are only about 1%. This sparsity may be due to consistent activation patterns in the brain during the same ratings; hence the regression machine only required a subset of training data to represent such patterns.</p>
        <p>Unlike RVR, where the hyper-parameters are determined through maximization of marginal likelihood, the regularization parameter for KRR was determined empirically by cross-validation. In <xref rid="f0040" ref-type="fig">Fig. 8</xref>, the correlations obtained by training with virtual reality (VR) game 1 then testing on VR game 2, and vice versa, were evaluated with different regularization parameter for four feature ratings. The graph shows that the correlation reaches a plateau with the regularization roughly between 10<sup>2</sup> and 10<sup>5</sup>. Alternatively, it is possible to optimize the regularization parameter by maximizing the marginal likelihood, which is equivalent to the Gaussian Processes (GP) approach (<xref rid="bb0010 bb0115" ref-type="bibr">Bishop, 2006; Rasmussen, 2006</xref>).<disp-formula id="fo0085"><label>(15)</label><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo stretchy="true">|</mml:mo><mml:mstyle><mml:mi mathvariant="bold">θ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="true">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>C</mml:mi></mml:mstyle><mml:msup><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>exp</mml:mo><mml:mo>{</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>T</mml:mi></mml:mstyle></mml:msup><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>C</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>}</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:mi>C</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>I</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mstyle mathvariant="bold"><mml:mi>K</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>This equation is a generalized version of Eq. <xref rid="fo0075" ref-type="disp-formula">(13)</xref> for RVR, and the vector <bold>θ</bold> contains hyper-parameters, which the algorithm optimizes. The regularization parameter for ridge regression is simply obtained by <italic>λ</italic> = <italic>θ</italic><sub>1</sub>/<italic>θ</italic><sub>2</sub>. Intriguingly, in <xref rid="f0040" ref-type="fig">Fig. 8</xref>, it seemed the regularization determined by maximizing marginal likelihood was over-regularized, and the results were not very desirable. This demonstrates the importance of well specified models to the application of Bayesian techniques, and could explain the better performance of KRR with respect to RVR. If a good model structure is not accurately known, then cross-validation may allow more accurate tuning of various hyper-parameters than the Bayesian evidence framework. In our case, the less accurate solution found by GP may be due to several factors. First, no temporal autocorrelations were modeled, whereas the actual noise for fMRI data is not independent and identically distributed (iid). The evidence framework may have found more accurate hyper-parameter estimates if auto-correlations had included in the model. Second, the objective function for maximizing the marginal likelihood is based on sum of the squares differences, which may have different characteristic from Pearson's correlation coefficient. Third, a proper covariance matrix <bold>C</bold> should contain a constant term <bold>C</bold> = <italic>θ</italic><sub>1</sub><bold>I</bold> + <italic>θ</italic><sub>2</sub><bold>K</bold> + <italic>θ</italic><sub>3</sub>. Empirical investigations showed that including the constant term improved the correlation to around the same accuracy as the plateau in the cross-validation plot.</p>
      </sec>
      <sec id="s0070">
        <title>Importance of pre-processing</title>
        <p>We believe that one of the contributing factors for our team's success was the spatial and temporal pre-processing. Spatial smoothing and temporal detrending have been shown to change the results of SPM, as well as the prediction accuracy (<xref rid="bb0080 bb0085 bb0140 bb0145" ref-type="bibr">LaConte et al., 2003, 2005; Strother, 2006; Tanabe et al., 2002</xref>). Visual inspection of the kernels (<xref rid="f0010" ref-type="fig">Fig. 2</xref>) shows that they can appear to be rather patchy. The raw kernel, without any temporal detrending, and the linear detrended kernel, both have less uniform intensities than those kernels with more low frequency components removed. Some of the pattern in the kernels is due to the fixation periods. One major reason why temporal detrending is important is because scans from the all three games were combined together. In other words, all the scans were assumed to be collected in the same session with the same intra-sessional variance. If the low frequency components dominated the major variance components, i.e. the first few principle components, the signals due to brain activations would be reduced. In <xref rid="f0045" ref-type="fig">Fig. 9</xref>, the results of cross-validation performed with four ratings for subject 2 are shown. Three different degrees of detrending were compared, as well as the result of high detrending plus spatial smoothing. These comparisons clearly show that higher detrending improves the prediction accuracy – except for “Faces.” In general though, detrending with eight DCT bases, with spatial smoothing (6 mm FWHM Gaussian) gave the best results for all four ratings. The improvement was most prominent for “Hits” and “Velocity.” For “Hits,” the correlation improved from 0.5 with no detrending or smoothing at all, to 0.8. For “Velocity” the correlation rose from 0.5 to 0.7 after high detrending and spatial smoothing. <xref rid="f0035" ref-type="fig">Fig. 7</xref> shows that our strategy performed better than other groups for those two ratings.</p>
      </sec>
      <sec id="s0075">
        <title>Visualization of the fMRI map</title>
        <p>If a linear kernel is used, it is possible to create a single summary map for a particular rating. In our experiment, this map was created directly from the weights in the feature space. <bold>w</bold> = ∑ <sub><italic>i</italic>= 1</sub><sup><italic>N</italic></sup><italic>β</italic><sub><italic>i</italic></sub><bold>x</bold><sub><bold>i</bold></sub> = <bold>X</bold><sup><bold>T</bold></sup><bold>β</bold>, which is a linear combination of all the training scans for KRR, or only the relevant scans (i.e. with non-zero weights) in the RVR framework. In other words, the prediction can be calculated from the dot-product of this weight map and the scan at a particular time point, <italic>t</italic><sub><italic>j</italic></sub> = &lt; <bold>w</bold>,<bold>x</bold><sub><italic>j</italic></sub>&gt; = <bold>w</bold><sup><bold>T</bold></sup><bold>x</bold><sub><italic>j</italic></sub><bold>.</bold> If <bold>β</bold> was learnt from the detrended kernel, then the weight map is <bold>w</bold> = <bold>X</bold><sup><bold>T</bold></sup><bold>Rβ</bold>. <xref rid="f0050" ref-type="fig">Fig. 10</xref> shows the weight maps for “Faces,” “Bodies” and “Gender,” which are surprisingly similar. Common areas include those known to be activated when viewing human/humanoid biological motion, particularly the posterior superior temporal sulcus (pSTS), extending to the ventral visual streams bilaterally corresponding to the fusiform and middle occipital gyri (<xref rid="bb0100" ref-type="bibr">Morris et al., 2005</xref>).</p>
        <sec id="s0080">
          <title>Effect of temporal shift</title>
          <p>As mentioned in the <xref rid="s0015" ref-type="sec">Methods</xref> section, temporal shifting was employed to account for shorter hemodynamic delay. It was found, by cross-validation, that shifting the original training targets by one scan (1TR) earlier would yield more accurate predictions. <xref rid="t0020" ref-type="table">Table 4</xref> shows the results of cross-validation for “Hits,” “Velocity,” and “Faces.” In these three feature ratings, only “Hits” did not show consistent improvement across all three subjects. “Velocity” and “Faces” both showed increasing accuracy for all subjects. This led us to ask why measured brain activity preceding an event would appear to be more predictive. It is possible that the regions involved for those two ratings had an HRF that was considerably shorter than the HRFs of regions involved in discriminating other ratings. This seems unlikely, however, as the most relevant or highly weighted voxels were distributed across several regions of the brain and, as we subsequently show, there was a spatial shift in the most heavily weighted voxels accompanying the temporal shift in prediction. The alternative explanation is that brain activity preceding the event reflects what is subsequently recorded.</p>
          <p>The “Velocity” rating is related to the amplitude of joystick movement, so the involvement of processes underlying voluntary motor control would be expected. Motor preparation, or the readiness potential, has been known to precede onset of voluntary motor execution by over a second. This would conceivably correspond to the period of 1 TR. As expected, inspection of the weight vector in voxel space (<xref rid="f0055" ref-type="fig">Fig. 11</xref>) shows that the motor areas around M1, the supplementary motor area and cerebellum had activity positively weighted with ratings (<xref rid="bb0015" ref-type="bibr">Cunnington et al., 2002</xref>). In addition, dorsal visual areas over the occipital and parietal cortices may be associated with the visual effect of the moving background and motor attention (<xref rid="bb0120" ref-type="bibr">Rushworth et al., 2001</xref>).</p>
          <p>PBAIC provided an objective measure of accuracy with which to compare approaches for modeling fMRI data, but with more challenging patterns of noise than is typical for most fMRI studies. Accurately predicting a brain state requires a model of the pattern of activity that differentiates the brain state from others. It is interesting to note that multivariate (rather than mass-univariate) methods were able to model these patterns of brain activity most accurately. The competition also demonstrated that utilizing prior information, such as removing low frequency drifts or selecting functional regions, is the key to achieving good empirical success. Compared to these pre-processing issues, the choice of multivariate learning algorithm appears to have relatively little effect. Other useful findings from the results of PBAIC2007 were that the predictability of different ratings can be used to determine the robustness of fMRI patterns for the corresponding cognitive processes. The fact that some processes were better encoded using non-linear models may suggest that what are often considered to be the same cognitive state, may in fact be encoded in the brain by a multitude of different patterns of activity. This knowledge may serve as guidance for brain computer interfaces or real-time fMRI. The scripts and learning algorithm we implemented for the competition are available from <ext-link ext-link-type="uri" xlink:href="http://fim.nimh.nih.gov/people/chiayueh-carlton-chu">http://fim.nimh.nih.gov/people/chiayueh-carlton-chu</ext-link>.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bb0005">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Unified segmentation</article-title>
          <source>Neuroimage</source>
          <volume>26</volume>
          <year>2005</year>
          <fpage>839</fpage>
          <lpage>851</lpage>
          <pub-id pub-id-type="pmid">15955494</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0010">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bishop</surname>
              <given-names>C.B.</given-names>
            </name>
          </person-group>
          <source>Pattern recognition and machine learning</source>
          <year>2006</year>
          <publisher-name>Springer</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0015">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cunnington</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Windischberger</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Deecke</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Moser</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>The preparation and execution of self-initiated and externally-triggered movement: a study of event-related fMRI</article-title>
          <source>Neuroimage</source>
          <volume>15</volume>
          <year>2002</year>
          <fpage>373</fpage>
          <lpage>385</lpage>
          <pub-id pub-id-type="pmid">11798272</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0020">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Duda</surname>
              <given-names>R.O.</given-names>
            </name>
            <name>
              <surname>Hart</surname>
              <given-names>P.E.</given-names>
            </name>
            <name>
              <surname>Stork</surname>
              <given-names>D.G.</given-names>
            </name>
          </person-group>
          <source>Pattern Classification</source>
          <year>2001</year>
          <publisher-name>Wiley &amp; Sons</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0025">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eger</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>fMRI activity patterns in human LOC carry information about object exemplars within category</article-title>
          <source>J. Cogn. Neurosci.</source>
          <volume>20</volume>
          <year>2008</year>
          <fpage>356</fpage>
          <lpage>370</lpage>
          <pub-id pub-id-type="pmid">18275340</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0030">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fisher</surname>
              <given-names>R.A.</given-names>
            </name>
          </person-group>
          <article-title>Frequency distribution of the values of the correlation coefficient in samples of an indefinitely large population</article-title>
          <source>Biometrika</source>
          <volume>10</volume>
          <year>1915</year>
          <fpage>507</fpage>
          <lpage>521</lpage>
        </element-citation>
      </ref>
      <ref id="bb0035">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friman</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Borga</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lundberg</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Knutsson</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Detection and detrending in fMRI data analysis</article-title>
          <source>Neuroimage</source>
          <volume>22</volume>
          <year>2004</year>
          <fpage>645</fpage>
          <lpage>655</lpage>
          <pub-id pub-id-type="pmid">15193593</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0040">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Mourao-Miranda</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hulme</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian decoding of brain images</article-title>
          <source>Neuroimage</source>
          <volume>39</volume>
          <year>2008</year>
          <fpage>181</fpage>
          <lpage>205</lpage>
          <pub-id pub-id-type="pmid">17919928</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0045">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
          </person-group>
          <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>2</volume>
          <year>1995</year>
          <fpage>189</fpage>
          <lpage>210</lpage>
        </element-citation>
      </ref>
      <ref id="bb0050">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gitelman</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Modeling regional and psychophysiologic interactions in fMRI: the importance of hemodynamic deconvolution</article-title>
          <source>Neuroimage</source>
          <volume>19</volume>
          <year>2003</year>
          <fpage>200</fpage>
          <lpage>207</lpage>
          <pub-id pub-id-type="pmid">12781739</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grootoonk</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hutton</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Howseman</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Josephs</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Characterization and correction of interpolation effects in the realignment of fMRI time series</article-title>
          <source>Neuroimage</source>
          <volume>11</volume>
          <year>2000</year>
          <fpage>49</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">10686116</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0060">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Predicting the stream of consciousness from activity in human visual cortex</article-title>
          <source>Curr. Biol.</source>
          <volume>15</volume>
          <year>2005</year>
          <fpage>1301</fpage>
          <lpage>1307</lpage>
          <pub-id pub-id-type="pmid">16051174</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0065">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Decoding mental states from brain activity in humans</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <volume>7</volume>
          <year>2006</year>
          <fpage>523</fpage>
          <lpage>534</lpage>
          <pub-id pub-id-type="pmid">16791142</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0070">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Sakai</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Gilbert</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Passingham</surname>
              <given-names>R.E.</given-names>
            </name>
          </person-group>
          <article-title>Reading hidden intentions in the human brain</article-title>
          <source>Curr. Biol.</source>
          <volume>17</volume>
          <year>2007</year>
          <fpage>33</fpage>
          <lpage>328</lpage>
        </element-citation>
      </ref>
      <ref id="bb0075">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hsiang</surname>
              <given-names>T.C.</given-names>
            </name>
          </person-group>
          <article-title>A bayesian view on ridge regression</article-title>
          <source>Statistician</source>
          <volume>24</volume>
          <year>1975</year>
          <fpage>267</fpage>
          <lpage>268</lpage>
        </element-citation>
      </ref>
      <ref id="bb0080">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>LaConte</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Muley</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ashe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frutiger</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rehm</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hansen</surname>
              <given-names>L.K.</given-names>
            </name>
            <name>
              <surname>Yacoub</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Rottenberg</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Strother</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>The evaluation of preprocessing choices in single-subject BOLD fMRI using NPAIRS performance metrics</article-title>
          <source>Neuroimage</source>
          <volume>18</volume>
          <year>2003</year>
          <fpage>10</fpage>
          <lpage>27</lpage>
          <pub-id pub-id-type="pmid">12507440</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0085">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>LaConte</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Strother</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Cherkassky</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Support vector machines for temporal classification of block design fMRI data</article-title>
          <source>Neuroimage</source>
          <volume>26</volume>
          <year>2005</year>
          <fpage>317</fpage>
          <lpage>329</lpage>
          <pub-id pub-id-type="pmid">15907293</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0090">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
            <name>
              <surname>Pauls</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Augath</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Trinath</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Oeltermann</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title>
          <source>Nature</source>
          <volume>412</volume>
          <year>2001</year>
          <fpage>150</fpage>
          <lpage>157</lpage>
          <pub-id pub-id-type="pmid">11449264</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0095">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacKay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <article-title>Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks</article-title>
          <source>Netw.: Comput. Neural Syst.</source>
          <volume>6</volume>
          <year>1995</year>
          <fpage>469</fpage>
          <lpage>505</lpage>
        </element-citation>
      </ref>
      <ref id="bb0100">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Morris</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Pelphrey</surname>
              <given-names>K.A.</given-names>
            </name>
            <name>
              <surname>McCarthy</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Regional brain activation evoked when approaching a virtual human on a virtual walk</article-title>
          <source>J. Cogn. Neurosci.</source>
          <volume>17</volume>
          <year>2005</year>
          <fpage>1744</fpage>
          <lpage>1752</lpage>
          <pub-id pub-id-type="pmid">16269110</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0105">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mourao-Miranda</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic discrimination analysis: a spatial-temporal SVM</article-title>
          <source>Neuroimage</source>
          <volume>36</volume>
          <year>2007</year>
          <fpage>88</fpage>
          <lpage>99</lpage>
          <pub-id pub-id-type="pmid">17400479</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0110">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mourao-Miranda</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Reynaud</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>McGlone</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>The impact of temporal compression and space selection on SVM analysis of single-subject and multi-subject fMRI data</article-title>
          <source>Neuroimage</source>
          <volume>33</volume>
          <year>2006</year>
          <fpage>1055</fpage>
          <lpage>1065</lpage>
          <pub-id pub-id-type="pmid">17010645</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0115">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rasmussen</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <source>Gaussian Processes for Machine Learning</source>
          <year>2006</year>
        </element-citation>
      </ref>
      <ref id="bb0120">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rushworth</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Krams</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Passingham</surname>
              <given-names>R.E.</given-names>
            </name>
          </person-group>
          <article-title>The attentional role of the left parietal cortex: the distinct lateralization and localization of motor attention in the human brain</article-title>
          <source>J. Cogn. Neurosci.</source>
          <volume>13</volume>
          <year>2001</year>
          <fpage>698</fpage>
          <lpage>710</lpage>
          <pub-id pub-id-type="pmid">11506665</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0125">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Shawe-Taylor</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cristianini</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <chapter-title>Kernel methods for pattern analysis</chapter-title>
          <year>2004</year>
          <publisher-name>University Press</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0130">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shinkareva</surname>
              <given-names>S.V.</given-names>
            </name>
            <name>
              <surname>Mason</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Malave</surname>
              <given-names>V.L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Mitchell</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Just</surname>
              <given-names>M.A.</given-names>
            </name>
          </person-group>
          <article-title>Using FMRI brain activation to identify cognitive states associated with perception of tools and dwellings</article-title>
          <source>PLoS ONE</source>
          <volume>3</volume>
          <year>2008</year>
          <fpage>e1394</fpage>
          <pub-id pub-id-type="pmid">18167553</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Lewis</surname>
              <given-names>B.K.</given-names>
            </name>
            <name>
              <surname>Ruttimann</surname>
              <given-names>U.E.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>F.Q.</given-names>
            </name>
            <name>
              <surname>Sinnwell</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Duyn</surname>
              <given-names>J.H.</given-names>
            </name>
            <name>
              <surname>Frank</surname>
              <given-names>J.A.</given-names>
            </name>
          </person-group>
          <article-title>Investigation of low frequency drift in fMRI signal</article-title>
          <source>Neuroimage</source>
          <volume>9</volume>
          <year>1999</year>
          <fpage>526</fpage>
          <lpage>533</lpage>
          <pub-id pub-id-type="pmid">10329292</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0140">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Strother</surname>
              <given-names>S.C.</given-names>
            </name>
          </person-group>
          <article-title>Evaluating fMRI preprocessing pipelines</article-title>
          <source>IEEE Eng. Med. Biol. Mag.</source>
          <volume>25</volume>
          <year>2006</year>
          <fpage>27</fpage>
          <lpage>41</lpage>
          <pub-id pub-id-type="pmid">16568935</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0145">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tanabe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Tregellas</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Freedman</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>F.G.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of detrending methods for optimal fMRI preprocessing</article-title>
          <source>Neuroimage</source>
          <volume>15</volume>
          <year>2002</year>
          <fpage>902</fpage>
          <lpage>907</lpage>
          <pub-id pub-id-type="pmid">11906230</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0150">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tipping</surname>
              <given-names>M.E.</given-names>
            </name>
          </person-group>
          <article-title>Sparse Bayesian learning and the relevance vector machine</article-title>
          <source>J. Mach. Learn. Res.</source>
          <volume>1</volume>
          <year>2001</year>
          <fpage>211</fpage>
          <lpage>244</lpage>
        </element-citation>
      </ref>
      <ref id="bb0155">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ulusoy</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Bishop</surname>
              <given-names>C.M.</given-names>
            </name>
          </person-group>
          <source>Comparison of generative and discriminative techniques for object detection and classification</source>
          <year>2005</year>
        </element-citation>
      </ref>
      <ref id="bb0160">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watanabe</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Knowing and guessing. A quantitative study of inference and information</article-title>
          <source>J. Inform. Theory</source>
          <volume>16</volume>
          <year>1970</year>
          <fpage>361</fpage>
          <lpage>362</lpage>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="s0085" sec-type="supplementary-material">
      <label>Appendix A</label>
      <title>Supplementary data</title>
      <p>
        <supplementary-material content-type="local-data" id="ec0005">
          <media xlink:href="mmc1.doc" mimetype="application" mime-subtype="msword"/>
        </supplementary-material>
      </p>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>J.A. is supported by Wellcome Trust. The data collection was supported by the <italic>Experience Based Cognition Project</italic> (Walter Schneider, PI, University of Pittsburgh).</p>
    </ack>
    <fn-group>
      <fn id="fn0005">
        <label>1</label>
        <p>This simplification is distinct from Ockham's Razor, and the bias-variance trade-off involved in model selection.</p>
      </fn>
    </fn-group>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig. 1</label>
      <caption>
        <p>Gray matter mask overlaid on the original fMRI scan, where the segmentation was achieved by SPM5. Although part of the CSF was not cleanly removed, the masking did eliminate around 80% of the voxels from the original image.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig. 2</label>
      <caption>
        <p>Linear kernel with no detrending and different degrees of detrending. The raw kernel without any temporal detrending and the linear detrended kernel seem to have less uniform intensities than those kernels with more low frequency components removed.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig. 3</label>
      <caption>
        <p>The graph shows the improvement of prediction accuracy achieved by constrained deconvolution, followed by re-convolving and smoothing. On the top left graph is the blue line showing the true rating, and the green line shows the prediction. The HRF was deconvolved from this prediction, under the constraint that the results fall between zero and one (top right graph). On the bottom right, the deconvolved prediction is re-convolved with the canonical HRF. The correlation had substantial improvement. A final smoothing step (bottom left) further increased the correlation.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig. 4</label>
      <caption>
        <p>Regional masks generated from functional regions of the brain. The masks were downloaded from the McConnell Brain Imaging Centre. The auditory mask improved the prediction of “Dog,” and the visual mask improved the prediction of “Exterior/Interior” (inside or outside the buildings).</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig. 5</label>
      <caption>
        <p>Model fitting approach to boost the prediction of “Instruction” to a correlation of 0.99. The top left graph shows the original prediction. The average shape of the response to “Instruction” was generated to fit the raw prediction.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig. 6</label>
      <caption>
        <p>Prediction accuracy of our final (third) submission for all three subjects. The top graph shows the compulsory feature ratings, and the bottom graph shows the optional feature ratings.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig. 7</label>
      <caption>
        <p>This is the maximum correlation over the three subjects for each team. The result of our team is shown in the thick line. Our team predicted well for most ratings, except “Arousal” and “Valence.” This figure is originally from <ext-link ext-link-type="uri" xlink:href="http://www.ebc.pitt.edu/2007/Slides/All.ppt">http://www.ebc.pitt.edu/2007/Slides/All.ppt</ext-link>.</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig. 8</label>
      <caption>
        <p>Cross-validation results of subject 2 using kernel ridge regression (KRR) to predict four ratings – “Hits,” “Fruits Vegetable,” “Faces,” and “Velocity.” The horizontal axis indicates different amounts of regularization for KRR. The plotted line of VR1 indicates the prediction of the first session by training from the second session, and vice versa. The dot is the prediction for VR1 estimated via maximizing of marginal likelihood, and the cross is the prediction for VR2.</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
    <fig id="f0045">
      <label>Fig. 9</label>
      <caption>
        <p>The graph shows the prediction accuracy for subject 2 for predicting the first session of the VR game by training with data from the second session. Different pre-processing settings were used.</p>
      </caption>
      <graphic xlink:href="gr9"/>
    </fig>
    <fig id="f0050">
      <label>Fig. 10</label>
      <caption>
        <p>The weight map, or weight vector, in feature space shows positive weightings in posterior superior temporal sulcus (pSTS) for predicting “Gender,” “Faces,” and “Body” of subject 3. The red indicates positive weightings and the blue indicates negative weightings.</p>
      </caption>
      <graphic xlink:href="gr10"/>
    </fig>
    <fig id="f0055">
      <label>Fig. 11</label>
      <caption>
        <p>The weight map of “Velocity” for subject 3. There contain strongly positive weightings in both motor and visual areas.</p>
      </caption>
      <graphic xlink:href="gr11"/>
    </fig>
    <table-wrap id="t0005" position="float">
      <label>Table 1</label>
      <caption>
        <p>Description of feature ratings.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left">Feature rating</th>
            <th align="left">Description</th>
            <th align="left">Rating type</th>
            <th align="left">Addition interpretation and findings from our team</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">Arousal</td>
            <td align="left">How much does what is going on in the scene affect how calm the subject is.</td>
            <td align="left">Subjective (Discrete, 5 levels)</td>
            <td/>
          </tr>
          <tr>
            <td align="left">Valence</td>
            <td align="left">How positive or negative is the environment.</td>
            <td align="left">Subjective (Discrete, 5 levels)</td>
            <td/>
          </tr>
          <tr>
            <td align="left">Hits</td>
            <td align="left">Times when subject correctly picked up fruit or weapon or took picture of a pierced person.</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">It involved different cognitive functions. The subject had to first control the joystick to aim the object (motor and visual), and then click the button to pick up object or take picture (motor). A high-pitched ring was accompanied with a successful hit (auditory).</td>
          </tr>
          <tr>
            <td align="left">Search People</td>
            <td align="left">Times when subject searched for pierced people.</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">During these “periods,” subjects took pictures of people, i.e. highly correlated with the optional rating “hits people.”</td>
          </tr>
          <tr>
            <td align="left">Search Weapons</td>
            <td align="left">Times when subject searched for weapons.</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">During these “periods,” subjects took weapons from the ground, i.e. highly correlated with the optional rating “hits weapons.”</td>
          </tr>
          <tr>
            <td align="left">Search Fruit</td>
            <td align="left">Times when subject searched for fruits.</td>
            <td align="left">Computed from VR software (Binary, 2 levels, 0 or 1)</td>
            <td align="left">During these “periods,” subjects took fruits from the ground, i.e. highly correlated with the optional rating “hits fruit.”</td>
          </tr>
          <tr>
            <td align="left">Instructions</td>
            <td align="left">Times when task instructions were presented.</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">They were strong auditory stimuli, as the volumes were relatively high.</td>
          </tr>
          <tr>
            <td align="left">Dog</td>
            <td align="left">Times when dog was audible to the subject.</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">They were weak auditory stimuli, as the volumes were relatively moderate.</td>
          </tr>
          <tr>
            <td align="left">Faces</td>
            <td align="left">The degree to which faces of a pierced or unpierced person were visible to the subject.</td>
            <td align="left">Computed from eye tracker (Continuous)</td>
            <td align="left">Empirical results show that using whole brain achieved better prediction accuracy than using face selective areas or visual cortex alone.</td>
          </tr>
          <tr>
            <td align="left">Fruits Vegetables</td>
            <td align="left">The degree to which fruits or vegetables were visible to the subject.</td>
            <td align="left">Computed from eye tracker (Continuous)</td>
            <td/>
          </tr>
          <tr>
            <td align="left">Weapons Tools</td>
            <td align="left">The degree to which weapons or tools were visible to the subject.</td>
            <td align="left">Computed from eye tracker (Continuous)</td>
            <td/>
          </tr>
          <tr>
            <td align="left">Interior Exterior</td>
            <td align="left">Times when subject was inside a building (1 subject was inside, 0 = subject was outdoors).</td>
            <td align="left">Computed from VR software (Binary, 0 or 1)</td>
            <td align="left">We found that visual cortex was involved in this stimuli, because the overall luminance was generally higher outdoors than indoors.</td>
          </tr>
          <tr>
            <td align="left">Velocity</td>
            <td align="left">Velocity of the subject moving in the VR world but not interacting with an object.</td>
            <td align="left">Computed from VR software (continuous)</td>
            <td align="left">This condition should involve motor and visual functions, because subjects controlled the joysticks to move, and moving would cause motion in vision.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0010" position="float">
      <label>Table 2</label>
      <caption>
        <p>Comparing the accuracies of KRR and RVR for predicting the third session of subject 3.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Velocity</th>
            <th align="left">Hits</th>
            <th align="left">Weapons Tools</th>
            <th align="left">Fruits Vegetable</th>
            <th align="left">Faces</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">Kernel ridge regression</td>
            <td align="left">0.8277</td>
            <td>
              <bold>0.7835</bold>
            </td>
            <td>
              <bold>0.5470</bold>
            </td>
            <td>
              <bold>0.5366</bold>
            </td>
            <td>
              <bold>0.8091</bold>
            </td>
          </tr>
          <tr>
            <td align="left">RVR</td>
            <td>
              <bold>0.8309</bold>
            </td>
            <td align="left">0.7552</td>
            <td align="left">0.4998</td>
            <td align="left">0.4955</td>
            <td align="left">0.7995</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0015" position="float">
      <label>Table 3</label>
      <caption>
        <p>Sparsity measures for RVR (percentage of the training scans contributing to the prediction).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Velocity</th>
            <th align="left">Hits</th>
            <th align="left">Weapons Tools</th>
            <th align="left">Fruits Vegetable</th>
            <th align="left">Faces</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">RVR</td>
            <td align="left">21.3%</td>
            <td align="left">24.4%</td>
            <td align="left">22.8%</td>
            <td align="left">23.3%</td>
            <td align="left">18%</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0020" position="float">
      <label>Table 4</label>
      <caption>
        <p>Cross-validation results for “Hits,” “Faces” and “Velocity,” obtained by shifting the training target one time point earlier.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th rowspan="2"/>
            <th colspan="2" align="left">Subject 1<hr/></th>
            <th colspan="2" align="left">Subject 2<hr/></th>
            <th colspan="2" align="left">Subject 3<hr/></th>
          </tr>
          <tr>
            <th align="left">Predict VR1</th>
            <th align="left">Predict VR2</th>
            <th align="left">Predict VR1</th>
            <th align="left">Predict VR2</th>
            <th align="left">Predict VR1</th>
            <th align="left">Predict VR2</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="7">
              <italic>“Hits”</italic>
            </td>
          </tr>
          <tr>
            <td align="left">Original</td>
            <td align="left">0.5873</td>
            <td align="left">0.6861</td>
            <td>
              <bold>0.7427</bold>
            </td>
            <td>
              <bold>0.8030</bold>
            </td>
            <td align="left">0.6019</td>
            <td>
              <bold>0.7551</bold>
            </td>
          </tr>
          <tr>
            <td align="left">Apply shift</td>
            <td>
              <bold>0.6094</bold>
            </td>
            <td>
              <bold>0.7272</bold>
            </td>
            <td align="left">0.735</td>
            <td align="left">0.8</td>
            <td>
              <bold>0.6096</bold>
            </td>
            <td align="left">0.7341</td>
          </tr>
          <tr>
            <td colspan="7">  </td>
          </tr>
          <tr>
            <td colspan="7">
              <italic>“Faces”</italic>
            </td>
          </tr>
          <tr>
            <td align="left">Original</td>
            <td align="left">0.5538</td>
            <td align="left">0.5436</td>
            <td align="left"><bold>0.</bold>589</td>
            <td align="left"><bold>0.</bold>7521</td>
            <td align="left">0.8313</td>
            <td align="left"><bold>0.</bold>8706</td>
          </tr>
          <tr>
            <td align="left">Apply shift</td>
            <td>
              <bold>0.5549</bold>
            </td>
            <td>
              <bold>0.553</bold>
            </td>
            <td>
              <bold>0.7114</bold>
            </td>
            <td>
              <bold>0.8155</bold>
            </td>
            <td align="left">
              <bold>0. 8328</bold>
            </td>
            <td>
              <bold>0.8859</bold>
            </td>
          </tr>
          <tr>
            <td colspan="7">  </td>
          </tr>
          <tr>
            <td colspan="7">
              <italic>“Velocity”</italic>
            </td>
          </tr>
          <tr>
            <td align="left">Original</td>
            <td align="left">0.7217</td>
            <td align="left">0.7207</td>
            <td align="left">0.7010</td>
            <td align="left">0.6347</td>
            <td align="left">0.664</td>
            <td align="left">0.7022</td>
          </tr>
          <tr>
            <td align="left">Apply shift</td>
            <td>
              <bold>0.7432</bold>
            </td>
            <td>
              <bold>0.7312</bold>
            </td>
            <td>
              <bold>0.7481</bold>
            </td>
            <td>
              <bold>0.6464</bold>
            </td>
            <td>
              <bold>0.7059</bold>
            </td>
            <td>
              <bold>0.7508</bold>
            </td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>