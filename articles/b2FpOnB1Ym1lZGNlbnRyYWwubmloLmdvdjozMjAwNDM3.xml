<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="brief-report">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3200437</article-id>
      <article-id pub-id-type="pmid">21864690</article-id>
      <article-id pub-id-type="publisher-id">YNIMG8516</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2011.07.039</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Technical Note</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Comparing Dynamic Causal Models using AIC, BIC and Free Energy</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Penny</surname>
            <given-names>W.D.</given-names>
          </name>
          <email>w.penny@fil.ion.ucl.ac.uk</email>
          <xref rid="cr0005" ref-type="corresp">⁎</xref>
        </contrib>
      </contrib-group>
      <aff id="af0005">Wellcome Trust Centre for Neuroimaging, University College, London WC1N 3BG, UK</aff>
      <author-notes>
        <corresp id="cr0005"><label>⁎</label>Fax: + 44 207 813 1420. <email>w.penny@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>02</day>
        <month>1</month>
        <year>2012</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>02</day>
        <month>1</month>
        <year>2012</year>
      </pub-date>
      <volume>59</volume>
      <issue>1</issue>
      <fpage>319</fpage>
      <lpage>330</lpage>
      <history>
        <date date-type="received">
          <day>23</day>
          <month>6</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>7</day>
          <month>7</month>
          <year>2011</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2012 Elsevier Inc.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>In neuroimaging it is now becoming standard practise to fit multiple models to data and compare them using a model selection criterion. This is especially prevalent in the analysis of brain connectivity. This paper describes a simulation study which compares the relative merits of three model selection criteria (i) Akaike's Information Criterion (AIC), (ii) the Bayesian Information Criterion (BIC) and (iii) the variational Free Energy. Differences in performance are examined in the context of General Linear Models (GLMs) and Dynamic Causal Models (DCMs). We find that the Free Energy has the best model selection ability and recommend it be used for comparison of DCMs.</p>
      </abstract>
      <abstract abstract-type="graphical">
        <title>Highlights</title>
        <p>► The Free Energy is a better model comparison criterion than AIC or BIC ► The complexity of a model is not usefully characterised by the number of parameters. ► Empirical Bayesian estimation of prior variances would improve model comparison.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Bayesian</kwd>
        <kwd>Model comparison</kwd>
        <kwd>Brain connectivity</kwd>
        <kwd>Dynamic Causal Modelling</kwd>
        <kwd>fMRI</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <title>Introduction</title>
      <p>Mathematical models of scientific data can be formally compared using the Bayesian model evidence (<xref rid="bb0020 bb0090 bb0125" ref-type="bibr">Bernardo and Smith, 2000; Gelman et al., 1995; Mackay, 2003</xref>), an approach that is now widely used in statistics (<xref rid="bb0100" ref-type="bibr">Hoeting et al., 1999</xref>), signal processing (<xref rid="bb0140" ref-type="bibr">Penny and Roberts, 2002</xref>), machine learning (<xref rid="bb0015" ref-type="bibr">Beal and Ghahramani, 2003</xref>), and neuroimaging (<xref rid="bb0080 bb0135 bb0185" ref-type="bibr">Friston et al., 2008; Penny et al., 2003; Trujillo-Barreto et al., 2004</xref>). By comparing the evidence or ‘score’ of one model relative to another, a decision can be made as to which is the more veridical. This approach has now been widely adopted for the analysis of brain connectivity data, especially in the context of Dynamic Causal Modelling (DCM) (<xref rid="bb0085 bb0150" ref-type="bibr">Friston et al., 2003; Penny et al., 2004</xref>).</p>
      <p>Originally (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>), it was proposed to score DCMs using a combination of Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC) criteria. Specifically, it was proposed that (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>) if both AIC and BIC provided a log Bayes factor (difference in log model evidences) of greater than three in favour of model one versus two, one could safely conclude that model one was the more veridical. More recently it has been proposed (<xref rid="bb0180" ref-type="bibr">Stephan et al., 2010</xref>), on theoretical grounds, to instead score DCMs using the Free Energy (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>). However, until now there has been no empirical comparison of the model comparison abilities of the different approaches.</p>
      <p>This motivates the work in this paper which describes a simulation study comparing AIC, BIC and the Free Energy. Differences in performance are examined in the context of General Linear Models (GLMs) and Dynamic Causal Models (DCMs). Specifically, for each class of model we define a ‘full’ and a ‘nested’ model, where the nested model is a special case of the full model with a subset of parameters set to zero. We examine how model comparison accuracy varies as a function of number of data points and signal to noise ratio for the separate cases of data being generated by full or nested models. This allows us to assess the sensitivity and specificity of the different model comparison criteria. The paper uses simulated data generated from models with known parameters but these parameters are derived from empirical neuroimaging data. We start by briefly reviewing the relevant theoretical background and then go on to present our results.</p>
    </sec>
    <sec sec-type="methods" id="s0010">
      <title>Methods</title>
      <p>We consider Bayesian inference on data <italic>y</italic> using model <italic>m</italic> with parameters <italic>θ</italic>. In the analysis of brain connectivity, the data would comprise, for example, fMRI time series from multiple brain regions, the model would make specific assumptions about connectivity structure, and the parameters would correspond to connections strengths. A generic approach for statistical inference in this context is Bayesian estimation (<xref rid="bb0025 bb0090" ref-type="bibr">Bishop, 2006; Gelman et al., 1995</xref>) which provides estimates of two quantities. The first is the posterior distribution over model parameters <italic>p</italic>(<italic>θ</italic>|<italic>m</italic>, <italic>y</italic>) which can be used to make inferences about model parameters <italic>θ</italic>. The second is the probability of the data given the model, otherwise known as the model evidence. This can be used for model comparison, in that ratios of model evidences (Bayes factors) allow one to choose between models (<xref rid="bb0110 bb0155" ref-type="bibr">Kass and Raftery, 1995; Raftery, 1995</xref>). This paper focusses on Dynamic Causal Models and on model inference using AIC, BIC or Free Energy approximations to the model evidence. We first describe DCM, show how model parameters are estimated, describe Bayesian inference for General Linear Models and then go on to describe the different model selection criteria. In what follows N (<italic>x</italic> ; <italic>m</italic>, <italic>S</italic>) represents a multivariate Gaussian density over <italic>x</italic> with mean <italic>m</italic> and covariance <italic>S</italic>, and |<italic>S</italic>| denotes the determinant of matrix <italic>S</italic>.</p>
      <sec id="s0015">
        <title>DCM for fMRI</title>
        <p>Dynamic Causal Modelling is a framework for fitting differential equation models of neuronal activity to brain imaging data using Bayesian inference. There is now a library of DCMs and variants differ according to their level of biological realism and the data features which they explain. The DCM approach can be applied to functional Magnetic Resonance Imaging (fMRI), Electroencephalographic (EEG), Magnetoencephalographic (MEG), and Local Field Potential (LFP) data (<xref rid="bb0055" ref-type="bibr">Daunizeau et al., 2009</xref>). The empirical work in this paper uses DCM for fMRI.</p>
        <sec id="s0020">
          <title>Neurodynamics</title>
          <p>This paper uses DCM for fMRI with bilinear neurodynamics and an extended Balloon model (<xref rid="bb0070" ref-type="bibr">Friston, 2002</xref>) for the hemodynamics. The neurodynamics are described by the following multivariate differential equation<disp-formula id="fo0005"><label>(1)</label><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mspace width="0.25em"/><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mfenced><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>where <italic>t</italic> indexes continuous time and the dot notation denotes a time derivative. The <italic>i</italic>th entry in <italic>z</italic><sub><italic>t</italic></sub> corresponds to neuronal activity in the <italic>i</italic>th brain region, and <italic>u</italic><sub><italic>t</italic></sub>(<italic>j</italic>) is the <italic>j</italic>th experimental input.</p>
          <p>A DCM is characterised by a set of ‘intrinsic connections’, <italic>A</italic>, that specify which regions are connected and whether these connections are unidirectional or bidirectional. We also define a set of input connections, <italic>C</italic>, that specify which inputs are connected to which regions, and a set of modulatory connections, <italic>B</italic><sup><italic>j</italic></sup>, that specify which intrinsic connections can be changed by which inputs. Usually, the <italic>B</italic> parameters are of greatest interest as these describe how connections between brain regions are dependent on experimental manipulations.</p>
          <p>The overall specification of input, intrinsic and modulatory connectivity comprise our assumptions about model structure. This in turn represents a scientific hypothesis about the structure of the large-scale neuronal network mediating the underlying cognitive function. These hypotheses, or models are indexed by <italic>m</italic>.</p>
          <p>The simulations in this paper use ‘DCM 8’ (available in SPM8 prior to revision 4010) with a deterministic, single-state, bilinear neurodynamical model as described above.</p>
        </sec>
        <sec id="s0025">
          <title>Model predictions</title>
          <p>In DCM, neuronal activity gives rise to fMRI signals via an extended Balloon model (<xref rid="bb0045" ref-type="bibr">Buxton et al., 2004</xref>) and BOLD signal model (<xref rid="bb0175" ref-type="bibr">Stephan et al., 2007</xref>) for each region. This specifies how changes in neuronal activity give rise to changes in blood oxygenation that are measured with fMRI. The equations for these hemodynamics are provided in the <xref rid="s0095" ref-type="sec">Appendix A</xref> and depend on a set of hemodynamic parameters <italic>h</italic>.</p>
          <p>Overall, the DCM parameters are collectively written as the vector <italic>θ</italic> = {<italic>A</italic>, <italic>B</italic>, <italic>C</italic>, <italic>h</italic>}. Numerical integration of the neurodynamic (Eq. <xref rid="fo0005" ref-type="disp-formula">1</xref>) and hemodynamic equations (<xref rid="s0095" ref-type="sec">Appendix A</xref>) leads to prediction of fMRI activity in each brain region. These values are concatenated to produce a single model prediction vector <italic>g</italic>(<italic>θ</italic>).</p>
        </sec>
        <sec id="s0030">
          <title>Priors</title>
          <p>The priors factorise over parameter types<disp-formula id="fo0010"><label>(2)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>θ</italic>|<italic>m</italic>) = <italic>p</italic>(<italic>A</italic>|<italic>m</italic>)<italic>p</italic>(<italic>B</italic>|<italic>m</italic>)<italic>p</italic>(<italic>C</italic>|<italic>m</italic>)<italic>p</italic>(<italic>h</italic>|<italic>m</italic>)</textual-form><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>and each parameter prior is Gaussian. The priors used in this paper correspond to those used in ‘DCM8’. The priors over the intrinsic connections are chosen to encourage stable dynamics. This results in prior variances which depend on the number of regions in the model (<xref rid="bb0085" ref-type="bibr">Friston et al., 2003</xref>), and in this paper we model activity in three regions. For the intrinsic self-connections we have<disp-formula id="fo0015"><label>(3)</label><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ii</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N </mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ii</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="italic">self</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>with <italic>σ</italic><sub><italic>self</italic></sub> = 0.177. The time constant associated with a self-connection is <italic>τ</italic><sub><italic>i</italic></sub> = − 1/<italic>A</italic><sub><italic>ii</italic></sub>, and the time at which activity decays to half its initial value (half-life) is (1/<italic>A</italic><sub><italic>ii</italic></sub>)<italic>log</italic>0.5 (<xref rid="bb0085" ref-type="bibr">Friston et al., 2003</xref>). The prior over self-connections corresponds to a prior over half-life's that can be determined by sampling from <italic>p</italic>(<italic>A</italic><sub><italic>ii</italic></sub>|<italic>m</italic>) and transforming variables to <italic>τ</italic><sub><italic>i</italic></sub> = − 1/<italic>A</italic><sub><italic>ii</italic></sub>. This produces a mean half life of approximately 720 ms with 90% of the distribution between 500 and 1000 ms.</p>
          <p>For those intrinsic cross connections which are not fixed at zero by model assumptions <italic>m</italic> we have<disp-formula id="fo0020"><label>(4)</label><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N </mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="italic">cross</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>σ</italic><sub><italic>cross</italic></sub> = 0.5. Elements of the modulatory and input connectivity matrices (which are not fixed at zero by model assumptions) have shrinkage priors<disp-formula id="fo0025"><label>(5)</label><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi mathvariant="italic">ik</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:malignmark/><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi mathvariant="italic">ik</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="fo0030"><label>(6)</label><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:malignmark/><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>and <italic>σ</italic><sub><italic>s</italic></sub> = 2. In the above, <italic>i</italic> and <italic>k</italic> index brain regions and <italic>j</italic> indexes experimental input.</p>
          <p>The prior variance parameters <italic>σ</italic><sub><italic>self</italic></sub><sup>2</sup>, <italic>σ</italic><sub><italic>cross</italic></sub><sup>2</sup> and <italic>σ</italic><sub><italic>s</italic></sub><sup>2</sup> along with the prior variances on hemodynamic parameters (see <xref rid="s0095" ref-type="sec">Appendix A</xref>) determine the overall prior covariance on model parameters, <italic>C</italic><sub><italic>θ</italic></sub> (see next section). In the free energy model comparison criterion (see below) these variances contribute to the penalty paid for each parameter.</p>
        </sec>
      </sec>
      <sec id="s0035">
        <title>Optimisation</title>
        <p>The standard algorithm used to optimise DCMs is the Variational Laplace (VL) method described in (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>). The VL algorithm can be used for Bayesian estimation of any nonlinear model of the form<disp-formula id="fo0035"><label>(7)</label><alternatives><textual-form specific-use="jats-markup"><italic>y</italic> = <italic>g</italic>(<italic>θ</italic>) + <italic>e</italic></textual-form><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>where <italic>g</italic>(<italic>θ</italic>) is some nonlinear function, and <italic>e</italic> is zero mean additive Gaussian noise with covariance <italic>C</italic><sub><italic>y</italic></sub>. This covariance depends on hyperparameters <italic>λ</italic> as shown below. The likelihood of the data is therefore<disp-formula id="fo0040"><label>(8)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>y</italic>|<italic>θ</italic>, <italic>λ</italic>, <italic>m</italic>) = N(<italic>y</italic>; <italic>g</italic>(<italic>θ</italic>, <italic>m</italic>), <italic>C</italic><sub><italic>y</italic></sub>)</textual-form><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>The framework allows for Gaussian priors over model parameters<disp-formula id="fo0045"><label>(9)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>θ</italic>|<italic>m</italic>) = N(<italic>θ</italic>; <italic>μ</italic><sub><italic>θ</italic></sub>, <italic>C</italic><sub><italic>θ</italic></sub>)</textual-form><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>where the prior mean and covariance are assumed known. The error covariances are assumed to decompose into terms of the form<disp-formula id="fo0050"><label>(10)</label><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mspace width="0.12em"/><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>where <italic>Q</italic><sub><italic>i</italic></sub> are known precision basis functions. The hyperparameters that govern these error precisions are collectively written as the vector <italic>λ</italic>. These will be estimated. Additionally, the hyperparameters are constrained by the prior<disp-formula id="fo0055"><label>(11)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>λ</italic>|<italic>m</italic>) = N(<italic>λ</italic>; <italic>μ</italic><sub><italic>λ</italic></sub>, <italic>C</italic><sub><italic>λ</italic></sub>)</textual-form><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>The above distributions allow one to write down an expression for the joint log likelihood of the data, parameters and hyperparameters<disp-formula id="fo0060"><label>(12)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>y</italic>, <italic>θ</italic>, <italic>λ</italic>|<italic>m</italic>) = <italic>p</italic>(<italic>y</italic>|<italic>θ</italic>, <italic>λ</italic>, <italic>m</italic>)<italic>p</italic>(<italic>θ</italic>|<italic>m</italic>)<italic>p</italic>(<italic>λ</italic>|<italic>m</italic>)</textual-form><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>The VL algorithm then assumes an approximate posterior density of the following factorised form<disp-formula id="fo0065"><label>(13)</label><mml:math id="M13" altimg="si13.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The parameters of these approximate posteriors are then iteratively updated so as to minimise the Kullback–Liebler (KL)-divergence between the true and approximate posteriors. This algorithm is described fully in (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>).</p>
        <p>We emphasise here that the Variational Laplace framework assumes that the prior means and covariances (<italic>μ</italic><sub><italic>θ</italic></sub>, <italic>C</italic><sub><italic>θ</italic></sub>, <italic>μ</italic><sub><italic>λ</italic></sub>, <italic>C</italic><sub><italic>λ</italic></sub>) are known. They are not estimated from data, as is the case for Empirical Bayes methods (<xref rid="bb0050" ref-type="bibr">Carlin and Louis, 2000</xref>). We will return to this issue in the discussion.</p>
        <sec id="s0040">
          <title>Hyperparameters in DCM for fMRI</title>
          <p>In DCM for fMRI the precision basis functions <italic>Q</italic><sub><italic>i</italic></sub>, defined in Eq. <xref rid="fo0050" ref-type="disp-formula">(10)</xref>, are set to <italic>Q</italic><sub><italic>i</italic></sub> = <italic>I</italic> for each brain region. The quantity <italic>γ</italic><sub><italic>i</italic></sub> = <italic>exp</italic>(<italic>λ</italic><sub><italic>i</italic></sub>) therefore corresponds to the noise precision in region <italic>i</italic>.</p>
          <p>The overall error covariance matrix <italic>C</italic><sub><italic>y</italic></sub> has a block structure corresponding to the assumption that observation noise is independent and identically distributed in each region. This is valid as time series data are usually pre-whitened before entering into a DCM analysis (<xref rid="bb0085" ref-type="bibr">Friston et al., 2003</xref>). The prior mean and covariance of the associated latent variables are set to<disp-formula id="fo0070"><label>(14)</label><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
          <p>This corresponds to the assumption that the mean prior noise precision, <inline-formula><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>γ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mrow></mml:math></inline-formula>. These values, along with the priors on the neurodynamic parameters, have been set so as to produce data sets with typical signal to noise ratios encountered in fMRI.</p>
        </sec>
      </sec>
      <sec id="s0045">
        <title>Model evidence</title>
        <p>The model evidence, also known as the marginal likelihood, is not straightforward to compute, since its computation involves integrating out the dependence on model parameters<disp-formula id="fo0075"><label>(15)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>y</italic>|<italic>m</italic>) = ∫∫<italic>p</italic>(<italic>y</italic>|<italic>θ</italic>, <italic>λ</italic>, <italic>m</italic>)<italic>p</italic>(<italic>θ</italic>|<italic>m</italic>)<italic>p</italic>(<italic>λ</italic>|<italic>m</italic>)<italic>d</italic><italic>θ</italic><italic>d</italic><italic>λ</italic>.</textual-form><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>λ</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>The following sections describe Free Energy, AIC and BIC approximations to the (log) model evidence. Once the evidence has been computed models <italic>m</italic><sub>1</sub> and <italic>m</italic><sub>2</sub> can be compared using the Bayes factor<disp-formula id="fo0080"><label>(16)</label><alternatives><textual-form specific-use="jats-markup"><italic>B</italic><sub>12</sub> = ½</textual-form><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>12</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula>with a value of 20 corresponding to a posterior probability of greater than 0.95 in favour of model <italic>m</italic><sub>1</sub>. The corresponding log Bayes factor is 3. The use of Bayes factors for model comparison is described more fully elsewhere (<xref rid="bb0110 bb0150" ref-type="bibr">Kass and Raftery, 1995; Penny et al., 2004</xref>). Comparison of a large number of models is best implemented using the full posterior density, <italic>p</italic>(<italic>m</italic>|<italic>y</italic>), as described in (<xref rid="bb0145" ref-type="bibr">Penny et al., 2010</xref>).</p>
      </sec>
      <sec id="s0050">
        <title>Free energy</title>
        <p>It is possible to place a lower bound on the log model evidence of the following form (<xref rid="bb0130" ref-type="bibr">Beal, 2003</xref>)<disp-formula id="fo0085"><label>(17)</label><alternatives><textual-form specific-use="jats-markup"><italic>l</italic><italic>o</italic><italic>g</italic><italic>p</italic>(<italic>y</italic>|<italic>m</italic>) = <italic>F</italic>(<italic>m</italic>) + <italic>K</italic><italic>L</italic>[<italic>q</italic>(<italic>θ</italic>, <italic>λ</italic>|<italic>m</italic>)||<italic>p</italic>(<italic>θ</italic>, <italic>λ</italic>|<italic>y</italic>, <italic>m</italic>)]</textual-form><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="true">[</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>where <italic>F(m)</italic> is known as the negative variational free energy (henceforth ‘Free Energy’) and the last term is the Kullback–Liebler distance between the true posterior density, <italic>p</italic>(<italic>θ</italic>, <italic>λ</italic>|<italic>y</italic>, <italic>m</italic>) and an approximate posterior <italic>q</italic>(<italic>θ</italic>, <italic>λ</italic>|<italic>m</italic>). Because <italic>KL</italic> is always positive (<xref rid="bb0125" ref-type="bibr">Mackay, 2003</xref>), <italic>F(m)</italic> provides a lower bound on the model evidence.</p>
        <p>The Free Energy is defined as<disp-formula id="fo0090"><label>(18)</label><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced open="[" close="]"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:mfenced><mml:mspace width="0.5em"/><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:math></disp-formula>and can be estimated using a Laplace approximation (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>), <italic>F</italic><sub><italic>L</italic></sub>(<italic>m</italic>), as derived in <xref rid="s0110" ref-type="sec">Appendix B</xref>. As noted in (<xref rid="bb0195" ref-type="bibr">Wipf and Nagarajan, 2009</xref>), because the Laplace approximation is not exactly equal to the Free Energy, the above lower bound property no longer holds. That is, the Laplace approximation does not lower bound the log model evidence. As we shall see, however, it nevertheless provides a very useful model comparison criterion. The Laplace approximation to the Free Energy is given in Eq. <xref rid="fo0285" ref-type="disp-formula">(57)</xref> and can be expressed as a sum of accuracy and complexity terms (<xref rid="bb0130" ref-type="bibr">Beal, 2003</xref>)<disp-formula id="fo0095"><label>(19)</label><alternatives><textual-form specific-use="jats-markup"><italic>F</italic><sub><italic>L</italic></sub>(<italic>m</italic>) = <italic>A</italic><italic>c</italic><italic>c</italic><italic>u</italic><italic>r</italic><italic>a</italic><italic>c</italic><italic>y</italic>(<italic>m</italic>) − <italic>C</italic><italic>o</italic><italic>m</italic><italic>p</italic><italic>l</italic><italic>e</italic><italic>x</italic><italic>i</italic><italic>t</italic><italic>y</italic>(<italic>m</italic>)</textual-form><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula><disp-formula id="fo0100"><label>(20)</label><mml:math id="M21" altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="fo0105"><label>(21)</label><mml:math id="M22" altimg="si22.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>λ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>N</italic> is the number of data points and the ‘error terms’ are<disp-formula id="fo0110"><label>(22)</label><mml:math id="M23" altimg="si23.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>λ</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>The first row of Eq. <xref rid="fo0105" ref-type="disp-formula">(21)</xref> is the complexity term for the parameters and the second row the complexity term for the hyperparameters. If the hyperparameters are known then the last row of Eq. <xref rid="fo0105" ref-type="disp-formula">(21)</xref> disappears. In this case we can write the complexity as<disp-formula id="fo0115"><label>(23)</label><mml:math id="M24" altimg="si24.gif" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>In the limit that the posterior equals the prior (<italic>e</italic><sub><italic>θ</italic></sub> = 0,<italic>C</italic><sub><italic>θ</italic></sub> = <italic>S</italic><sub><italic>θ</italic></sub>), the complexity term equals zero. The last term in Eq. <xref rid="fo0115" ref-type="disp-formula">(23)</xref>, <inline-formula><mml:math id="M25" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, is also referred to as an Occam factor (see page 349 in (<xref rid="bb0125" ref-type="bibr">Mackay, 2003</xref>)). Because the determinant of a matrix corresponds to the volume spanned by its eigenvectors, this Occam factor gets larger and the model evidence smaller as the posterior volume, |<italic>S</italic><sub><italic>θ</italic></sub>|, reduces in proportion to the prior volume, |<italic>C</italic><sub><italic>θ</italic></sub>|. Models for which parameters have to be specified precisely (small posterior volume) are brittle. They are not good models (complexity is high).</p>
        <p>The above considerations also apply to cases where hyperparameters are estimated. There is an additional complexity term (last line of Eq. <xref rid="fo0105" ref-type="disp-formula">21</xref>) and therefore an additional Occam factor.</p>
        <sec id="s0055">
          <title>Correlated parameters</title>
          <p>Other factors being equal, models with strong correlation in the posterior are not good models. For example, given a model with just two parameters the determinant of the posterior covariance is given by<disp-formula id="fo0120"><label>(24)</label><mml:math id="M26" altimg="si26.gif" overflow="scroll"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>where <italic>r</italic> is the posterior correlation, <italic>σ</italic><sub><italic>θ</italic><sub>1</sub></sub> and <italic>σ</italic><sub><italic>θ</italic><sub>2</sub></sub> are the posterior standard deviations of the two parameters. For the case of two parameters having a similar effect on model predictions the posterior correlation will be high, therefore implying a large complexity penalty.</p>
          <p>However, there is another factor at play. This is that neither parameter will be estimated accurately (the posterior variances will be high). This second factor can offset the higher complexity due to correlation and can lead to a situation in which additional extraneous parameters will not lead to a significant drop in free energy. One would then appeal to a further Occam's Razor principle (<xref rid="bb0125" ref-type="bibr">Mackay, 2003</xref>), namely, that in the absence of significant free energy differences one should prefer the simpler model (see <xref rid="s0090" ref-type="sec">Discussion</xref>).</p>
          <p>When fitting DCMs to fMRI data it is likely that some parameters will be correlated with each other. This correlation can be examined by looking at the posterior covariance matrix <italic>S</italic><sub><italic>θ</italic></sub>. A good example of this is provided in Fig. 6 of <xref rid="bb0175" ref-type="bibr">Stephan et al. (2007)</xref> who describe posterior correlations among hemodynamic and connectivity parameters. Importantly, these correlations are accomodated in the Free Energy model comparison criterion (see Eq. <xref rid="fo0115" ref-type="disp-formula">23</xref> and above). This is possible because Variational Laplace does not assume that parameters are a posteriori independent among themselves, rather it is assumed that the parameters are a posteriori independent of the hyperparameters (see Eq. <xref rid="fo0065" ref-type="disp-formula">13</xref>).</p>
        </sec>
        <sec id="s0060">
          <title>Decompositions</title>
          <p>It is instructive to decompose approximations to the model evidence into contributions from specific sets of parameters or predictions. In the context of DCM, one can decompose the accuracy terms into contributions from different brain regions, as described previously (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>). This enables insight to be gained into why one model is better than another. It may be, for example, that one model predicts activity more accurately in a particular brain region.</p>
          <p>Similarly, it is possible to decompose the complexity term into contributions from different sets of parameters. If we ignore correlation among different parameter sets then the complexity is approximately<disp-formula id="fo0125"><label>(25)</label><mml:math id="M27" altimg="si27.gif" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mspace width="0.12em"/><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where <italic>j</italic> indexes the <italic>j</italic>th parameter set. In the context of DCM these could index input connections (<italic>j</italic> = 1), intrinsic connections (<italic>j</italic> = 2), modulatory connections (<italic>j</italic> = 3) etc. We will see an example of this in the <xref rid="s0075" ref-type="sec">Results</xref> section.</p>
        </sec>
        <sec id="s0065">
          <title>General Linear Models</title>
          <p>For General Linear Models (GLMs) model predictions are given by<disp-formula id="fo0130"><label>(26)</label><alternatives><textual-form specific-use="jats-markup"><italic>g</italic>(<italic>θ</italic>) = <italic>X</italic><italic>θ</italic></textual-form><mml:math id="M28" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>where <italic>X</italic> is a design matrix and <italic>θ</italic> are now regression coefficients. The posterior distribution is analytic and given by (<xref rid="bb0025" ref-type="bibr">Bishop, 2006</xref>)<disp-formula id="fo0135"><label>(27)</label><mml:math id="M29" altimg="si29.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>S</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
          <p>These parameter values can then be plugged into Eqs. <xref rid="fo0095 fo0100 fo0105 fo0110" ref-type="disp-formula">(19) to (22)</xref> to compute the Free Energy. If the hyperparameters are assumed known then the Free Energy expression in Eq. <xref rid="fo0095" ref-type="disp-formula">(19)</xref> is exactly equal to the log model evidence.That is, <italic>F</italic><sub><italic>L</italic></sub>(<italic>m</italic>) = <italic>logp</italic>(<italic>y</italic>|<italic>m</italic>). We will revisit this case in the <xref rid="s0075" ref-type="sec">Results</xref> section. If the hyperparameters are estimated then the Free Energy provides a very close approximation, as confirmed by sampling methods (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>).</p>
        </sec>
      </sec>
      <sec id="s0070">
        <title>AIC and BIC</title>
        <p>A simple approximation to the log model evidence is given by the Bayesian Information Criterion (<xref rid="bb0165" ref-type="bibr">Schwarz, 1978</xref>)<disp-formula id="fo0140"><label>(28)</label><mml:math id="M30" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></disp-formula>where <italic>p</italic> is the number of parameters, and <italic>N</italic> is the number of data points. The BIC is a special case of the Free Energy approximation that drops all terms that do not scale with the number of data points (see e.g. Appendix A2 in (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>) for a derivation). This is equivalent to the statement that BIC is equal to the Free Energy under the infinite data limit, and when the priors over parameters are flat, and the variational posterior is exact (see section 2.3 in (<xref rid="bb0010" ref-type="bibr">Attias, 1999</xref>) and page 217 in (<xref rid="bb0025" ref-type="bibr">Bishop, 2006</xref>)). In practise, as we shall see, these three requirements are almost never met and BIC will produce model comparisons that are often very different to those from the Free Energy.</p>
        <p>An alternative model selection criterion is Akaike's Information Criterion (or ‘An Information Criterion’) (<xref rid="bb0005" ref-type="bibr">Akaike, 1973</xref>)<disp-formula id="fo0145"><label>(29)</label><alternatives><textual-form specific-use="jats-markup"><italic>A</italic><italic>I</italic><italic>C</italic> = <italic>A</italic><italic>c</italic><italic>c</italic><italic>u</italic><italic>r</italic><italic>a</italic><italic>c</italic><italic>y</italic>(<italic>m</italic>) − <italic>p</italic></textual-form><mml:math id="M31" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
        <p>AIC is not a formal approximation to the model evidence but derives from information theoretic considerations. Specifically, AIC model selection will choose that model in the comparison set with minimal expected KL divergence to the true model (<xref rid="bb0005 bb0035" ref-type="bibr">Akaike, 1973; Burnham and Anderson, 2002</xref>). There are precedents in the literature, however, for using it as a surrogate for the model evidence, in order to derive a posterior density over models (<xref rid="bb0040" ref-type="bibr">Burnham and Anderson, 2004</xref>) (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>).</p>
        <p>The AIC criterion has been reported to perform poorly for small numbers of data points (<xref rid="bb0030 bb0040" ref-type="bibr">Brockwell and Davis, 2009; Burnham and Anderson, 2004</xref>). This has motivated the inclusion of a correction term<disp-formula id="fo0150"><label>(30)</label><mml:math id="M32" altimg="si32.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>known as the ‘corrected’ AIC (AICc) (<xref rid="bb0105" ref-type="bibr">Hurvich and Tsai, 1989</xref>). The AICc criterion thus penalises parameters more than does AIC. The two criteria become approximately equal for <italic>N</italic> &gt; <italic>p</italic><sup>2</sup> and identical in the limit of very large sample sizes. We note, however, that for <italic>N</italic> &lt; <italic>p</italic> + 1 the denominator in the correction term becomes negative and AICc penalises parameters less than does AIC. In the empirical work in this paper we therefore avoid this (highly unlikely) regime.</p>
        <p>In applications of AIC and BIC to DCMs (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>), the estimated parameters are taken to be equal to the posterior means <italic>m</italic><sub><italic>θ</italic></sub> and <italic>m</italic><sub><italic>λ</italic></sub>. AIC and BIC are useful approximations because one only needs to quantify the fit of the model to provide an estimate of the log-evidence. AIC and BIC are qualitatively different to the free energy approximation in that the same fixed penalty is paid for each parameter in the model.</p>
      </sec>
    </sec>
    <sec id="s0075">
      <title>Results</title>
      <sec id="s0080">
        <title>Linear models</title>
        <p>We first compare the different approximations to the model evidence using Bayesian GLMs. We define these using the following prior and likelihood<disp-formula id="fo0155"><label>(31)</label><mml:math id="M33" altimg="si33.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mspace width="1.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>X</mml:mi><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>θ</italic> is the [<italic>p</italic> × 1] vector of regression coefficients, <italic>y</italic> is the [<italic>N</italic> × 1] vector of data points, <italic>X</italic> is the [<italic>N</italic> × <italic>p</italic>] design matrix, and for the prior mean we have <italic>μ</italic><sub><italic>θ</italic></sub> = 0. For the work in this paper we assume isotropic covariance matrices<disp-formula id="fo0160"><label>(32)</label><mml:math id="M34" altimg="si34.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>σ</italic><sub><italic>p</italic></sub> and <italic>σ</italic><sub><italic>e</italic></sub> are the standard deviations of the prior and observation error. We assume that these parameters are known.</p>
        <p>We compare Bayes factors based on AIC, BIC and <italic>F</italic><sub><italic>L</italic></sub> for nested GLMs derived from an fMRI study. The fMRI data set was collected to study neuronal responses to images of faces and is available from the SPM web site (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/data/face_rep/face_rep_SPM5.html">http://www.fil.ion.ucl.ac.uk/spm/data/face_rep/face_rep_SPM5.html</ext-link>.). Each face was presented twice, and faces either belonged to familiar or unfamiliar people. This gave rise to four conditions, each of which was modelled with 3 hemodynamic basis functions (<xref rid="bb0075" ref-type="bibr">Friston et al., 2007b</xref>). For a full description of this data set and similar analyses see (<xref rid="bb0095" ref-type="bibr">Henson et al., 2002</xref>).</p>
        <p>We first define a ‘nested’ model in which only 3 of these conditions are modelled, resulting in 9 regressors. We then define a ‘full’ model as containing an extra 3 regressors from the additional condition (first response to unfamiliar faces). <xref rid="f0005" ref-type="fig">Fig. 1</xref> shows the design matrix for the full model. The design matrices for the full and nested models are therefore different, with the full model design matrix having 12 regressors and the nested model having 9 regressors.</p>
        <p>Estimated regression coefficients, <inline-formula><mml:math id="M35" altimg="si35.gif" overflow="scroll"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:math></inline-formula>, and noise variance estimates, <italic>σ̂</italic><sub><italic>e</italic></sub> = 0.73 were extracted for a voxel showing a significant overall response to faces (i.e. over all conditions). The corresponding fMRI time series comprised <italic>N</italic> = 351 values. We then created simulated data based on this observed fMRI data as follows.</p>
        <p>First, we estimated the deviation of the fitted regression coefficients about zero and set the prior SD to this value, <italic>σ</italic><sub><italic>p</italic></sub> = 6.05. This estimation was based on parameter fits from data at a single voxel. The use of a common <italic>σ</italic><sub><italic>p</italic></sub> value for all regression coefficients implies that the effects are of similar magnitude for all four conditions and all three temporal basis functions, and is a reasonable assumption. We then computed &lt; <italic>σ</italic><sub><italic>y</italic></sub> &gt;, the average signal standard deviation when drawing parameters the prior <italic>p</italic>(<italic>θ</italic>).</p>
        <p>We then produced simulated data sets where the Signal to Noise ratio<disp-formula id="fo0165"><label>(33)</label><mml:math id="M36" altimg="si36.gif" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&gt;</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula>was set to a range of values by choosing an appropriate <italic>σ</italic><sub><italic>e</italic></sub>. SNR defined in this manner can be related to the proportion of variance explained by the model, as shown in Appendix <xref rid="s0115" ref-type="sec">C</xref>. The observed fMRI data have a value of <italic>SNR</italic> = 1.3.</p>
        <p>Each simulated data set was then generated by drawing regression coefficients from their prior densities, producing model predictions <italic>g</italic> = <italic>Xθ</italic> (for both full and nested models) and adding zero mean Gaussian noise with variance <italic>σ</italic><sub><italic>e</italic></sub><sup>2</sup>.</p>
        <p>We then fitted both full and nested models to each simulated data set and estimated Bayes factors using AIC, BIC and <italic>F</italic><sub><italic>L</italic></sub>. These criteria were computed by substituting <italic>X</italic>, <italic>C</italic><sub><italic>y</italic></sub>, <italic>C</italic><sub><italic>θ</italic></sub>, and <italic>μ</italic><sub><italic>θ</italic></sub> as defined in this section into Eq. <xref rid="fo0135" ref-type="disp-formula">(27)</xref> for computing the posterior mean and covariance for linear models. The prediction errors, <italic>e</italic><sub><italic>y</italic></sub>, and parameter errors, <italic>e</italic><sub><italic>θ</italic></sub>, were then computed from Eqs. <xref rid="fo0110 fo0130" ref-type="disp-formula">(22) and (26)</xref>. We could then compute the accuracy and complexity terms using Eqs. <xref rid="fo0100 fo0105" ref-type="disp-formula">(20) and (21)</xref> (the complexity terms for <italic>λ</italic> were ignored as the observation noise variance was known for these simulations).</p>
        <p><xref rid="f0010" ref-type="fig">Fig. 2</xref> shows results for data drawn from the full model. The figure plots the log Bayes factors (differences in log model evidence) at various values of <italic>SNR</italic>, where each point in each curve was averaged over 1000 simulated data sets. At low SNRs, experimental effects should be impossible to detect. This is reflected in the Free Energy log Bayes factor which correctly asymptotes to a value of zero, indicating neither model is preferred. In this regime, however, BIC and to a lesser extent AIC both incorrectly favour the nested model. The error bars on the plots (not shown) are extremely tight in this regime, being ± 0.0001, ± 0.09 and ± 0.35 for SNRs of 0.0025, 0.029 and 0.055 respectively (averaged over the three criteria). This means we can be highly confident that <italic>F</italic><sub><italic>L</italic></sub> is unbiased but that AIC and BIC are biassed towards the nested model.</p>
        <p>The above procedure was then repeated but this time generating data from the nested model. The results are shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref> (note the broader range of SNRs plotted). In the low SNR regime, model comparison should again be impossible. This is correctly reflected in the <italic>F</italic><sub><italic>L</italic></sub> criterion with a log Bayes factor approaching zero, but not so in the AIC or BIC criteria.</p>
        <p>Finally, we examined the dependence of model comparison on the number of data points, <italic>N</italic>. We varied <italic>N</italic> over 20 values between 32 and 512 with 1000 replications at each value, using <italic>SNR</italic> = 0.5 (results were qualitatively similar for other SNRs). The results are shown in <xref rid="f0020" ref-type="fig">Fig. 4</xref> for data generated from the full model. As expected, Bayes factors increase with the number of data points. The free energy, AIC and AICc show very similar performance with <italic>F</italic><sub><italic>L</italic></sub> being slightly better at low <italic>N</italic> and AIC/AICc at high <italic>N</italic>. The BIC criterion is biassed towards the nested model.</p>
        <p><xref rid="f0025" ref-type="fig">Fig. 5</xref> shows the results for data generated from the nested model. The Bayes factors from the free energy and BIC increase with the number of data points, whereas this is not the case for AIC and AICc. We see that AIC and AICc are equivalent for large sample sizes. For small sample sizes AICc pays a larger parameter penalty. This is beneficial when the nested model is true (<xref rid="f0025" ref-type="fig">Fig. 5</xref>) but not when the full model is true (<xref rid="f0020" ref-type="fig">Fig. 4</xref>). Overall, we do not see a good reason for favouring AICc over AIC and so exclude it from subsequent model comparisons.</p>
        <p>Theory (<xref rid="bb0010" ref-type="bibr">Attias, 1999</xref>) tells us that BIC should converge to the Free Energy for large sample sizes. However, this is only the case for flat priors over parameters and if the variational posterior is correct. As we have linear models, the last requirement is met but the prior over parameters is Gaussian, rather than flat. A data set comprising 512 points is about the maximum one could hope to get from a single session of fMRI scanning (approximately 17 min with a TR of 2s). We therefore conclude that for neuroimaging applications BIC and Free Energy are likely to give different results.</p>
      </sec>
      <sec id="s0085">
        <title>DCM for fMRI</title>
        <p>We now compare the model comparison criteria using DCM for fMRI. We generate data using synthetic DCMs with known parameter values. However, to ensure the data are realistic we use parameter values that were estimated from neuroimaging data.</p>
        <p>This data derive from a previously published study on the cortical dynamics of intelligible speech (<xref rid="bb0120" ref-type="bibr">Leff et al., 2008</xref>). We used data from a single representative subject. This study applied DCM for fMRI to investigate activity among three key multimodal brain regions: the left posterior and anterior superior temporal sulci (subsequently referred to as regions P and A respectively) and pars orbitalis of the inferior frontal gyrus (region F). The aim of the study was to see how connections among regions depended on whether the auditory input was intelligible speech or time-reversed speech. Full details of the experimental paradigm and imaging parameters are available in (<xref rid="bb0120" ref-type="bibr">Leff et al., 2008</xref>). The time series which were modelled in this study comprise <italic>N</italic> = 488 data points in each of three brain regions.</p>
        <p>We focus on just two of the models considered by Leff et al. (<xref rid="bb0120" ref-type="bibr">Leff et al., 2008</xref>). These are a ‘nested’ model, which has full intrinsic connectivity with auditory input, <italic>u</italic><sub><italic>aud</italic></sub>, entering region P, and a modulatory connection from region P to F (this allows region F to be differentially responsive to intelligible versus time-reversed speech). We also define a ‘full’ model which is identical but has an additional modulatory connection from region P to A (<italic>b</italic><sub><italic>AP</italic></sub> — see below). The two networks are shown in <xref rid="f0030" ref-type="fig">Fig. 6</xref>. The two models differ in only a single connection and we chose these very similar models to make model comparison as challenging as possible.</p>
        <p>Mathematically, neurodynamics evolve according to<disp-formula id="fo0170"><label>(34)</label><mml:math id="M37" altimg="si37.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="left"><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>A</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">PP</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">PF</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">PA</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">FP</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">FF</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">FA</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">AP</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">AF</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant="italic">AA</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi mathvariant="italic">int</mml:mi></mml:msub><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>b</mml:mi><mml:mi mathvariant="italic">FP</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>b</mml:mi><mml:mi mathvariant="italic">AP</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>z</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi mathvariant="italic">aud</mml:mi></mml:msub><mml:mfenced open="[" close="]"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>c</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>u</italic><sub><italic>aud</italic></sub> is a train of auditory input spikes, <italic>u</italic><sub><italic>int</italic></sub> indicates whether the input is intelligible (<xref rid="bb0120" ref-type="bibr">Leff et al., 2008</xref>), <italic>a</italic><sub><italic>AF</italic></sub> denotes the value of the intrinsic connection from region <italic>F</italic> to <italic>A</italic>, <italic>b</italic><sub><italic>FP</italic></sub> and <italic>b</italic><sub><italic>AP</italic></sub> are the strengths of the two modulatory connections, and <italic>c</italic><sub><italic>P</italic></sub> is the strength of the input connection. For the nested DCM we have <italic>b</italic><sub><italic>AP</italic></sub> = 0.</p>
        <p>We first generated data sets from the full model over a range of SNRs as follows. To best reflect the empirical fMRI data all parameters other than the modulatory parameters were held constant. For each simulated data set the modulatory parameters were first drawn from their prior densities (see Eq. <xref rid="fo0025" ref-type="disp-formula">5</xref>). Additionally, the modulatory parameters were then constrained to be positive (by taking the absolute value) so that modulatory effects would be facilitating.</p>
        <p>Synthetic fMRI data was then generated by integrating the neurodynamic and hemodynamic equations and adding observation noise to obtain the target SNR. The SNR was defined in the same way as for the linear models, but with the signal standard deviation, &lt; <italic>σ</italic><sub><italic>y</italic></sub> &gt;, averaged over the three predicted time series (one for each brain region). The observed fMRI data have a value of SNR = 0.2. We then fitted both full and nested models to each simulated data set and estimated Bayes factors using AIC, BIC and <italic>F</italic><sub><italic>L</italic></sub>.</p>
        <p><xref rid="f0035" ref-type="fig">Fig. 7</xref> shows results for data drawn from the full model. The figure plots the log Bayes factors (differences in log model evidence) at various values of SNR, where each point in each curve was averaged over 50 simulated data sets. For these DCM simulations, the averaging was implemented using the median operator (rather than the mean) as the results were more variable than for the GLM case. The curves in <xref rid="f0035" ref-type="fig">Fig. 7</xref> show that only the Free Energy criterion is able to correctly identify the full model.</p>
        <p>The above procedure was then repeated but this time generating data from the nested model. Again, each point in each curve is the median value over 50 simulated data sets. The results are shown in <xref rid="f0040" ref-type="fig">Fig. 8</xref> (note the broader range of SNRs plotted).</p>
        <p>The results on data from the nested model are very similar to those for the GLM case (compare <xref rid="f0015 f0040" ref-type="fig">Figs. 3 and 8</xref>). The results for data from the full model, however, are not (compare <xref rid="f0010 f0035" ref-type="fig">Figs. 2 and 7</xref>), as AIC and BIC are unable to correctly identify the full model even at high SNR. In order to find out why this is the case we examined DCMs fitted to data at SNR = 2, and examined the relative contributions to the model evidence, as described in <xref rid="s0060" ref-type="sec">Decompositions</xref> section.</p>
        <p>For this high SNR scenario we found, slightly to our surprise, that the full DCMs were only slightly more accurate than the nested DCMs. Unsurprisingly, this increase in accuracy was realised in region A, which receives modulatory input in the full but not in the nested model (see <xref rid="f0030" ref-type="fig">Fig. 6</xref>). However, the main quantity driving the difference in Free Energy between full and nested DCMs was not the accuracy but rather the complexity.</p>
        <p>It turns out that the nested DCMs are able to produce a reasonable data fit by using a very large value for the intrinsic connection, <italic>a</italic><sub><italic>AF</italic></sub> (from region F to A). This connection value (typically 1.5) was about 5 times bigger than the value for a full DCM (typically 0.3). This makes sense because, in the nested model, the connection from P to F is modulated by intelligibility, and by facilitating the intrinsic connection from F to A this ‘modulatory signal’ is passed on to region A. Since this modulation is of an additive nature, this therefore crudely mimics a direct modulation of the P to A connection. However, such a strong intrinsic connection from F to A is a-priori unlikely (the prior is a zero-mean Gaussian, with standard deviation <italic>σ</italic><sub><italic>cross</italic></sub> = 0.5). The nested models are therefore heavily penalised for having such unlikely parameter values (being three standard deviations away from their prior means). Only the Free Energy criterion is sensitive to such subtleties because AIC and BIC pay the same penalty for each parameter, regardless of magnitude.</p>
        <p>As mentioned above, the empirical SNR for this data is SNR = 0.2 which is very low. Fitting the full and nested DCMs to this data yielded a Free Energy difference of only 0.11 (in favour of the full DCM). This difference is negligible, and points to the difficulty of model inference for very similar models and at low SNR (as exemplified by <xref rid="f0035 f0040" ref-type="fig">Figs. 7 and 8</xref>). In this regime it may be a better idea to make inferences over families of models (<xref rid="bb0145" ref-type="bibr">Penny et al., 2010</xref>) and to look for consistent differences over a group of subjects (<xref rid="bb0170" ref-type="bibr">Stephan et al., 2009</xref>).</p>
      </sec>
    </sec>
    <sec id="s0090">
      <title>Discussion</title>
      <p>We have described a simulation study which compared the relative merits of AIC, BIC and Free Energy model selection criteria. Differences in performance were examined in the context of GLMs and DCMs and we found that the Free Energy has the best model selection ability and recommended it be used for comparison of DCMs. Similar conclusions have been reached in earlier work comparing Free Energy with BIC in the context of non-Gaussian autoregressive modelling (<xref rid="bb0160" ref-type="bibr">Roberts and Penny, 2002</xref>) and Hidden Markov Modelling (<xref rid="bb0190" ref-type="bibr">Valente and Wellekens, 2004</xref>).</p>
      <p>The GLM simulation results showed that, at low SNR, AIC and BIC incorrectly selected nested models when data were generated by full models. At higher SNR, however, this bias disappeared and AIC/BIC showed increased sensitivity. We also investigated a corrected AIC criterion but this showed no benefit over the standard AIC measure.</p>
      <p>The DCM simulation results showed that only the Free Energy was able to correctly detect that data had been generated from the full model. By decomposing the Free Energy difference into contributions from different regions and parameters, we found that this ability was mainly due to penalising the nested model for having a very large, and a-priori unlikely, intrinsic connection from brain region F to A. Because AIC and BIC use the same complexity penalty for every parameter, and one that is not matched to prior expectations, they lack the sensitivity that is required, in this case, to infer that data was drawn from the full model.</p>
      <p>We emphasise that this will not always be the case, and AIC/BIC can in general be sensitive to ‘full model’ effects in DCMs. This is demonstrated, for example, in our previous work (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>). However, if prior information about parameter values is available then it should be used, and can be used to good effect in the Free Energy criterion.</p>
      <p>It may also be argued that in the application in this paper AIC and BIC are implicitly using prior information in that the accuracy term is computed at the maximum posterior value. Being a posterior estimate this is naturally constrained by the prior. To avoid this one would have to implement a separate Maximum Likelihood optimisation. Given this fact, it therefore seems consistent to also use prior information when approximating the evidence.</p>
      <p>According to conventions in Bayesian statistics (<xref rid="bb0110" ref-type="bibr">Kass and Raftery, 1995</xref>), and as stated above, models can be considered clearly distinguishable once the log Bayes factor exceeds three. The simulation results for both GLMs and DCMs show smaller Bayes factors when the true model is nested rather than full. This is particularly pronounced for the (challengingly similar) DCMs examined in this paper for which the Free Energy only achieves a Log Bayes Factor of three at an SNR of 10. In such a case, modellers and imaging neuroscientists should appeal to a second Occam principle (<xref rid="bb0125" ref-type="bibr">Mackay, 2003</xref>), not the numerical one embedded in the equation for the Free Energy, but a conceptual one that when two models cannot be clearly distinguished one should prefer the simpler one.</p>
      <p>In previous work (<xref rid="bb0150" ref-type="bibr">Penny et al., 2004</xref>) we have advocated the combined use of AIC and BIC criteria for the comparison of DCMs. This was motivated by a concern about how Free Energy model inference depends on the chosen values of the prior means and variances (see earlier section on priors). Specifically, the values <italic>σ</italic><sub><italic>self</italic></sub>, <italic>σ</italic><sub><italic>cross</italic></sub> and <italic>σ</italic><sub><italic>s</italic></sub> implicitly set the penalty paid for intrinsic, modulatory and input parameters (as governed by Eq. <xref rid="fo0105" ref-type="disp-formula">(21)</xref> via the overall prior covariance matrix <italic>C</italic><sub><italic>θ</italic></sub>.).</p>
      <p>This therefore motivates the future application of an Empirical Bayes (<xref rid="bb0050" ref-type="bibr">Carlin and Louis, 2000</xref>) approach which would estimate these variance parameters from data. This would effectively perform a search in the continuous space of prior variances instead of the discrete space (e.g., nested versus full) examined in this paper. Such an approach can be implemented within the new framework of post-hoc model selection (<xref rid="bb0065" ref-type="bibr">Friston and Penny, 2011</xref>).</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bb0005">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Akaike</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Information measures and model selection</article-title>
          <source>Bull. Int. Stat. Inst.</source>
          <volume>50</volume>
          <year>1973</year>
          <fpage>277</fpage>
          <lpage>290</lpage>
        </element-citation>
      </ref>
      <ref id="bb0010">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Attias</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <chapter-title>Inferring parameters and structure of latent variable models by variational Bayes</chapter-title>
          <source>Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</source>
          <year>1999</year>
        </element-citation>
      </ref>
      <ref id="bb0015">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Beal</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ghahramani</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <chapter-title>The variational Bayesian EM algorithms for incomplete data: with application to scoring graphical model structures</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Bernardo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bayarri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Berger</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dawid</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <series>Bayesian Statistics</series>
          <volume>7</volume>
          <year>2003</year>
          <publisher-name>Cambridge University Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0020">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bernardo</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>A.F.M.</given-names>
            </name>
          </person-group>
          <chapter-title>Bayesian Theory</chapter-title>
          <year>2000</year>
          <publisher-name>Wiley</publisher-name>
          <publisher-loc>Chichester</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0025">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bishop</surname>
              <given-names>C.M.</given-names>
            </name>
          </person-group>
          <chapter-title>Pattern Recognition and Machine Learning</chapter-title>
          <year>2006</year>
          <publisher-name>Springer</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0030">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Brockwell</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <source>Time Series: Theory and Methods</source>
          <edition>2 edition</edition>
          <year>2009</year>
          <publisher-name>Springer</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0035">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Burnham</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <chapter-title>Model selection and multimodel inference: a practical information theoretic approach</chapter-title>
          <edition>2nd edition</edition>
          <year>2002</year>
          <publisher-name>Springer-Verlag</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0040">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Burnham</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Multimodel inference: understanding AIC and BIC in model selection</article-title>
          <source>Sociol. Methods Res.</source>
          <volume>33</volume>
          <year>2004</year>
          <fpage>261</fpage>
          <lpage>304</lpage>
        </element-citation>
      </ref>
      <ref id="bb0045">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buxton</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Uludag</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Dubowitz</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Modelling the hemodynamic response to brain activation</article-title>
          <source>NeuroImage</source>
          <volume>23</volume>
          <year>2004</year>
          <fpage>220</fpage>
          <lpage>233</lpage>
        </element-citation>
      </ref>
      <ref id="bb0050">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Carlin</surname>
              <given-names>B.P.</given-names>
            </name>
            <name>
              <surname>Louis</surname>
              <given-names>T.A.</given-names>
            </name>
          </person-group>
          <chapter-title>Bayes and Empirical Bayes Methods for Data Analysis</chapter-title>
          <year>2000</year>
          <publisher-name>Chapman and Hall</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modelling of distributed electromagnetic responses</article-title>
          <source>NeuroImage</source>
          <volume>47</volume>
          <issue>2</issue>
          <year>2009</year>
          <fpage>590</fpage>
          <lpage>601</lpage>
          <pub-id pub-id-type="pmid">19398015</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0060">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mattout</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Variational free energy and the Laplace approximation</article-title>
          <source>NeuroImage</source>
          <volume>34</volume>
          <issue>1</issue>
          <year>2007</year>
          <fpage>220</fpage>
          <lpage>234</lpage>
          <pub-id pub-id-type="pmid">17055746</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0065">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Post hoc Bayesian model selection</article-title>
          <source>NeuroImage</source>
          <volume>56</volume>
          <issue>4</issue>
          <year>2011</year>
          <fpage>2089</fpage>
          <lpage>2099</lpage>
          <pub-id pub-id-type="pmid">21459150</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0070">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian estimation of dynamical systems: an application to fMRI</article-title>
          <source>NeuroImage</source>
          <volume>16</volume>
          <year>2002</year>
          <fpage>513</fpage>
          <lpage>530</lpage>
          <pub-id pub-id-type="pmid">12030834</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0075">
        <element-citation publication-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Nichols</surname>
              <given-names>T.E.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
          </person-group>
          <source>Statistical Parametric Mapping: The Analysis of Functional Brain Images</source>
          <year>2007</year>
          <publisher-name>Academic Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="bb0080">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Phillips</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Trujillo-Bareto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Henson</surname>
              <given-names>R.N.A.</given-names>
            </name>
            <name>
              <surname>Flandin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Mattout</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Multiple sparse priors for the M/EEG inverse problem</article-title>
          <source>NeuroImage</source>
          <volume>39</volume>
          <issue>3</issue>
          <year>2008</year>
          <fpage>1104</fpage>
          <lpage>1120</lpage>
          <pub-id pub-id-type="pmid">17997111</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0085">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Harrison</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic causal modelling</article-title>
          <source>NeuroImage</source>
          <volume>19</volume>
          <issue>4</issue>
          <year>2003</year>
          <fpage>1273</fpage>
          <lpage>1302</lpage>
          <pub-id pub-id-type="pmid">12948688</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0090">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Carlin</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Stern</surname>
              <given-names>H.S.</given-names>
            </name>
            <name>
              <surname>Rubin</surname>
              <given-names>D.B.</given-names>
            </name>
          </person-group>
          <chapter-title>Bayesian Data Analysis</chapter-title>
          <year>1995</year>
          <publisher-name>Chapman and Hall</publisher-name>
          <publisher-loc>Boca Raton</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0095">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Henson</surname>
              <given-names>R.N.A.</given-names>
            </name>
            <name>
              <surname>Shallice</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gorno-Tempini</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Face repetition effects in implicit and explicit memory tests as measured by fMRI</article-title>
          <source>Cereb. Cortex</source>
          <volume>12</volume>
          <year>2002</year>
          <fpage>178</fpage>
          <lpage>186</lpage>
          <pub-id pub-id-type="pmid">11739265</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0100">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hoeting</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Madigan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
            <name>
              <surname>Volinsky</surname>
              <given-names>C.T.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian model averaging: a tutorial</article-title>
          <source>Stat. Sci.</source>
          <volume>14</volume>
          <issue>4</issue>
          <year>1999</year>
          <fpage>382</fpage>
          <lpage>417</lpage>
        </element-citation>
      </ref>
      <ref id="bb0105">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hurvich</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tsai</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Regression and time series model selection in small samples</article-title>
          <source>Biometrika</source>
          <volume>76</volume>
          <year>1989</year>
          <fpage>297</fpage>
          <lpage>307</lpage>
        </element-citation>
      </ref>
      <ref id="bb0110">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kass</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <article-title>Bayes factors</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>90</volume>
          <year>1995</year>
          <fpage>773</fpage>
          <lpage>795</lpage>
        </element-citation>
      </ref>
      <ref id="bb0115">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kleinbaum</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Kupper</surname>
              <given-names>L.L.</given-names>
            </name>
            <name>
              <surname>Muller</surname>
              <given-names>K.E.</given-names>
            </name>
          </person-group>
          <chapter-title>Applied Regression Analysis and Other Multivariable Methods</chapter-title>
          <year>1988</year>
          <publisher-name>PWS-Kent</publisher-name>
          <publisher-loc>Boston</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0120">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Leff</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schofield</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Crinion</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>The cortical dynamics of intelligible speech</article-title>
          <source>J. Neurosci.</source>
          <volume>28</volume>
          <issue>49</issue>
          <year>2008</year>
          <fpage>13209</fpage>
          <lpage>13215</lpage>
          <pub-id pub-id-type="pmid">19052212</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0125">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Mackay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <chapter-title>Information Theory, Inference and Learning Algorithms</chapter-title>
          <year>2003</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0130">
        <mixed-citation publication-type="other">M. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.</mixed-citation>
      </ref>
      <ref id="bb0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Variational Bayesian Inference for fMRI time series</article-title>
          <source>NeuroImage</source>
          <volume>19</volume>
          <issue>3</issue>
          <year>2003</year>
          <fpage>727</fpage>
          <lpage>741</lpage>
          <pub-id pub-id-type="pmid">12880802</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0140">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Roberts</surname>
              <given-names>S.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian multivariate autoregresive models with structured priors</article-title>
          <source>IEE Proc. Vis., Image Signal Process.</source>
          <volume>149</volume>
          <issue>1</issue>
          <year>2002</year>
          <fpage>33</fpage>
          <lpage>41</lpage>
        </element-citation>
      </ref>
      <ref id="bb0145">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rosa</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Schofield</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Leff</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>Comparing families of dynamic causal models</article-title>
          <source>PLoS Comput. Biol.</source>
          <volume>6</volume>
          <issue>3</issue>
          <year>2010</year>
          <fpage>e1000709</fpage>
          <pub-id pub-id-type="pmid">20300649</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0150">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Mechelli</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Comparing dynamic causal models</article-title>
          <source>NeuroImage</source>
          <volume>22</volume>
          <issue>3</issue>
          <year>2004</year>
          <fpage>1157</fpage>
          <lpage>1172</lpage>
          <pub-id pub-id-type="pmid">15219588</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0155">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <chapter-title>Bayesian model selection in social research</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Marsden</surname>
              <given-names>P.V.</given-names>
            </name>
          </person-group>
          <source>Sociological Methodology</source>
          <year>1995</year>
          <publisher-name>Mass</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
          <fpage>111</fpage>
          <lpage>196</lpage>
        </element-citation>
      </ref>
      <ref id="bb0160">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Roberts</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
          </person-group>
          <article-title>Variational Bayes for generalised autoregressive models</article-title>
          <source>IEEE Trans. Signal Process.</source>
          <volume>50</volume>
          <issue>9</issue>
          <year>2002</year>
          <fpage>2245</fpage>
          <lpage>2257</lpage>
        </element-citation>
      </ref>
      <ref id="bb0165">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarz</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Estimating the dimension of a model</article-title>
          <source>Ann. Stat.</source>
          <volume>6</volume>
          <year>1978</year>
          <fpage>461</fpage>
          <lpage>464</lpage>
        </element-citation>
      </ref>
      <ref id="bb0170">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Moran</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian model selection for group studies</article-title>
          <source>NeuroImage</source>
          <volume>46</volume>
          <issue>4</issue>
          <year>2009</year>
          <fpage>1004</fpage>
          <lpage>1017</lpage>
          <pub-id pub-id-type="pmid">19306932</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0175">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Weiskopf</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Drysdale</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Comparing hemodynamic models with DCM</article-title>
          <source>NeuroImage</source>
          <volume>38</volume>
          <issue>3</issue>
          <year>2007</year>
          <fpage>387</fpage>
          <lpage>401</lpage>
          <pub-id pub-id-type="pmid">17884583</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0180">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Moran</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>den Ouden</surname>
              <given-names>H.E.M.</given-names>
            </name>
            <name>
              <surname>Daunizeau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Ten simple rules for dynamic causal modeling</article-title>
          <source>NeuroImage</source>
          <volume>49</volume>
          <issue>4</issue>
          <year>2010</year>
          <fpage>3099</fpage>
          <lpage>3109</lpage>
          <pub-id pub-id-type="pmid">19914382</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0185">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Aubert-Vazquez</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Valdes-Sosa</surname>
              <given-names>P.A.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian model averaging in EEG/MEG imaging</article-title>
          <source>NeuroImage</source>
          <volume>21</volume>
          <year>2004</year>
          <fpage>1300</fpage>
          <lpage>1319</lpage>
          <pub-id pub-id-type="pmid">15050557</pub-id>
        </element-citation>
      </ref>
      <ref id="bb0190">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Valente</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Wellekens</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <chapter-title>Scoring unknown speaker clustering: VB vs BIC</chapter-title>
          <source>ICSLP 2004, 8th Biennial conference of International Conference on Spoken Language Processing</source>
          <year>2004</year>
          <publisher-name>Jeju Island</publisher-name>
          <publisher-loc>Korea</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bb0195">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wipf</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Nagarajan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A unified Bayesian framework for MEG/EEG source imaging</article-title>
          <source>NeuroImage</source>
          <volume>44</volume>
          <issue>3</issue>
          <year>2009</year>
          <fpage>947</fpage>
          <lpage>966</lpage>
          <pub-id pub-id-type="pmid">18602278</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="s0095">
      <label>Appendix A</label>
      <title>Hemodynamics</title>
      <p>In DCM, neuronal activity gives rise to fMRI activity by a dynamic process described by an extended Balloon model (<xref rid="bb0045" ref-type="bibr">Buxton et al., 2004</xref>) and BOLD signal model (<xref rid="bb0175" ref-type="bibr">Stephan et al., 2007</xref>) for each region. This specifies how changes in neuronal activity give rise to changes in blood oxygenation that are measured with fMRI.</p>
      <p>The hemodynamic model involves a set of hemodynamic state variables, state equations and hemodynamic parameters. For the <italic>i</italic>th region, neuronal activity <italic>z(i)</italic> causes an increase in vasodilatory signal <italic>s</italic><sub><italic>i</italic></sub> that is subject to autoregulatory feedback. Inflow <italic>f</italic><sub><italic>i</italic></sub> responds in proportion to this signal with concomitant changes in blood volume <italic>v</italic><sub><italic>i</italic></sub> and deoxyhemoglobin content <italic>q</italic><sub><italic>i</italic></sub>.<disp-formula id="fo0175"><label>(35)</label><mml:math id="M38" altimg="si38.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>ρ</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Outflow is related to volume <italic>f</italic><sub><italic>out</italic></sub> = <italic>v</italic><sup>1/<italic>α</italic></sup> through Grubb's exponent <italic>α</italic> (<xref rid="bb0085" ref-type="bibr">Friston et al., 2003</xref>). The oxygen extraction is a function of flow<disp-formula id="fo0180"><label>(36)</label><alternatives><textual-form specific-use="jats-markup"><italic>E</italic>(<italic>f</italic>, <italic>ρ</italic>) = 1 − (1 − <italic>ρ</italic>)<sup>1 / <italic>f</italic></sup></textual-form><mml:math id="M39" altimg="si39.gif" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></disp-formula>where <italic>ρ</italic> is resting oxygen extraction fraction. The free parameters of the model are the rate of signal decay in each region, <italic>κ</italic><sub><italic>i</italic></sub>, and the transit time in each region, <italic>τ</italic><sub><italic>i</italic></sub>. The other parameters are fixed to <italic>γ</italic> = <italic>α</italic> = <italic>ρ</italic> = 0.32.</p>
      <sec id="s0100">
        <label>A.1</label>
        <title>BOLD signal model</title>
        <p>The Blood Oxygenation Level Dependent (BOLD) signal is then taken to be a static nonlinear function of volume and deoxyhemoglobin that comprises a volume-weighted sum of extra- and intra-vascular signals. This is based on a simplified approach from Stephan et al. (<xref rid="bb0175" ref-type="bibr">Stephan et al., 2007</xref>) (Eq. <xref rid="fo0060" ref-type="disp-formula">12</xref>) that improves upon the earlier model (<xref rid="bb0085" ref-type="bibr">Friston et al., 2003</xref>)<disp-formula id="fo0185"><label>(37)</label><mml:math id="M40" altimg="si40.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mfenced open="[" close="]"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>4.3</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>ρ</mml:mi><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>ε</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>ρ</mml:mi><mml:mi>T</mml:mi><mml:mi>E</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ε</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>V</italic><sub>0</sub> is resting blood volume fraction, <italic>θ</italic><sub>0</sub> is the frequency offset at the outer surface of the magnetised vessel for fully deoxygenated blood at 1.5T, TE is the echo time and <italic>r</italic><sub>0</sub> is the slope of the relation between the intravascular relaxation rate and oxygen saturation (<xref rid="bb0175" ref-type="bibr">Stephan et al., 2007</xref>). In this paper we use the standard parameter values <italic>V</italic><sub>0</sub> = 4, <italic>r</italic><sub>0</sub> = 25, <italic>θ</italic><sub>0</sub> = 40.3 and for our fMRI imaging sequence we have <italic>TE</italic> = 0.04.</p>
        <p>The only free parameter of the BOLD signal model is <italic>ε</italic>, the ratio of intra- to extra-vascular signal. Together the above equations describe a nonlinear hemodynamic process and BOLD signal model that convert neuronal activity in the <italic>i</italic>th region, <italic>z</italic><sub><italic>i</italic></sub>, to the fMRI signal, <italic>y</italic><sub><italic>i</italic></sub>.</p>
      </sec>
      <sec id="s0105">
        <label>A.2</label>
        <title>Priors</title>
        <p>The unknown parameters are {<italic>κ</italic><sub><italic>i</italic></sub>, <italic>τ</italic><sub><italic>i</italic></sub>, <italic>ε</italic>}. These are represented as<disp-formula id="fo0190"><label>(38)</label><mml:math id="M41" altimg="si41.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.64</mml:mn><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ε</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and we have Gaussian priors<disp-formula id="fo0195"><label>(39)</label><mml:math id="M42" altimg="si42.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.135</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.135</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:mi>ε</mml:mi><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.135</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>h</italic> = {<italic>θ</italic><sub><italic>κ</italic><sub><italic>i</italic></sub></sub>, <italic>θ</italic><sub><italic>τ</italic><sub><italic>i</italic></sub></sub>, <italic>ε</italic>} are the hemodynamic parameters to be estimated.</p>
      </sec>
    </sec>
    <sec id="s0110">
      <label>Appendix B</label>
      <title>Laplace approximation</title>
      <p>In what follows we have simplified notation by dropping the dependence on model <italic>m</italic>. The negative variational free energy (henceforth ‘Free Energy’) is defined as<disp-formula id="fo0200"><label>(40)</label><mml:math id="M43" altimg="si43.gif" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced open="[" close="]"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:mfenced><mml:mspace width="0.5em"/><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:math></disp-formula>where<disp-formula id="fo0205"><label>(41)</label><alternatives><textual-form specific-use="jats-markup"><italic>p</italic>(<italic>y</italic>, <italic>θ</italic>, <italic>λ</italic>) = <italic>p</italic>(<italic>y</italic>|<italic>θ</italic>, <italic>λ</italic>)<italic>p</italic>(<italic>θ</italic>)<italic>p</italic>(<italic>λ</italic>)</textual-form><mml:math id="M44" altimg="si44.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>We can rewrite this as<disp-formula id="fo0210"><label>(42)</label><alternatives><textual-form specific-use="jats-markup"><italic>F</italic> = <italic>I</italic> + <italic>H</italic>(<italic>θ</italic>) + <italic>H</italic>(<italic>λ</italic>)</textual-form><mml:math id="M45" altimg="si45.gif" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>where<disp-formula id="fo0215"><label>(43)</label><alternatives><textual-form specific-use="jats-markup"><italic>I</italic> = ∫∫<italic>q</italic>(<italic>θ</italic>|<italic>y</italic>)<italic>q</italic>(<italic>λ</italic>|<italic>y</italic>)<italic>U</italic>(<italic>θ</italic>, <italic>λ</italic>)<italic>d</italic><italic>θ</italic><italic>d</italic><italic>λ</italic></textual-form><mml:math id="M46" altimg="si46.gif" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>and <italic>H(x)</italic> is the (differential) entropy of <italic>x</italic> and<disp-formula id="fo0220"><label>(44)</label><alternatives><textual-form specific-use="jats-markup"><italic>U</italic>(<italic>θ</italic>, <italic>λ</italic>) = <italic>l</italic><italic>o</italic><italic>g</italic><italic>p</italic>(<italic>y</italic>, <italic>θ</italic>, <italic>λ</italic>)</textual-form><mml:math id="M47" altimg="si47.gif" overflow="scroll"><mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>For a Gaussian density <italic>p</italic>(<italic>x</italic>) = <italic>N</italic>(<italic>x</italic> ; <italic>m</italic>, <italic>S</italic>) the entropy is<disp-formula id="fo0225"><label>(45)</label><alternatives><textual-form specific-use="jats-markup"><italic>H</italic>(<italic>x</italic>) = ½(<italic>k</italic><italic>l</italic><italic>o</italic><italic>g</italic>2<italic>π</italic><italic>e</italic> + <italic>l</italic><italic>o</italic><italic>g</italic>|<italic>S</italic>|)</textual-form><mml:math id="M48" altimg="si48.gif" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula>where <italic>k</italic> = <italic>dim</italic>(<italic>S</italic>). Hence<disp-formula id="fo0230"><label>(46)</label><mml:math id="M49" altimg="si49.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>p</italic> is the number of parameters and <italic>h</italic> is the number of hyperparameters. The Variational Laplace approximation to the Free Energy is then given by<disp-formula id="fo0235"><label>(47)</label><mml:math id="M50" altimg="si50.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the integral <italic>I</italic> has been replaced by<disp-formula id="fo0240"><label>(48)</label><alternatives><textual-form specific-use="jats-markup"><italic>I</italic><sub><italic>L</italic></sub> = ∫∫<italic>q</italic>(<italic>θ</italic>|<italic>y</italic>)<italic>q</italic>(<italic>λ</italic>|<italic>y</italic>)<italic>U</italic><sub><italic>L</italic></sub>(<italic>θ</italic>, <italic>λ</italic>)<italic>d</italic><italic>θ</italic><italic>d</italic><italic>λ</italic></textual-form><mml:math id="M51" altimg="si51.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>and the function <italic>U</italic><sub><italic>L</italic></sub>(<italic>θ</italic>, <italic>λ</italic>) is given by a second order Taylor series expansion around the approximate (variational) posterior means<disp-formula id="fo0245"><label>(49)</label><mml:math id="M52" altimg="si52.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>U</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mspace width="0.5em"/><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>H</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mspace width="0.5em"/><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>H</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the curvatures<disp-formula id="fo0250"><label>(50)</label><mml:math id="M53" altimg="si53.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>are evaluated at the approximate (variational) posterior means <italic>λ</italic> = <italic>m</italic><sub><italic>λ</italic></sub> and <italic>θ</italic> = <italic>m</italic><sub><italic>θ</italic></sub>. Note that the first order (gradient) term in Eq. <xref rid="fo0245" ref-type="disp-formula">(49)</xref> is zero because we are at a maximum. This gives<disp-formula id="fo0255"><label>(51)</label><alternatives><textual-form specific-use="jats-markup"><italic>I</italic><sub><italic>L</italic></sub> = <italic>U</italic>(<italic>m</italic><sub><italic>θ</italic></sub>, <italic>m</italic><sub><italic>λ</italic></sub>) + ½<italic>T</italic><italic>r</italic>(<italic>S</italic><sub><italic>θ</italic></sub><italic>H</italic><sub><italic>θ</italic></sub>) + ½<italic>T</italic><italic>r</italic>(<italic>S</italic><sub><italic>λ</italic></sub><italic>H</italic><sub><italic>λ</italic></sub>)</textual-form><mml:math id="M54" altimg="si54.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mi>H</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:msub><mml:mi>H</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>During VL optimisation (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>) the posterior covariances are set to the negative inverse curvatures<disp-formula id="fo0260"><label>(52)</label><mml:math id="M55" altimg="si55.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Hence<disp-formula id="fo0265"><label>(53)</label><mml:math id="M56" altimg="si56.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p>Substituting this into Eq. <xref rid="fo0235" ref-type="disp-formula">(47)</xref> gives<disp-formula id="fo0270"><label>(54)</label><mml:math id="M57" altimg="si57.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>This corresponds to equation 8 in (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>). We note that<disp-formula id="fo0275"><label>(55)</label><mml:math id="M58" altimg="si58.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>U</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>λ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>where the error terms are<disp-formula id="fo0280"><label>(56)</label><mml:math id="M59" altimg="si59.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>λ</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Finally, we have<disp-formula id="fo0285"><label>(57)</label><mml:math id="M60" altimg="si60.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>θ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>e</mml:mi><mml:mi>λ</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>C</mml:mi><mml:mi>λ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>This corresponds to equation 21 in (<xref rid="bb0060" ref-type="bibr">Friston et al., 2007a</xref>).</p>
      <p>The quantity <italic>U</italic><sub><italic>L</italic></sub>(<italic>θ</italic>, <italic>λ</italic>) is equal to <italic>U</italic>(<italic>θ</italic>, <italic>λ</italic>) if the latter is a quadratic function. This is the case for linear Gaussian models. For all other models, where the quadratic relationship does not hold exactly, <italic>U</italic><sub><italic>L</italic></sub> can be bigger or smaller than <italic>U</italic>. For this reason <italic>F</italic><sub><italic>L</italic></sub> can be bigger or smaller than <italic>F</italic>, so <italic>F</italic><sub><italic>L</italic></sub> is not a lower bound on the log model evidence (<xref rid="bb0195" ref-type="bibr">Wipf and Nagarajan, 2009</xref>).</p>
    </sec>
    <sec id="s0115">
      <label>Appendix C</label>
      <title>Proportion of variance explained</title>
      <p>The proportion of variance explained by a model can be written as<disp-formula id="fo0290"><label>(58)</label><mml:math id="M61" altimg="si61.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <italic>σ</italic><sub><italic>y</italic></sub><sup>2</sup> is the variance of the signal and <italic>σ</italic><sub><italic>e</italic></sub><sup>2</sup> is the variance of the noise. The left hand side is written with the symbol <italic>R</italic><sup>2</sup> because <italic>R</italic> is also equal to the correlation coefficient between the model predictions and data (<xref rid="bb0115" ref-type="bibr">Kleinbaum et al., 1988</xref>). We can divide the numerator and denominator by <italic>σ</italic><sub><italic>y</italic></sub><sup>2</sup> to give<disp-formula id="fo0295"><label>(59)</label><mml:math id="M62" altimg="si62.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p>Plugging in our definition for SNR gives<disp-formula id="fo0300"><label>(60)</label><mml:math id="M63" altimg="si63.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mfenced open="(" close=")"><mml:mstyle><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p>Thus, SNRs of 0.2, 1.3 and 2 correspond to <italic>R</italic><sup>2</sup>'s of 0.04, 0.63, and 0.80.</p>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>This work was supported by the Wellcome Trust. We thank Gareth Barnes, Guillaume Flandin, Karl Friston, Vladimir Litvak, Klaas Stephan, Maria Rosa and Ged Ridgway for useful feedback on this work.</p>
    </ack>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig. 1</label>
      <caption>
        <p>Design matrix for the full GLM. The nested GLM uses an identical design matrix but with the first three columns removed. The full design matrix comprises <italic>N</italic> = 351 rows, one for each fMRI scan, and twelve columns, one for each putative experimental effect.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig. 2</label>
      <caption>
        <p>Log Bayes factor of full versus nested model, Log <italic>B</italic><sub><italic>f</italic>, <italic>n</italic></sub>, versus the signal to noise ratio, SNR, when the true model is the full GLM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue) and BIC (red).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig. 3</label>
      <caption>
        <p>Log Bayes factor of nested versus full model, Log <italic>B</italic><sub><italic>n</italic>, <italic>f</italic></sub>, versus the signal to noise ratio, SNR, when the true model is the nested GLM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue) and BIC (red).</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig. 4</label>
      <caption>
        <p>Log Bayes factor of full versus nested model, Log <italic>B</italic><sub><italic>f</italic>, <italic>n</italic></sub>, versus the number of data points, <italic>N</italic>, when the true model is the full GLM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue), BIC (red) and AICc (green).</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig. 5</label>
      <caption>
        <p>Log Bayes factor of nested versus full model, Log <italic>B</italic><sub><italic>n</italic>, <italic>f</italic></sub>, versus the number of data points, <italic>N</italic>, when the true model is the nested GLM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue), BIC (red) and AICc (green).</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig. 6</label>
      <caption>
        <p>A nested (left) and full (right) DCM. The full DCM is identical to the nested DCM except for having an additional modulatory forward connection from region P to region A. Intrinsic connections are indicated by dotted arrows, modulatory connections by overlaid solid arrows and inputs by solid squares with an arrow.</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig. 7</label>
      <caption>
        <p>Log Bayes factor of full versus nested model, Log <italic>B</italic><sub><italic>f</italic>, <italic>n</italic></sub>, versus the signal to noise ratio, SNR, when the true model is the full DCM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue) and BIC (red).</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig. 8</label>
      <caption>
        <p>Log Bayes factor of nested versus full model, Log <italic>B</italic><sub><italic>n</italic>, <italic>f</italic></sub>, versus the signal to noise ratio, SNR, when the true model is the nested DCM for <italic>F</italic><sub><italic>L</italic></sub> (black), AIC (blue) and BIC (red).</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
  </floats-group>
</article>