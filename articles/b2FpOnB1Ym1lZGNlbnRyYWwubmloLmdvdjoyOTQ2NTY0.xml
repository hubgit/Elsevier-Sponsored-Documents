<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2946564</article-id>
      <article-id pub-id-type="pmid">20483377</article-id>
      <article-id pub-id-type="publisher-id">YNIMG7315</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.015</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Evaluating an acoustically quiet EPI sequence for use in fMRI studies of speech and auditory processing</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Peelle</surname>
            <given-names>Jonathan E.</given-names>
          </name>
          <email>jonathan.peelle@mrc-cbu.cam.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Eason</surname>
            <given-names>Rowena J.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Schmitter</surname>
            <given-names>Sebastian</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Schwarzbauer</surname>
            <given-names>Christian</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="fn1" ref-type="fn">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>Matthew H.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>MRC Cognition and Brain Sciences Unit, 15 Chaucer Road, Cambridge CB2 7EF, UK</aff>
      <aff id="aff2"><label>b</label>German Cancer Research Center, Heidelberg, Germany</aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. <email>jonathan.peelle@mrc-cbu.cam.ac.uk</email></corresp>
        <fn id="fn1">
          <label>1</label>
          <p>Current address: University of Aberdeen, Aberdeen, Scotland.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>01</day>
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <volume>52</volume>
      <issue>4-3</issue>
      <fpage>1410</fpage>
      <lpage>1419</lpage>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>2</month>
          <year>2010</year>
        </date>
        <date date-type="rev-recd">
          <day>14</day>
          <month>4</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>6</day>
          <month>5</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2010 Elsevier Inc.</copyright-statement>
        <copyright-year>2010</copyright-year>
        <copyright-holder/>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Echoplanar MRI is associated with significant acoustic noise, which can interfere with the presentation of auditory stimuli, create a more challenging listening environment, and increase discomfort felt by participants. Here we investigate a scanning sequence that significantly reduces the amplitude of acoustic noise associated with echoplanar imaging (EPI). This is accomplished using a constant phase encoding gradient and a sinusoidal readout echo train to produce a narrow-band acoustic frequency spectrum, which is adapted to the scanner's frequency response function by choosing an optimum gradient switching frequency. To evaluate the effect of these nonstandard parameters we conducted a speech experiment comparing four different EPI sequences: Quiet, Sparse, Standard, and Matched Standard (using the same readout duration as Quiet). For each sequence participants listened to sentences and signal-correlated noise (SCN), which provides an unintelligible amplitude-matched control condition. We used BOLD sensitivity maps to quantify sensitivity loss caused by the longer EPI readout duration used in the Quiet and Matched Standard EPI sequences. We found that the Quiet sequence provided more robust activation for SCN in primary auditory areas and comparable activation in frontal and temporal regions for Sentences &gt; SCN, but less sentence-related activity in inferotemporal cortex. The increased listening effort associated with the louder Standard sequence relative to the Quiet sequence resulted in increased activation in the left temporal and inferior parietal cortices. Together, these results suggest that the Quiet sequence is suitable, and perhaps preferable, for many auditory studies. However, its applicability depends on the specific brain regions of interest.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Sparse imaging</kwd>
        <kwd>Quiet EPI</kwd>
        <kwd>BOLD sensitivity</kwd>
        <kwd>Listening effort</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <title>Introduction</title>
      <p>During echoplanar imaging (EPI), rapid switching causes the gradient coils to oscillate, resulting in the significant acoustic noise typically associated with fMRI scanning (<xref rid="bib42 bib43" ref-type="bibr">Price et al., 2001; Ravicz et al., 2000</xref>). This acoustic noise, which can exceed 100 dBA, presents several serious challenges for studying the processing of auditory stimuli (<xref rid="bib34" ref-type="bibr">Moelker and Pattynama, 2003</xref>). First, and perhaps most obviously, the high sound levels generated can render experimental stimuli unintelligible. Second, even if it is possible to hear the stimuli, listeners must separate them from the noise of the scanner, adding additional perceptual and cognitive demands to the task—that is, increases in listening effort (<xref rid="bib13" ref-type="bibr">Davis and Johnsrude, 2003</xref>). Such task demands are likely to differentially affect participants with difficulties in auditory processing due to hearing impairment or normal aging (<xref rid="bib23 bib39 bib56" ref-type="bibr">Grimault et al., 2001; Peelle and Wingfield, 2005; Wingfield et al., 2006</xref>). Finally, the acoustic noise of the scanner itself will activate auditory cortex (<xref rid="bib5" ref-type="bibr">Bandettini et al., 1998</xref>), which may diminish effects induced by experimental manipulations (<xref rid="bib19 bib21" ref-type="bibr">Elliott et al., 1999; Gaab et al., 2007</xref>). Thus, standard EPI sequences are sub-optimal for auditory tasks, and may make any results difficult to interpret. In addition, the reduction of acoustic noise may also be desirable for participant comfort, particularly when dealing with children or other special populations.</p>
      <p>One common solution to the challenge posed by acoustic scanner noise is to use a sparse imaging procedure in which the repetition time (TR) of a sequence is longer than its acquisition time (TA), clustering slice acquisition in time in order to provide a silent period between the acquisition of consecutive volumes (<xref rid="bib17 bib24 bib46" ref-type="bibr">Edmister et al., 1999; Hall et al., 1999; Scheffler et al., 1998</xref>). Auditory stimuli can then be presented during these silent periods without disruption from echoplanar scanner noise; the delay in the hemodynamic response to a stimulus enables the BOLD signal changes associated with these stimuli to be measured by the next volume of data acquired. Because of the longer TR, for a constant amount of scanning time, fewer images are acquired in a sparse imaging paradigm than in a continuous paradigm. This approach therefore reduces the temporal resolution of the data and, due to there being fewer observations, potentially reduces the accuracy of the parameter estimates (although this may be offset by higher overall levels of signal due to the absence of spin history effects). Some modifications to sparse imaging paradigms have been developed to compensate for these shortfalls by collecting multiple volumes following a silent period (e.g., <xref rid="bib49" ref-type="bibr">Schwarzbauer et al., 2006</xref>). Nonetheless, for a given amount of scanning time, sparse imaging approaches are fundamentally limited in the number of volumes that can be acquired relative to continuous sequences, and the extent to which differently-timed responses to a stimulus can be measured.</p>
      <p>Another important consideration is the fact that responses in auditory regions are not only influenced by the amplitude of the scanner noise, but by other parameters, such as its perceived continuity: auditory cortex responds strongly to pulsed noises in the frequency ranges associated with typical gradient switching, and thus typical EPI sequences make for particularly effective stimulation (<xref rid="bib22 bib28 bib50 bib53" ref-type="bibr">Giraud et al., 2000; Harms and Melcher, 2002; Seifritz et al., 2003; Tanaka et al., 2000</xref>). Thus, a second approach for addressing some of the issues faced in auditory fMRI studies is to change the qualitative nature of the acoustic noise. This approach was taken by <xref rid="bib51" ref-type="bibr">Seifritz et al. (2006)</xref>, who developed an EPI sequence that emits continuous noise (rather than pulsed) by implementing a quasi-continuous gradient switching pattern. The authors presented audio recordings of the noise generated by both types of sequences to participants and recorded neural responses using a sparse imaging paradigm. They found that conventional EPI produced stronger responses than the continuous noise EPI. Additionally, responses to pure tones in auditory cortex were greater when measured with continuous noise EPI relative to conventional EPI. These results emphasize that the nature or quality of acoustic stimulation from the scanner, and not just its average loudness, must be considered in auditory fMRI studies.</p>
      <p>Although changing the acoustic characteristics of the scanner noise effectively boosts BOLD responses in auditory areas for some stimuli, it still leaves open the possibility of interference by energetic masking, and may also lead to extra challenges of listening effort, especially for more complex (e.g., linguistic) stimuli. One way to mitigate these effects is to use active noise control to minimize the effects of scanner noise (<xref rid="bib9 bib26" ref-type="bibr">Chambers et al., 2007; Hall et al., 2009</xref>). Here we adopt an alternate approach to reducing the impact of acoustic noise by using an EPI sequence that is sufficiently quiet to allow participants to easily perceive auditory stimuli, even in the presence of pulsed scanner noise (<xref rid="bib47" ref-type="bibr">Schmitter et al., 2008</xref>). In this sequence, acoustic noise is minimized by using a constant phase encoding gradient and a sinusoidal readout echo train to produce a narrow-band acoustic frequency spectrum. The scanner-specific frequency response function can be measured using an MR-compatible microphone placed inside the magnet bore. It is then possible to choose a readout gradient switching frequency (within the limits imposed by BOLD fMRI) that results in a lower acoustic response based on this frequency response function. In addition, the clicking noise of the slice-selection gradient is reduced by choosing a lower slew rate.</p>
      <p>This modified gradient switching scheme can reduce the acoustic noise of EPI by up to 20–30 dB compared to trapezoidal EPI using the same imaging parameters. However, it may also influence data quality. For example, the longer EPI readout duration required by using a slower gradient switching frequency would be expected to exacerbate susceptibility effects near tissue boundaries, such as in inferior temporal and orbital frontal regions (<xref rid="bib16 bib37" ref-type="bibr">Devlin et al., 2000; Ojemann et al., 1997</xref>). In addition, the nonuniform sampling of <italic>k</italic>-space requires an adaptation of standard image reconstruction software, and because of the sinusoidal readout gradient, the resulting images are also smoother than those from a standard sequence.</p>
      <p>The primary aim of the current study is to evaluate the data provided by this new sequence relative to existing EPI sequences. We chose to do so using auditory stimuli that result in robust and replicable patterns of activation in well-known regions of cortex based on several previous studies (<xref rid="bib14 bib36 bib44" ref-type="bibr">Davis et al., 2007; Mummery et al., 1999; Rodd et al., 2005</xref>).</p>
    </sec>
    <sec sec-type="methods">
      <title>Method</title>
      <sec>
        <title>Participants</title>
        <p>Six healthy right-handed adults aged 20–26 years (3 females) participated in this study. All were native English speakers with self-reported normal hearing and no history of neurological problems. Written consent was obtained from all participants on a protocol approved by the local ethics committee.</p>
      </sec>
      <sec sec-type="materials">
        <title>Materials</title>
        <p>The materials consisted of a set of unambiguous sentences created for <xref rid="bib44" ref-type="bibr">Rodd et al. (2005)</xref>. Sentences ranged in duration from 1.14 to 3.58 s and contained simple declarative statements (e.g., “The police returned to the museum.”). The 120 sentences used were divided into 4 groups of 30, matched for duration, naturalness, imageability, and number of words using Match software (<xref rid="bib54" ref-type="bibr">Van Casteren and Davis 2007</xref>), available from <ext-link ext-link-type="uri" xlink:href="http://www.mrc-cbu.cam.ac.uk/people/maarten.van-casteren/mixandmatch.html">http://www.mrc-cbu.cam.ac.uk/people/maarten.van-casteren/mixandmatch.html</ext-link>. For each sentence, a probe word was generated for use in a behavioral task. Half of these probe words were semantically related to the sentence, and half were unrelated.</p>
        <p>For a baseline condition, sentences matched in duration to the experimental sentences were used to create signal-correlated noise (SCN) (<xref rid="bib48" ref-type="bibr">Schroeder, 1968</xref>). These stimuli have the same overall amplitude envelope and spectral profile as the original sentences but are lacking spectral detail, and are therefore entirely unintelligible. They thus provide a good control for the acoustic stimulation and temporal pattern of the sentences without conveying any linguistic information.</p>
      </sec>
      <sec>
        <title>Procedure</title>
        <p>Participants were instructed to attend to each stimulus, and after each sentence respond to the probe word on the screen. In the case of the real sentences, the probe word had a 50% probability of being semantically related to the just-heard sentence; participants indicated whether or not the probe word was semantically related to the sentence using a button-press response. For example, for the sentence “There were beer and cider on the kitchen shelf,” a related probe word might have been “drink”. For the SCN trials, the word “left” or “right” appeared on the screen, and participants were instructed to press the appropriate button. Participants were situated in the magnet and familiarized with this procedure through a short practice session, during which we also ensured the sentences were being presented at a comfortable listening level. Participants were informed that they would be performing the same task four times, but that due to different imaging parameters the scanner noise would differ. In all cases they were instructed to ignore the scanner noise as much as possible and concentrate on listening to the auditory stimuli.</p>
      </sec>
      <sec>
        <title>Image acquisition</title>
        <p>All images were acquired on a Siemens 3 T Tim Trio scanner (Siemens Medical Systems, Erlangen, Germany). The four EPI sequences used to collect data on the experimental paradigm are listed in <xref rid="tbl1" ref-type="table">Table 1</xref> and described below. In addition to EPI data, we cquired a T1-weighted structural image for each participant using an MPRAGE sequence (TR = 2250 ms, TE = 2.99 ms, TI = 900 ms, flip angle = 9°, FOV = 256 mm × 240 mm × 160 mm, voxel size = 1 mm × 1 mm × 1 mm). In addition, field map data (2D structural and phase difference images) were acquired using a standard double echo GE sequence (TE1/TE2 = 5.19/7.65 ms; TR = 400 ms; flip angle = 60°, slice thickness = 3 mm; matrix size = 64 × 64; in-plane resolution = 3 × 3 mm; total acquisition time = 54 s). The phase difference images were unwrapped and converted into magnetic field maps (<xref rid="bib31" ref-type="bibr">Jenkinson, 2003</xref>).</p>
        <sec>
          <title>Standard EPI sequence</title>
          <p>We acquired 32 slices in axial oblique orientation (i.e., angled to ensure the eyeballs were not on the same plane as auditory cortex to avoid possible Nyquist ghost artifacts), collected in a descending order, with a .75 mm gap between slices (TR = 2.8 s, TA = 2.8 s, TE = 30 ms, flip angle = 90°, FOV = 192 mm × 192 mm, matrix = 64 × 64 mm, voxel size = 3 × 3 × 3 mm, bandwidth = 2230 Hz/Px). We ensured full temporal lobe coverage in all participants, but did not cover portions of superior parietal lobe in participants with larger heads. Unless noted below these parameters were consistent across all sequences tested.</p>
        </sec>
        <sec>
          <title>Sparse EPI sequence</title>
          <p>The Sparse sequence used a sparse or clustered acquisition sequence commonly employed in fMRI studies of auditory processing. This approach relies on the fact that, due to the lag inherent in the hemodynamic response, the peak BOLD response to a stimulus occurs several seconds after the stimulus, and can thus be captured by a scan occurring later in time. The imaging parameters were identical to the standard sequence, with the exception that we used a TR of 11.2 s. Given the TA of 2.8 s, this left 8.4 s of silence in which we presented a single sentence or SCN, allowing the item to be presented in the absence of echoplanar scanner noise. The presentation of the sentences and SCN was timed so that the midpoint of each stimulus occurred 5 s before the midpoint of data acquisition in order to maximize detection of the peak hemodynamic response.</p>
        </sec>
        <sec>
          <title>Quiet EPI sequence</title>
          <p>This sequence is described in more detail in <xref rid="bib47" ref-type="bibr">Schmitter et al. (2008)</xref>. The principle behind this sequence is that because the acoustic response of gradient coils changes as a function of the gradient coil switching frequency, changing the gradient switching frequency to one with a low acoustic response can significantly reduce acoustic noise. The quiet sequence therefore involved gradient switching g(<italic>t</italic>) which is characterized by a narrow-band input spectrum G(<italic>f</italic>), located in a frequency interval where the frequency response function showed a local minimum, judged using the individual frequency response profile measured for the empty scanner using an MR-safe optical microphone (Sennheiser MO 2000, Sennheiser GmbH &amp; Co. KG, Wedemark, Germany). Based on these measurements a readout bandwidth of 1220 Hz/Px was chosen, along with a TE of 44 ms. Finally, the quiet sequence also used sinusoidally switched readout gradients, with the phase encoding gradient switched constant. This combination results in an S-shaped trajectory through <italic>k</italic>-space, which was regridded using a custom software implementation. The sinusoidal readout gradient leads to smoother image reconstruction in the readout direction. Note that because the regridding procedure is identical for all image volumes, it does not result in additional signal variance over time. Sound levels measured inside the scanner indicated a considerable sound level reduction of approximately 20 dBA compared to the Standard sequence. Finally, as noted previously, the subjective quality of the scanner noise can influence the measured activity in auditory cortex. In our study, the perceived continuity of the Quiet sequence was similar to that of the Standard and Matched Standard (see below) sequences; that is, the reduction in overall acoustic amplitude would be expected to reduce the energetic masking, but responses that relate to the nature of the background noise (<xref rid="bib51" ref-type="bibr">Seifritz et al., 2006</xref>) are unlikely to be significantly affected. This is an advantage in that, having equated the pulsatile nature of the scanning sequence across sequences, we can more confidently attribute changes in activity to the differences in acoustic noise levels. At the same time, there is a possibility that an EPI sequence that was not only acoustically quiet but also continuous would afford even greater sensitivity.</p>
        </sec>
        <sec>
          <title>Matched Standard EPI sequence</title>
          <p>There are two significant differences between the Standard and Quiet sequences: (1) the Quiet sequence has a longer TE, and a narrower bandwidth (1220 Hz/Px as opposed to 2230 Hz/Px); and (2) the Quiet sequence uses a different type of readout gradient, requiring custom reconstruction. Thus, if differences were found between the Quiet and Standard sequences, it would be difficult to know to which parameter we could attribute the difference. To address this issue we also collected data using the Matched Standard sequence, which is identical to the Standard sequence, with the exception that the duration of the EPI readout was matched to the Quiet sequence by choosing the same bandwidth of 1220 Hz/Px; thus, the frequency of the readout gradient was identical to that of the Quiet sequence.</p>
        </sec>
      </sec>
      <sec>
        <title>Signal-to-noise ratio measurements</title>
        <p>To estimate the temporal signal-to-noise ratio (SNR) for each sequence, 6 white matter voxels (MNI coordinates: [−27 −18 32], [−22 39 4], and [−20 18 26] in the left hemisphere, along with their right hemisphere counterparts) were selected from which to sample the signal. Each voxel had at least a 97% probability of being white matter according to the tissue probability maps distributed with SPM, and their location in white matter was verified on the structural images for individual participants in the current study. For each sequence, the data from each voxel were extracted from the unsmoothed normalized images, and baseline corrected using a 5th order cosine basis set to remove low-frequency drifts. The SNR was then calculated by dividing the mean of the timeseries by its standard deviation (<xref rid="bib32" ref-type="bibr">Krüger and Glover, 2001</xref>). For each sequence the SNR was calculated for each of the 6 voxels for each participant, and then the SNR values were averaged to provide a single SNR value for each sequence for each participant.</p>
      </sec>
      <sec>
        <title>BOLD sensitivity maps</title>
        <p>BOLD sensitivity maps were calculated from the field maps and the specific acquisition parameters of the Standard and Quiet sequences using the theoretical framework described by <xref rid="bib15" ref-type="bibr">De Panfilis and Schwarzbauer (2005)</xref> with the extension proposed by <xref rid="bib55" ref-type="bibr">Weiskopf et al. (2007)</xref> to account for the effect of susceptibility gradients in readout direction. Because the Standard and Sparse sequences differ only in TR, the BOLD sensitivity maps are identical. In addition, the estimated sensitivity maps for the Quiet and Matched Standard sequences can be considered identical, as the relevant imaging parameters were identical and the effect of the nonuniform <italic>k</italic>-space sampling used in the Quiet sequence is negligible.</p>
      </sec>
      <sec>
        <title>fMRI analysis</title>
        <p>Image preprocessing and statistical analyses were performed using SPM5 (Wellcome Trust Centre for Neuroimaging, London, UK). The first 9 images for the three continuous sequences and the first 3 images for the sparse sequence were discarded to allow for T2 saturation effects. Low-frequency drifts were removed with highpass filtering (with a cutoff period of 90 s) and autocorrelations were modeled using a first-order autoregressive model. Images for each participant were realigned to the first image in the series (<xref rid="bib20" ref-type="bibr">Friston et al., 1995</xref>), corrected for effects of magnetic field inhomogeneity (<xref rid="bib11" ref-type="bibr">Cusack et al., 2003</xref>) and coregistered with the structural image (<xref rid="bib2" ref-type="bibr">Ashburner and Friston, 1997</xref>). The transformation required to bring a participant's images into standard MNI152 space was calculated using MNI-space tissue probability maps (2 × 2 × 2 mm) included with SPM5 (<xref rid="bib3" ref-type="bibr">Ashburner and Friston, 2005</xref>), and these warping parameters were then applied to all functional images for that participant. During normalization the data were interpolated to 2 mm isotropic voxels using trilinear interpolation. Prior to statistical analysis the data were spatially smoothed with a 10 mm FWHM isotropic Gaussian kernel.</p>
        <p>Data were initially analyzed separately for each participant, using a separate general linear model for each scanning sequence. Each event of interest (sentence or SCN) was convolved with a standard hemodynamic response function to create the repressors used in the model. This was specified as an event with no duration occurring at the middle of each stimulus. The 6 motion parameters obtained during realignment were included in the model as additional regressors. Following analysis on the subject level, images containing the contrasts of parameter estimates for each subject were entered into second-level group analyses. To ensure fair comparison for results across sequences despite differences in sensitivity and signal dropout, we used the same explicit brain mask (distributed with SPM) for all analyses. We did not use a proportional threshold for the fMRI data but analyzed all voxels included in the mask.</p>
        <p>For comparisons among the four sequences we examined both parameter estimates and <italic>t</italic>-statistics derived from the first-level (single-subject) analyses. We examined <italic>t</italic>-statistics in addition to parameter estimates because they give an indication as to the effect size relative to the variability present (which we expected might differ by sequence). For example, due to the lack of spin history effects, images acquired with a sparse sequence tend to exhibit higher overall levels of signal, but also more variability due to the reduced number of observations.</p>
        <p>Images are displayed on an MNI-space template brain included with MRIcron (<xref rid="bib45" ref-type="bibr">Rorden and Brett, 2000</xref>), available from <ext-link ext-link-type="uri" xlink:href="http://www.cabiatl.com/mricro/mricron/">http://www.cabiatl.com/mricro/mricron/</ext-link>. Coordinates listed for maxima are in MNI152 space.</p>
      </sec>
    </sec>
    <sec>
      <title>Results</title>
      <sec>
        <title>Behavioral performance</title>
        <p>We first examined accuracy on the behavioral task in all four scanning sequences, which was ≥ 93% correct in all cases. Accuracy did not differ as a function of scanning sequence, evidenced by a Friedman nonparametric test [<italic>χ</italic><sup>2</sup>(3) = 2.32, <italic>n.s.</italic>], indicating that participants were able to accurately perceive the speech stimuli present during each scanning sequence. However, we note that equal intelligibility can still be achieved through different amounts of listening effort, which we expect to be the case when auditory stimuli are presented in the context of background noise.</p>
      </sec>
      <sec>
        <title>Signal-to-noise ratio</title>
        <p>The sinusoidal readout gradient used in the Quiet sequence leads to an increase of the effective voxel size in readout direction, which translates into a linear increase in the SNR. Therefore, we first estimated the expected change in SNR based on this difference, relative to the Matched Standard sequence, which in all other ways is identical. The relative SNR gain, compared to the trapezoidal gradient switching schemes used in the other sequences, can be calculated from the areas under the respective gradient waveforms to give <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="monospace"><mml:mi>r</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mtext>g</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <italic>T</italic><sub>s</sub> is the gradient ramp time and <italic>T</italic><sub>g</sub> the total duration of a trapezoidal gradient lobe. Compared to a <italic>T</italic><sub>s</sub>/<italic>T</italic><sub>g</sub> = .15 (Matched Standard sequence), the expected SNR gain of the Quiet sequence is 34%.</p>
        <p>Signal-to-noise ratio (SNR) values calculated based on extracted data for each sequence are plotted in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. As expected, due to increased intrinsic smoothness, the Quiet sequence showed significantly higher SNR values. To assess whether the differences in values were significant, we submitted these data to a within-subjects one-way analysis of variance (ANOVA) to test for a main effect of sequence, which was significant, <italic>F</italic>(3,15) = 52.6, <italic>p</italic> &lt; .001, MSE = 701.9. We then conducted post-hoc <italic>t</italic>-tests between the Quiet sequence and all other sequences; these confirmed that, the Quiet sequence had a higher SNR than all of the other sequences, all <italic>t</italic>s &gt; 8.9 and all <italic>p</italic>s &lt; .001. The relative change in SNR between the Matched Standard and Quiet sequences was 27%, which is in reasonably good agreement with the theoretical gain described above. Although the increase in SNR was expected due to the increased smoothness inherent in the Quiet sequence, it should be noted that this SNR difference in the unsmoothed data becomes negligible in the smoothed data used in the fMRI analyses, as the increase in the effective voxel size is small compared to the dimension of the Gaussian smoothing kernel (10 mm FWHM).</p>
      </sec>
      <sec>
        <title>BOLD sensitivity</title>
        <p>We calculated BOLD sensitivity maps for each subject based on spatially normalized field maps, and then created group average maps, shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>a. This was done for the Standard and Quiet sequence (as noted previously, these are identical to those for the Sparse and Matched Standard, respectively). Although both sequences exhibit lower signal around the temporal lobes, this was most pronounced in the Quiet sequence. Shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>b is the comparison of the BOLD sensitivity maps across sequence. The absolute difference in sensitivity is shown in the top panel; there were no regions that showed more sensitivity for the quiet sequence, but several regions that showed less sensitivity (cool colors). To quantify this difference we performed a pairwise <italic>t</italic>-test to compare the two sets of BOLD sensitivity maps across participants. The maps were smoothed with a 10 mm FWHM isotropic Gaussian kernel prior to analysis to match the fMRI data. The results in the bottom portion of <xref rid="fig2" ref-type="fig">Fig. 2</xref>b, and listed in <xref rid="tbl2" ref-type="table">Table 2</xref>, show significant differences in sensitivity using a voxelwise threshold of <italic>p</italic> &lt; .0001 (uncorrected) and a minimum cluster extent of 50 voxels which in all cases led to clusters which were whole-brain FWE-corrected for significance using cluster extent, <italic>p</italic> &lt; .05 [FWHM = 14.7 mm × 12.9 mm × 11.5 mm, resel count = 1216.4]. Differences in BOLD sensitivity were widespread, and included inferior temporal and orbital frontal regions where the sensitivity was lowest. Whole-brain maps of BOLD sensitivity and differences are shown in <xref rid="app1" ref-type="sec">Fig. S1–S4</xref>.</p>
      </sec>
      <sec>
        <title>Acoustic activity</title>
        <p>We used the SPM anatomy toolbox (<xref rid="bib18" ref-type="bibr">Eickhoff et al., 2005</xref>) to create a binary mask encompassing regions TE1.0 and TE1.1 of bilateral primary auditory cortex (<xref rid="bib35" ref-type="bibr">Morosan et al., 2001</xref>), outlined in blue in <xref rid="fig3" ref-type="fig">Fig. 3</xref>a. Regions included in the mask had at least an 80% chance of belonging to these subdivisions based on cytoarchitectonic characteristics. We then conducted a within-subjects ANOVA to identify voxels in which the main effect of SCN relative to the mean over scans differed by sequence type, shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>a. We used a cluster-defining voxelwise threshold of <italic>p</italic> &lt; .005 (uncorrected), and FWE-corrected for set-level significance within the search volume using cluster extent <italic>p</italic> &lt; .05 (<xref rid="bib57" ref-type="bibr">Worsley et al., 1992</xref>). For the peak voxel in each cluster, we then extracted the parameter estimates for each of the four sequences, shown in the top of <xref rid="fig3" ref-type="fig">Fig. 3</xref>a, to illustrate the nature of the difference between sequences. We also extracted the <italic>t</italic>-statistics from the first-level single-subject analysis, plotted along the bottom. These results indicate that in primary auditory regions the Quiet sequence gave the most robust response to SCN stimuli.</p>
      </sec>
      <sec>
        <title>Speech activity</title>
        <p>A second comparison of interest was the increase in activity when participants heard sentences compared to when they heard SCN, a contrast which results in robust language-related activation when data are collected using a sparse scanning sequence (<xref rid="bib14 bib44 bib49" ref-type="bibr">Davis et al., 2007; Rodd et al., 2005; Schwarzbauer et al., 2006</xref>). Results for the Sentences &gt; SCN contrast averaged across the four scanning sequences are shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>b and listed in <xref rid="tbl3" ref-type="table">Table 3</xref>, using a cluster-defining voxelwise threshold of <italic>p</italic> &lt; .005 (uncorrected), and whole-brain FWE-corrected for significance using cluster extent, <italic>p</italic> &lt; .05 [FWHM = 9.4 mm × 9.7 mm × 9.6 mm, resel count = 2653.5].</p>
        <p>To examine whether the Sentences &gt; SCN contrast differed as a function of scanning sequence we extracted parameter estimates and first-level <italic>t</italic>-statistics for each participant from five of the peak voxels identified by the Sentences &gt; SCN contrast, and subjected each to a one-way within-subjects ANOVA, Bonferroni-corrected to control for multiple comparisons within each dependent measure (i.e. separately for parameter estimates and <italic>t</italic>-statistics). For the parameter estimates, there were significant effects of sequence in the left posterior superior temporal gyrus (STG) [−68 −22 10: <italic>F</italic>(3,15) = 14.4, <italic>p</italic> &lt; .005] and right posterior STG [68 −12 −6: <italic>F</italic>(3,15) = 16.2, <italic>p</italic> &lt; .005]. None of the regions showed a significant effect of sequence on first-level <italic>t</italic>-statistics (all other <italic>p</italic>s &gt; .05).</p>
        <p>As noted previously, even sentences in which word report is equivalent can differ in the amount of effort required to achieve this level of intelligibility. To look for neural correlates of listening effort, we conducted a second-level paired-samples <italic>t</italic>-test comparing the Sentences &gt; SCN parameter estimates for the Standard and Quiet sequences, under the assumption that the acoustically louder Standard sequence would be more effortful than the Quiet sequence, and thus result in increased neural activity. This comparison is shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, with maxima listed in <xref rid="tbl4" ref-type="table">Table 4</xref>. We used a cluster-defining voxelwise threshold of <italic>p</italic> &lt; .005 (uncorrected), and whole-brain FWE-corrected for significance using cluster extent, <italic>p</italic> ≤ .052 [FWHM = 8.9 mm × 9.0 mm × 8.9 mm, resel count = 3305.9]. This analysis revealed several significant peaks along left superior temporal cortex, as well as left inferior parietal cortex (extending into angular gyrus).</p>
        <p>Finally, to assess the qualitative effect of different sequences on the Sentences &gt; SCN contrast at a group level, we performed group (random effects) analyses for each of the four sequences for this contrast. For each sequence the significant effect of Sentences &gt; SCN is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>a, with a cluster-defining voxelwise threshold of <italic>p</italic> &lt; .005 (uncorrected), and whole-brain FWE-corrected for significance using cluster extent, <italic>p</italic> &lt; .05. <xref rid="fig5" ref-type="fig">Fig. 5</xref>b shows the arbitrarily-thresholded subtraction of group <italic>t</italic>-statistics, demonstrating differences in group <italic>t</italic>-statistics for the Quiet sequence relative to the other three sequences. Of particular interest are the higher <italic>t</italic>-statistics near primary auditory areas (green circles) and lower <italic>t</italic>-statistics in inferior temporal regions (magenta circles) in the Quiet sequence relative to all other sequences. These qualitative observations are consistent with our quantitative comparisons, and support increased sensitivity of the Quiet sequence near primary auditory areas, and reduced sensitivity in inferior temporal regions.</p>
      </sec>
    </sec>
    <sec>
      <title>Discussion</title>
      <p>There are multiple challenges associated with using conventional echoplanar fMRI sequences to study neural responses to auditory stimuli due to acoustic scanner noise. Although for many years sparse imaging has proved to be a valuable approach, information regarding the timecourse of the BOLD response is lacking in these studies. An ideal sequence would be sufficiently quiet so as not to challenge auditory processing, and yet provide continuous information regarding brain activity. In the current report we set out to see whether the quiet EPI sequence introduced by <xref rid="bib47" ref-type="bibr">Schmitter et al. (2008)</xref> might fulfill this role. Previously it has been shown that this sequence increases sensitivity in primary auditory areas in response to pure tone stimulation (<xref rid="bib47" ref-type="bibr">Schmitter et al., 2008</xref>); in the current report we extended these results by including a greater number of comparison sequences, and by assessing the sensitivity of the Quiet sequence to both nonlinguistic auditory stimuli (i.e., SCN) and sentences. These are discussed in turn below.</p>
      <sec>
        <title>Responses to SCN in primary auditory cortex</title>
        <p>The neural response of auditory cortex to acoustic scanner noise has been a subject of interest for some time. It is clear that the considerable sound produced by echoplanar imaging produces robust responses in primary auditory cortex and surrounding areas (<xref rid="bib5 bib25 bib52" ref-type="bibr">Bandettini et al., 1998; Hall et al., 2000; Talavage et al., 1999</xref>), and that this elevated response can reduce task-related activation (<xref rid="bib19 bib21 bib27" ref-type="bibr">Elliott et al., 1999; Gaab et al., 2007; Haller et al., 2005</xref>). We hypothesized, therefore, that the Quiet sequence would show a greater response to basic task-related acoustic stimulation than the louder continuous sequences. This prediction was borne out: the Quiet sequence showed the largest response to SCN in cytoarchitectonically-defined primary auditory cortex.</p>
        <p>Perhaps somewhat surprising is that the response was larger in these auditory regions in the Quiet sequence than the Sparse sequence, in which there is no scanner noise during stimulus presentation. One possible reason for this is that, for a given period of scanning time, the Quiet sequence collects more scans compared to the Sparse sequence. Even if all other factors were held constant, this substantial increase in degrees of freedom would result in a more reliable response measurement. Although these additional degrees of freedom are reflected in the single-subject <italic>t</italic>-statistics, they do not affect the parameter estimates, in which the Quiet sequence also appears to show a significant advantage. This may be due to variability in the timecourse of the BOLD response, either across participants (<xref rid="bib1" ref-type="bibr">Aguirre et al., 1998</xref>) or across stimuli. That is, in a sparse imaging paradigm, the time of each stimulus relative to a scan is typically chosen so as to capture the BOLD response approximately 4–6 s after the event, and thus hopefully measure the peak response. However, because the timecourse of the response is not identical across stimuli or participants, it is possible for the single scan acquired in sparse imaging to miss the peak response. The continuous data acquisition afforded by the Quiet sequence may enhance the ability to detect the BOLD response from auditory regions. In principle, this may be especially true when longer stimuli, such as sentences, are used, as the estimation of when a discrete “event” occurs is not straightforward. However, if this was the case, we would expect the Quiet sequence to consistently outperform the Sparse sequence (i.e., in all conditions), as it should always be able to give a better estimation of the hemodynamic response. Because of the lack of differences in the Sentences &gt; SCN comparison (discussed below), then, this seems to be an unlikely explanation.</p>
        <p>A third possible reason why the Quiet sequence showed greater auditory activation than the Sparse sequence is that, although the Sparse sequence does not involve scanner noise concurrent with the stimuli, it is still relatively loud (∼ 90 dBA). This may induce other phasic changes in the response of auditory cortex which our TR of 11.2 s (which in the context of sparse imaging is fairly moderate) would not permit to completely return to baseline. This may be particularly relevant for sparse imaging as with this type of acoustic stimulation significant onset responses (i.e. edge detection) might be expected for each scan (<xref rid="bib8 bib29" ref-type="bibr">Chait et al., 2007; Herdener et al., 2007</xref>).</p>
      </sec>
      <sec>
        <title>Responses to sentences in frontal, temporal, and parietal cortices</title>
        <p>To identify a sentence processing network, we first looked for the effect of Sentences &gt; SCN averaged over scanning sequence. This analysis identified regions in frontal and temporal cortex that are in good agreement with a number of previous studies using the same comparison (<xref rid="bib14 bib36 bib44 bib49" ref-type="bibr">Davis et al., 2007; Mummery et al., 1999; Rodd et al., 2005; Schwarzbauer et al., 2006</xref>). Within these regions we selected 5 maxima at which to examine the difference in responses between sequences. Only the left and right posterior STG regions showed a significant difference in parameter estimates among sequences, with the Sparse sequence providing the largest parameter estimates. All other regions showed statistically equivalent parameter estimates across sequence, which was also the case for all regions with the first-level <italic>t</italic>-statistics. These results suggest that, in contrast to what was observed in primary auditory areas, the response in these sentence processing regions is largely insensitive to the scanning sequence used. The higher parameter estimate for the Sparse sequence may be explained by the decay of spin history effects, associated with the pause between scans, which typically provide larger yet more variable signal. This conclusion is supported by the lack of a clear advantage for the Sparse sequence in the first-level <italic>t</italic>-statistics (in which scan-by-scan variance is taken into account).</p>
        <p>One core sentence processing area not identified by our whole-brain analysis of Sentences &gt; SCN is the left posterior inferior temporal gyrus and/or nearby fusiform gyrus (hereafter referred to together as inferotemporal cortex), a region that has been gaining in prominence in theories of speech processing (<xref rid="bib30" ref-type="bibr">Hickok and Poeppel, 2007</xref>). In a voxel-based lesion-symptom mapping study, <xref rid="bib6" ref-type="bibr">Bates et al. (2003)</xref> showed that damage to this region in stroke patients is associated with reduced auditory sentence comprehension. In neuroimaging studies activity in inferotemporal regions has been reported for spoken words &gt; pseudowords (<xref rid="bib7 bib38" ref-type="bibr">Binder et al., 2000; Orfanidou et al., 2006</xref>; see also the meta-analysis in <xref rid="bib12" ref-type="bibr">Davis and Gaskell (2009)</xref>) and during comprehension of connected speech (<xref rid="bib4 bib10" ref-type="bibr">Awad et al., 2007; Crinion et al., 2003</xref>). Perhaps most relevant for the current discussion, robust left inferotemporal activity in response to sentences has been observed in previous studies using materials similar to the ones in the current study (<xref rid="bib14 bib44" ref-type="bibr">Davis et al., 2007; Rodd et al., 2005</xref>). As reviewed by <xref rid="bib41" ref-type="bibr">Price (2000)</xref>, activity in left inferotemporal cortex is common in word processing studies of both written and spoken language, including silent reading (words &gt; false font), auditory and visual word retrieval, reading Braille, and several types of naming. The common activity across different tasks and modalities suggests a critical role in language processing. Based primarily on studies of single word processing, <xref rid="bib41" ref-type="bibr">Price (2000)</xref> concluded that left inferotemporal cortex played a role in semantic processing. This is consistent with more recent studies of sentence processing in which sentences containing increased semantic ambiguity result in increased inferotemporal recruitment (<xref rid="bib14 bib44" ref-type="bibr">Davis et al., 2007; Rodd et al., 2005</xref>). Together the available evidence makes a compelling case for left posterior inferotemporal cortex playing a critical role in semantic processing, and we thus expect its ubiquitous involvement in determining the meaning of connected speech.</p>
        <p>The absence of inferotemporal activation in response to sentences in our current analysis is best explained by the significant signal dropout in this region in the Quiet and Matched Standard sequences caused by the longer duration of the EPI readout used in these sequences. In fact, the comparison of group results show that the Quiet sequence detected less sentence-related activity in this region than any of the other three sequences. Thus, the Quiet sequence appears to do as well as other sequences within most of these speech-processing regions, but is not sensitive to activations in inferotemporal cortex.</p>
        <p>The lack of posterior inferotemporal activation due to susceptibility effects in our study highlights the difficulty of obtaining adequate signal in this region, even with standard EPI sequences, which may contribute to the lack of consensus regarding speech-related responses in posterior inferior temporal cortex in fMRI studies. Even with standard TEs (i.e., 30 ms for a 3 T scanner), inadequate active shimming may lead to substantial signal loss, which may go unnoticed. We reiterate the recommendation of <xref rid="bib40" ref-type="bibr">Poldrack et al. (2008)</xref> that fMRI researchers check internally, and also report, average BOLD sensitivity and group coverage maps (e.g., the mask used in analysis) to provide an indication of the likelihood of detecting activations throughout the brain, particularly in regions prone to susceptibility artifact. An indication of the coverage provided by a mask (implicit, explicit, or a combination) is especially important if a proportional threshold is used in analysis. Given the increasing realization of the critical role played by inferior temporal regions in speech processing, this step would be extremely beneficial in interpreting the sometimes inconsistent fMRI results from this region.</p>
        <p>Finally, we have suggested above that even if stimuli can be perceived in the presence of scanner noise, the additional effort required to achieve this perception will result in increased cognitive demand. For example, <xref rid="bib13" ref-type="bibr">Davis and Johnsrude (2003)</xref> used three different acoustic manipulations to degrade sentence stimuli, and found that increased activity in large portions of left temporal, prefrontal and premotor cortices was associated with listening to the more degraded sentences. In the current study we hypothesized that due to increased energetic masking, additional neural activity would be associated with successful sentences comprehension during the Standard sequence relative to the Quiet sequence. Consistent with this prediction, we found significantly more speech-related activity in several areas of left temporal cortex, as well as left inferior parietal cortex. Although this finding must be interpreted with some caution due to the difference in scanning sequence, these areas did not show large differences in BOLD sensitivity, and thus we think it likely that these increases can be attributed to the different acoustic environments of the Standard and Quiet sequences. This finding underscores the importance of listening effort in interpreting data from auditory fMRI studies, and in particular the effects of standard EPI noise on required effort. Interestingly, contrary to <xref rid="bib13" ref-type="bibr">Davis and Johnsrude (2003)</xref>, we did not see significant increases in frontal regions associated with listening effort. This might be due to the relatively small sample size in the current study, or may instead reflect different processes required when acoustic degradation is intrinsic to the speech signal and hence originates from the same spatial location as the noise (<xref rid="bib13" ref-type="bibr">Davis and Johnsrude, 2003</xref>) or spatially separate (as in the current study).</p>
      </sec>
    </sec>
    <sec>
      <title>Conclusions</title>
      <p>Relative to standard continuous EPI sequences or popular sparse imaging approaches, the Quiet scanning sequence we evaluated (<xref rid="bib47" ref-type="bibr">Schmitter et al., 2008</xref>) provides increased sensitivity to acoustic response in primary auditory regions, and a comparable response to sentences within a fronto-temporal speech network. The most notable disadvantage of the Quiet sequence was an increase in signal dropout in inferior temporal cortex associated with its longer EPI readout duration, and the resulting lack of sensitivity to language-related activation in this region. As a future improvement, parallel imaging acquisition techniques could be used to reduce the duration of the EPI readout, which would considerably improve its sensitivity of the Quiet EPI in regions affected by macroscopic magnetic field inhomogeneities. Future work comparing the effect of sound level reductions (as in the current study) and changes in perceived continuity (<xref rid="bib51" ref-type="bibr">Seifritz et al., 2006</xref>) or active noise cancelling (<xref rid="bib26" ref-type="bibr">Hall et al., 2009</xref>) would also be of interest.</p>
      <p>It is important to emphasize that various types of stimuli may interact with acoustic scanner noise differently; one example of this being the different effects we have reported for SCN and Sentences &gt; SCN. Thus, care should be taken when generalizing our findings to novel stimuli. However, the current results suggest that technical developments in acoustically quiet echoplanar imaging have the potential to increase sensitivity to the BOLD response for a wide variety of auditory stimuli. These approaches may be most helpful when acoustic-related activity in and near primary auditory cortex is of primary interest. In addition, they may have applicability in non-auditory tasks in service of increasing participant comfort and removing potential task confounds, provided areas prone to susceptibility effects are not of interest.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aguirre</surname>
              <given-names>G.K.</given-names>
            </name>
            <name>
              <surname>Zarahn</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>D'Esposito</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>The variability of human, BOLD hemodynamic responses</article-title>
          <source>NeuroImage</source>
          <volume>8</volume>
          <year>1998</year>
          <fpage>360</fpage>
          <lpage>369</lpage>
          <pub-id pub-id-type="pmid">9811554</pub-id>
        </element-citation>
      </ref>
      <ref id="bib2">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Multimodal image coregistration and partitioning — a unified framework</article-title>
          <source>NeuroImage</source>
          <volume>6</volume>
          <year>1997</year>
          <fpage>209</fpage>
          <lpage>217</lpage>
          <pub-id pub-id-type="pmid">9344825</pub-id>
        </element-citation>
      </ref>
      <ref id="bib3">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Unified segmentation</article-title>
          <source>NeuroImage</source>
          <volume>26</volume>
          <year>2005</year>
          <fpage>839</fpage>
          <lpage>851</lpage>
          <pub-id pub-id-type="pmid">15955494</pub-id>
        </element-citation>
      </ref>
      <ref id="bib4">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Awad</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Warren</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Turkheimer</surname>
              <given-names>F.E.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.S.</given-names>
            </name>
          </person-group>
          <article-title>A common system for the comprehension and production of narrative speech</article-title>
          <source>J. Neurosci.</source>
          <volume>27</volume>
          <year>2007</year>
          <fpage>11455</fpage>
          <lpage>11464</lpage>
          <pub-id pub-id-type="pmid">17959788</pub-id>
        </element-citation>
      </ref>
      <ref id="bib5">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bandettini</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Jesmanowicz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Van Kylen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Birn</surname>
              <given-names>R.M.</given-names>
            </name>
            <name>
              <surname>Hyde</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Functional MRI of brain activation induced by scanner acoustic noise</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>39</volume>
          <year>1998</year>
          <fpage>410</fpage>
          <lpage>416</lpage>
          <pub-id pub-id-type="pmid">9498597</pub-id>
        </element-citation>
      </ref>
      <ref id="bib6">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bates</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Saygin</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Dick</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sereno</surname>
              <given-names>M.I.</given-names>
            </name>
            <name>
              <surname>Knight</surname>
              <given-names>R.T.</given-names>
            </name>
            <name>
              <surname>Dronkers</surname>
              <given-names>N.F.</given-names>
            </name>
          </person-group>
          <article-title>Voxel-based lesion-symptom mapping</article-title>
          <source>Nat. Neurosci.</source>
          <volume>6</volume>
          <year>2003</year>
          <fpage>448</fpage>
          <lpage>450</lpage>
          <pub-id pub-id-type="pmid">12704393</pub-id>
        </element-citation>
      </ref>
      <ref id="bib7">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Binder</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Frost</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Hammeke</surname>
              <given-names>T.A.</given-names>
            </name>
            <name>
              <surname>Bellgowan</surname>
              <given-names>P.S.</given-names>
            </name>
            <name>
              <surname>Springer</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Kaufman</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Possing</surname>
              <given-names>E.T.</given-names>
            </name>
          </person-group>
          <article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title>
          <source>Cereb. Cortex</source>
          <volume>10</volume>
          <year>2000</year>
          <fpage>512</fpage>
          <lpage>528</lpage>
          <pub-id pub-id-type="pmid">10847601</pub-id>
        </element-citation>
      </ref>
      <ref id="bib8">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chait</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>de Cheveigné</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Simon</surname>
              <given-names>J.Z.</given-names>
            </name>
          </person-group>
          <article-title>Processing asymmetry of transitions between order and disorder in human auditory cortex</article-title>
          <source>J. Neurosci.</source>
          <volume>27</volume>
          <year>2007</year>
          <fpage>5207</fpage>
          <lpage>5214</lpage>
          <pub-id pub-id-type="pmid">17494707</pub-id>
        </element-citation>
      </ref>
      <ref id="bib9">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chambers</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bullock</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kahana</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kots</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Developments in active noise control sound system for magnetic resonance imaging.</article-title>
          <source>Appl Acoust</source>
          <volume>68</volume>
          <year>2007</year>
          <fpage>281</fpage>
          <lpage>295</lpage>
        </element-citation>
      </ref>
      <ref id="bib10">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crinion</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Lambon Ralph</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Warburton</surname>
              <given-names>E.A</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.S</given-names>
            </name>
            <name>
              <surname>Hyde</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Temporal lobe regions engaged during normal speech comprehension.</article-title>
          <source>Brain</source>
          <volume>126</volume>
          <year>2003</year>
          <fpage>1193</fpage>
          <lpage>1201</lpage>
          <pub-id pub-id-type="pmid">12690058</pub-id>
        </element-citation>
      </ref>
      <ref id="bib11">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cusack</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Brett</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Osswald</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>An evaluation of the use of magnetic field maps to undistort echo-planar images</article-title>
          <source>NeuroImage</source>
          <volume>18</volume>
          <year>2003</year>
          <fpage>127</fpage>
          <lpage>142</lpage>
          <pub-id pub-id-type="pmid">12507450</pub-id>
        </element-citation>
      </ref>
      <ref id="bib12">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Gaskell</surname>
              <given-names>M.G.</given-names>
            </name>
          </person-group>
          <article-title>A complementary systems account of word learning: neural and behavioural evidence</article-title>
          <source>Philos. Roy. Soc. B</source>
          <volume>364</volume>
          <year>2009</year>
          <fpage>3773</fpage>
          <lpage>3800</lpage>
        </element-citation>
      </ref>
      <ref id="bib13">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.S.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical processing in spoken language comprehension</article-title>
          <source>J. Neurosci.</source>
          <volume>23</volume>
          <year>2003</year>
          <fpage>3423</fpage>
          <lpage>3431</lpage>
          <pub-id pub-id-type="pmid">12716950</pub-id>
        </element-citation>
      </ref>
      <ref id="bib14">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Coleman</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Absalom</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Rodd</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.S.</given-names>
            </name>
            <name>
              <surname>Matta</surname>
              <given-names>B.F.</given-names>
            </name>
            <name>
              <surname>Owen</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Menon</surname>
              <given-names>D.K.</given-names>
            </name>
          </person-group>
          <article-title>Dissociating speech perception and comprehension at reduced levels of awareness</article-title>
          <source>Proc. Natl. Acad. Sci. U. S. A.</source>
          <volume>104</volume>
          <year>2007</year>
          <fpage>16032</fpage>
          <lpage>16037</lpage>
          <pub-id pub-id-type="pmid">17938125</pub-id>
        </element-citation>
      </ref>
      <ref id="bib15">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Panfilis</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Schwarzbauer</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Positive or negative blips? The effect of phase encoding scheme on susceptibility-induced signal losses in EPI</article-title>
          <source>NeuroImage</source>
          <volume>25</volume>
          <year>2005</year>
          <fpage>112</fpage>
          <lpage>121</lpage>
          <pub-id pub-id-type="pmid">15734348</pub-id>
        </element-citation>
      </ref>
      <ref id="bib16">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Devlin</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Russell</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Moss</surname>
              <given-names>H.E.</given-names>
            </name>
            <name>
              <surname>Matthews</surname>
              <given-names>P.M.</given-names>
            </name>
            <name>
              <surname>Tyler</surname>
              <given-names>L.K.</given-names>
            </name>
          </person-group>
          <article-title>Susceptibility-induced loss of signal: comparing PET and fMRI on a semantic task</article-title>
          <source>NeuroImage</source>
          <volume>11</volume>
          <year>2000</year>
          <fpage>589</fpage>
          <lpage>600</lpage>
          <pub-id pub-id-type="pmid">10860788</pub-id>
        </element-citation>
      </ref>
      <ref id="bib17">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Edmister</surname>
              <given-names>W.B.</given-names>
            </name>
            <name>
              <surname>Talavage</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Ledden</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Weisskoff</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>Improved auditory cortex imaging using clustered volume acquisitions</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>7</volume>
          <year>1999</year>
          <fpage>89</fpage>
          <lpage>97</lpage>
          <pub-id pub-id-type="pmid">9950066</pub-id>
        </element-citation>
      </ref>
      <ref id="bib18">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eickhoff</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mohlberg</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Grefkes</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fink</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Amunts</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zilles</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>
          <source>NeuroImage</source>
          <volume>25</volume>
          <year>2005</year>
          <fpage>1325</fpage>
          <lpage>1335</lpage>
          <pub-id pub-id-type="pmid">15850749</pub-id>
        </element-citation>
      </ref>
      <ref id="bib19">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Elliott</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Bowtell</surname>
              <given-names>R.W.</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>P.G.</given-names>
            </name>
          </person-group>
          <article-title>The effect of scanner sound in visual, motor, and auditory functional MRI</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>41</volume>
          <year>1999</year>
          <fpage>1230</fpage>
          <lpage>1235</lpage>
          <pub-id pub-id-type="pmid">10371456</pub-id>
        </element-citation>
      </ref>
      <ref id="bib20">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.-B.</given-names>
            </name>
            <name>
              <surname>Heather</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
          </person-group>
          <article-title>Spatial registration and normalization of images</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>2</volume>
          <year>1995</year>
          <fpage>165</fpage>
          <lpage>189</lpage>
        </element-citation>
      </ref>
      <ref id="bib21">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gaab</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Gabrieli</surname>
              <given-names>J.D.E.</given-names>
            </name>
            <name>
              <surname>Glover</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Assessing the influence of scanner background noise on auditory processing. II. An fMRI study comparing auditory processing in the absence and presence of recorded scanner noise using a sparse design</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>28</volume>
          <year>2007</year>
          <fpage>721</fpage>
          <lpage>732</lpage>
          <pub-id pub-id-type="pmid">17089376</pub-id>
        </element-citation>
      </ref>
      <ref id="bib22">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Giraud</surname>
              <given-names>A.-L.</given-names>
            </name>
            <name>
              <surname>Lorenzi</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wable</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Representation of the temporal envelope of sounds in the human brain</article-title>
          <source>J. Neurophysiol.</source>
          <volume>84</volume>
          <year>2000</year>
          <fpage>1588</fpage>
          <lpage>1598</lpage>
          <pub-id pub-id-type="pmid">10980029</pub-id>
        </element-citation>
      </ref>
      <ref id="bib23">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grimault</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Micheyl</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Carlyon</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Arthaud</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Collet</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Perceptual auditory stream segregation of sequences of complex sounds in subjects with normal and impaired hearing</article-title>
          <source>Br. J. Audiol.</source>
          <volume>35</volume>
          <year>2001</year>
          <fpage>173</fpage>
          <lpage>182</lpage>
          <pub-id pub-id-type="pmid">11548044</pub-id>
        </element-citation>
      </ref>
      <ref id="bib24">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Haggard</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Akeroyd</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
            <name>
              <surname>Elliott</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Gurney</surname>
              <given-names>E.M.</given-names>
            </name>
            <name>
              <surname>Bowtell</surname>
              <given-names>R.W.</given-names>
            </name>
          </person-group>
          <article-title>“Sparse” temporal sampling in auditory fMRI</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>7</volume>
          <year>1999</year>
          <fpage>213</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="pmid">10194620</pub-id>
        </element-citation>
      </ref>
      <ref id="bib25">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
            <name>
              <surname>Gonçalves</surname>
              <given-names>M.S.</given-names>
            </name>
            <name>
              <surname>Foster</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Bowtell</surname>
              <given-names>R.W.</given-names>
            </name>
          </person-group>
          <article-title>Time-course of the auditory BOLD response to scanner noise</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>43</volume>
          <year>2000</year>
          <fpage>601</fpage>
          <lpage>606</lpage>
          <pub-id pub-id-type="pmid">10748437</pub-id>
        </element-citation>
      </ref>
      <ref id="bib26">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Chambers</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Akeroyd</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Foster</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic, psychophysical, and neuroimaging measurements of the effectiveness of active cancellation during auditory functional magnetic resonance imaging</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <volume>125</volume>
          <year>2009</year>
          <fpage>347</fpage>
          <lpage>359</lpage>
          <pub-id pub-id-type="pmid">19173422</pub-id>
        </element-citation>
      </ref>
      <ref id="bib27">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haller</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bartsch</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Radue</surname>
              <given-names>E.W.</given-names>
            </name>
            <name>
              <surname>Klarhöfer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Seifritz</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Scheffler</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Effect of fMRI acoustic noise on non-auditory working memory task: comparison between continuous and pulsed sound emitting EPI</article-title>
          <source>Magn. Reson. Mater. Phys.</source>
          <volume>18</volume>
          <year>2005</year>
          <fpage>263</fpage>
          <lpage>271</lpage>
        </element-citation>
      </ref>
      <ref id="bib28">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Harms</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Melcher</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Sound repetition rate in the human auditory pathway: representations in the waveshape and amplitude of fMRI activation</article-title>
          <source>J. Neurophysiol.</source>
          <volume>88</volume>
          <year>2002</year>
          <fpage>1433</fpage>
          <lpage>1450</lpage>
          <pub-id pub-id-type="pmid">12205164</pub-id>
        </element-citation>
      </ref>
      <ref id="bib29">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Herdener</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Esposito</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Di Salle</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Lehmann</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Bach</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Scheffler</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Seifritz</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>BOLD correlates of edge detection in human auditory cortex</article-title>
          <source>NeuroImage</source>
          <volume>36</volume>
          <year>2007</year>
          <fpage>194</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="pmid">17395491</pub-id>
        </element-citation>
      </ref>
      <ref id="bib30">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hickok</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>The cortical organization of speech processing</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <volume>8</volume>
          <year>2007</year>
          <fpage>393</fpage>
          <lpage>402</lpage>
          <pub-id pub-id-type="pmid">17431404</pub-id>
        </element-citation>
      </ref>
      <ref id="bib31">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Fast, automated N-dimensional phase-unwrapping algorithm</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>49</volume>
          <year>2003</year>
          <fpage>193</fpage>
          <lpage>197</lpage>
          <pub-id pub-id-type="pmid">12509838</pub-id>
        </element-citation>
      </ref>
      <ref id="bib32">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Krüger</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Glover</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Physiological noise in oxygenation-sensitive magnetic resonance imaging</article-title>
          <source>Magn. Reson. Med.</source>
          <volume>46</volume>
          <year>2001</year>
          <fpage>631</fpage>
          <lpage>637</lpage>
          <pub-id pub-id-type="pmid">11590638</pub-id>
        </element-citation>
      </ref>
      <ref id="bib33">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Loftus</surname>
              <given-names>G.R.</given-names>
            </name>
            <name>
              <surname>Masson</surname>
              <given-names>M.E.J.</given-names>
            </name>
          </person-group>
          <article-title>Using confidence intervals in within-subject designs</article-title>
          <source>Psychon. Bull. Rev.</source>
          <volume>1</volume>
          <year>1994</year>
          <fpage>476</fpage>
          <lpage>490</lpage>
        </element-citation>
      </ref>
      <ref id="bib34">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moelker</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pattynama</surname>
              <given-names>P.M.T.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic noise concerns in functional magnetic resonance imaging</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>20</volume>
          <year>2003</year>
          <fpage>123</fpage>
          <lpage>141</lpage>
          <pub-id pub-id-type="pmid">14601139</pub-id>
        </element-citation>
      </ref>
      <ref id="bib35">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Morosan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Rademacher</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Schleicher</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Amunts</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Schormann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zilles</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Human primary auditory cortex: cytoarchitectonic subdivisions and mapping into a spatial reference system</article-title>
          <source>NeuroImage</source>
          <volume>13</volume>
          <year>2001</year>
          <fpage>684</fpage>
          <lpage>701</lpage>
          <pub-id pub-id-type="pmid">11305897</pub-id>
        </element-citation>
      </ref>
      <ref id="bib36">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mummery</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Functional neuroimaging of speech perception in six normal and two aphasic subjects</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <volume>106</volume>
          <year>1999</year>
          <fpage>449</fpage>
          <lpage>457</lpage>
          <pub-id pub-id-type="pmid">10420635</pub-id>
        </element-citation>
      </ref>
      <ref id="bib37">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ojemann</surname>
              <given-names>J.G.</given-names>
            </name>
            <name>
              <surname>Akbudak</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Snyder</surname>
              <given-names>A.Z.</given-names>
            </name>
            <name>
              <surname>McKinstry</surname>
              <given-names>R.C.</given-names>
            </name>
            <name>
              <surname>Raichle</surname>
              <given-names>M.E.</given-names>
            </name>
            <name>
              <surname>Conturo</surname>
              <given-names>T.E.</given-names>
            </name>
          </person-group>
          <article-title>Anatomic localization and quantitative analysis of gradient refocused echo-planar fMRI susceptibility artifacts</article-title>
          <source>NeuroImage</source>
          <volume>6</volume>
          <year>1997</year>
          <fpage>156</fpage>
          <lpage>167</lpage>
          <pub-id pub-id-type="pmid">9344820</pub-id>
        </element-citation>
      </ref>
      <ref id="bib38">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Orfanidou</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Marslen-Wilson</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
          </person-group>
          <article-title>Neural response suppression predicts repetition priming of spoken words and pseudowords</article-title>
          <source>J. Cogn. Neurosci.</source>
          <volume>18</volume>
          <year>2006</year>
          <fpage>1237</fpage>
          <lpage>1252</lpage>
          <pub-id pub-id-type="pmid">16859411</pub-id>
        </element-citation>
      </ref>
      <ref id="bib39">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peelle</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Wingfield</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Dissociations in perceptual learning revealed by adult age differences in adaptation to time-compressed speech</article-title>
          <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
          <volume>31</volume>
          <year>2005</year>
          <fpage>1315</fpage>
          <lpage>1330</lpage>
          <pub-id pub-id-type="pmid">16366792</pub-id>
        </element-citation>
      </ref>
      <ref id="bib40">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Poldrack</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Fletcher</surname>
              <given-names>P.C.</given-names>
            </name>
            <name>
              <surname>Henson</surname>
              <given-names>R.N.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Brett</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Nichols</surname>
              <given-names>T.E.</given-names>
            </name>
          </person-group>
          <article-title>Guidelines for reporting an fMRI study</article-title>
          <source>NeuroImage</source>
          <volume>40</volume>
          <year>2008</year>
          <fpage>409</fpage>
          <lpage>414</lpage>
          <pub-id pub-id-type="pmid">18191585</pub-id>
        </element-citation>
      </ref>
      <ref id="bib41">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>The anatomy of language: contributions from functional neuroimaging</article-title>
          <source>J. Anat.</source>
          <volume>197</volume>
          <year>2000</year>
          <fpage>335</fpage>
          <lpage>359</lpage>
          <pub-id pub-id-type="pmid">11117622</pub-id>
        </element-citation>
      </ref>
      <ref id="bib42">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Price</surname>
              <given-names>D.L.</given-names>
            </name>
            <name>
              <surname>De Wilde</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Papadaki</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Curran</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Kitney</surname>
              <given-names>R.I.</given-names>
            </name>
          </person-group>
          <article-title>Investigation of acoustic noise on 15 MRI scanners from 0.2T to 3T</article-title>
          <source>J. Magn. Reson. Imaging</source>
          <volume>13</volume>
          <year>2001</year>
          <fpage>288</fpage>
          <lpage>293</lpage>
          <pub-id pub-id-type="pmid">11169836</pub-id>
        </element-citation>
      </ref>
      <ref id="bib43">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ravicz</surname>
              <given-names>M.E.</given-names>
            </name>
            <name>
              <surname>Melcher</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Kiang</surname>
              <given-names>N.Y.-S.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic noise during functional magnetic resonance imaging</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <volume>108</volume>
          <year>2000</year>
          <fpage>1683</fpage>
          <lpage>1696</lpage>
          <pub-id pub-id-type="pmid">11051496</pub-id>
        </element-citation>
      </ref>
      <ref id="bib44">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rodd</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.S.</given-names>
            </name>
          </person-group>
          <article-title>The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity</article-title>
          <source>Cereb. Cortex</source>
          <volume>15</volume>
          <year>2005</year>
          <fpage>1261</fpage>
          <lpage>1269</lpage>
          <pub-id pub-id-type="pmid">15635062</pub-id>
        </element-citation>
      </ref>
      <ref id="bib45">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rorden</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Brett</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Stereotaxic display of brain lesions</article-title>
          <source>Behav. Neurol.</source>
          <volume>12</volume>
          <year>2000</year>
          <fpage>191</fpage>
          <lpage>2000</lpage>
          <pub-id pub-id-type="pmid">11568431</pub-id>
        </element-citation>
      </ref>
      <ref id="bib46">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scheffler</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Bilecen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Schmid</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Tschopp</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Seelig</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Auditory cortical responses in hearing subjects and unilateral deaf patients as detected by functional magnetic resonance imaging</article-title>
          <source>Cereb. Cortex</source>
          <volume>8</volume>
          <year>1998</year>
          <fpage>156</fpage>
          <lpage>163</lpage>
          <pub-id pub-id-type="pmid">9542894</pub-id>
        </element-citation>
      </ref>
      <ref id="bib47">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schmitter</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Diesch</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Amann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kroll</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Moayer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Schad</surname>
              <given-names>L.R.</given-names>
            </name>
          </person-group>
          <article-title>Silent echo-planar imaging for auditory FMRI</article-title>
          <source>Magn. Reson. Mater. Phys.</source>
          <volume>21</volume>
          <year>2008</year>
          <fpage>317</fpage>
          <lpage>325</lpage>
        </element-citation>
      </ref>
      <ref id="bib48">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schroeder</surname>
              <given-names>M.R.</given-names>
            </name>
          </person-group>
          <article-title>Reference signal for signal quality studies</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <volume>44</volume>
          <year>1968</year>
          <fpage>1735</fpage>
          <lpage>1736</lpage>
        </element-citation>
      </ref>
      <ref id="bib49">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarzbauer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Rodd</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Interleaved silent steady state (ISSS) imaging: a new sparse imaging method applied to auditory fMRI</article-title>
          <source>NeuroImage</source>
          <volume>29</volume>
          <year>2006</year>
          <fpage>774</fpage>
          <lpage>782</lpage>
          <pub-id pub-id-type="pmid">16226896</pub-id>
        </element-citation>
      </ref>
      <ref id="bib50">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Seifritz</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Di Salle</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Esposito</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Bilecen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Neuhoff</surname>
              <given-names>J.G.</given-names>
            </name>
            <name>
              <surname>Scheffler</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Sustained blood oxygenation and volume response to repetition rate-modulated sound in human auditory cortex</article-title>
          <source>NeuroImage</source>
          <volume>20</volume>
          <year>2003</year>
          <fpage>1365</fpage>
          <lpage>1370</lpage>
          <pub-id pub-id-type="pmid">14568505</pub-id>
        </element-citation>
      </ref>
      <ref id="bib51">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Seifritz</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Di Salle</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Esposito</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Herdener</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Neuhoff</surname>
              <given-names>J.G.</given-names>
            </name>
            <name>
              <surname>Scheffler</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Enhancing BOLD response in the auditory system by neurophysiologically tuned fMRI sequence</article-title>
          <source>NeuroImage</source>
          <volume>29</volume>
          <year>2006</year>
          <fpage>1013</fpage>
          <lpage>1022</lpage>
          <pub-id pub-id-type="pmid">16253522</pub-id>
        </element-citation>
      </ref>
      <ref id="bib52">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Talavage</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Edmister</surname>
              <given-names>W.B.</given-names>
            </name>
            <name>
              <surname>Ledden</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Weisskoff</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>Quantitative assessment of auditory cortex responses induced by imager acoustic noise</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>7</volume>
          <year>1999</year>
          <fpage>79</fpage>
          <lpage>88</lpage>
          <pub-id pub-id-type="pmid">9950065</pub-id>
        </element-citation>
      </ref>
      <ref id="bib53">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tanaka</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Fujita</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Watanabe</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hirabuki</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Takanashi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Oshiro</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Nakamura</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Effects of stimulus rate on the auditory cortex using fMRI with ‘sparse’ temporal sampling</article-title>
          <source>NeuroReport</source>
          <volume>11</volume>
          <year>2000</year>
          <fpage>2045</fpage>
          <lpage>2049</lpage>
          <pub-id pub-id-type="pmid">10884068</pub-id>
        </element-citation>
      </ref>
      <ref id="bib54">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Casteren</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
          </person-group>
          <article-title>Match: a program to assist in matching the conditions of factorial experiments</article-title>
          <source>Behav. Res. Meth.</source>
          <volume>39</volume>
          <year>2007</year>
          <fpage>973</fpage>
          <lpage>978</lpage>
        </element-citation>
      </ref>
      <ref id="bib55">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Weiskopf</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Hutton</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Josephs</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Deichmann</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Optimized EPI for fMRI studies of the orbitofrontal cortex: compensation of susceptibility-induced gradients in the readout direction</article-title>
          <source>Magn. Reson. Mater. Phys.</source>
          <volume>20</volume>
          <year>2007</year>
          <fpage>39</fpage>
          <lpage>49</lpage>
        </element-citation>
      </ref>
      <ref id="bib56">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wingfield</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>McCoy</surname>
              <given-names>S.L.</given-names>
            </name>
            <name>
              <surname>Peelle</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Tun</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Cox</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Effects of adult aging and hearing loss on comprehension of rapid speech varying in syntactic complexity</article-title>
          <source>J. Am. Acad. Audiol.</source>
          <volume>17</volume>
          <year>2006</year>
          <fpage>487</fpage>
          <lpage>497</lpage>
          <pub-id pub-id-type="pmid">16927513</pub-id>
        </element-citation>
      </ref>
      <ref id="bib57">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Marrett</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Neelin</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>A three-dimensional statistical analysis for CBF activation studies in human brain</article-title>
          <source>J. Cereb. Blood Flow Metab.</source>
          <volume>12</volume>
          <year>1992</year>
          <fpage>900</fpage>
          <lpage>918</lpage>
          <pub-id pub-id-type="pmid">1400644</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="app1" sec-type="supplementary-material">
      <label>Appendix A</label>
      <title>Supplementary data</title>
      <p>
        <supplementary-material content-type="local-data" id="d30e2194">
          <caption>
            <title>Fig. S1</title>
            <p>Mean BOLD sensitivity maps for the Standard sequence overlaid on a template brain. Axial slices are shown from −35 mm to 60 mm. Blue regions around orbital frontal and inferior temporal cortex indicate regions of low sensitivity.</p>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="d30e2206">
          <caption>
            <title>Fig. S2</title>
            <p>Mean BOLD sensitivity maps for the Quiet sequence overlaid on a template brain. Axial slices are shown from −35 mm to 60 mm. Blue regions around orbital frontal and inferior temporal cortex indicate regions of low sensitivity.</p>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="d30e2218">
          <caption>
            <title>Fig. S3</title>
            <p>Absolute difference in BOLD sensitivity for the Quiet compared to the Standard sequence overlaid on a template brain. Axial slices are shown from −35 mm to 60 mm. Cool colors indicate regions where the Quiet sequence is less sensitive than the Standard sequence.</p>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="d30e2230">
          <caption>
            <title>Fig. S4</title>
            <p>SPM{<italic>t</italic>} map showing voxels in which the BOLD sensitivity for the Quiet sequence was significantly lower than that of the Standard sequence, using a cluster-defining threshold of <italic>p</italic> &lt; .0001 (uncorrected) and a cluster extent of 50 voxels; all of the resulting clusters are whole-brain corrected for family wise error based on cluster extent, <italic>p</italic> &lt; .001. Axial slices of a template brain are shown from −35 mm to 60 mm.</p>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
      </p>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>This research was supported by the Medical Research Council (U.1055.04.013.01.01). We are grateful to Steve Eldridge and Helen Lloyd for their assistance with data collection, Stefan Hetzer for helpful discussions, and to our volunteers for their participation. Portions of this work were presented at the Annual Meeting of the Organization for Human Brain Mapping, June 2009.</p>
    </ack>
  </back>
  <floats-group>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Signal-to-noise ratios calculated for the four EPI sequences tested. Error bars represent 1 standard error of the mean with between-subjects variance removed, suitable for within-subjects comparisons (<xref rid="bib33" ref-type="bibr">Loftus and Masson, 1994</xref>). The increase in SNR for the Quiet sequence is due to its increased smoothness (see text for details); this effect is negligible in the activation analyses because of the smoothing applied during preprocessing.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>(a) Mean BOLD sensitivity maps for the Standard and Quiet sequences (which are identical for the Sparse and Matched Standard sequences, respectively). (b) Comparison of BOLD sensitivity maps. The effect size from the subtraction is shown at the top: warmer colors indicate voxels where the Quiet sequence shows increased sensitivity, and cool colors show decreased sensitivity relative to the Standard sequence. The bottom shows the results of a paired-samples <italic>t</italic>-test on smoothed BOLD sensitivity maps (Standard &gt; Quiet, voxelwise threshold of <italic>p</italic> &lt; .0001 uncorrected, cluster extent of 50 voxels, all reaching full brain significance for cluster extent).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>(a) Within cytoarchitectonically-defined bilateral primary auditory cortex (blue outline), we conducted a second-level within-subjects <italic>F</italic> test to identify voxels in which activity for SCN (relative to the mean over scans) differed significantly by imaging sequence. For two peaks resulting from this contrast, parameter estimates (top) and <italic>t</italic>-statistics (bottom) were extracted to illustrate the data driving the effect. (b) Whole-brain analysis to look for regions showing more activity for Sentences than SCN, averaged over scanning sequence, <italic>p</italic> &lt; .005 voxelwise threshold corrected for whole-brain significance using cluster extent (<italic>p</italic> &lt; .05). For five peaks resulting from this contrast, parameter estimates and <italic>t</italic>-statistics were extracted to examine the effects of imaging sequence on neural activity; *indicates significant ANOVA for a difference by sequence. MNI coordinates for extracted maxima are given above each plot. Abbreviations: Qu = Quiet, Sp = Sparse, St = Standard, MSt = Matched Standard sequence. Error bars represent 1 standard error of the mean with between-subjects variance removed, suitable for within-subjects comparisons.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Effects of listening effort. Sentences &gt; SCN activity which is significantly increased for the Standard sequence greater than the Quiet sequence, voxelwise <italic>p</italic> &lt; .005, corrected for whole-brain significance (<italic>p</italic> ≤ .052) using cluster extent.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="fig5">
      <label>Fig. 5</label>
      <caption>
        <p>Qualitative comparison of results for the four imaging sequences. (a) Whole-brain group analyses for Sentences &gt; SCN for each of the four imaging sequences, voxelwise <italic>p</italic> &lt; .005, corrected for whole-brain significance (<italic>p</italic> &lt; .05) using cluster extent. (b) Differences in group <italic>t</italic>-statistics between the Quiet sequence and the other three sequences. All voxels shown differ by 2 or more. Cool colors indicate voxels in which the Quiet sequence showed lower <italic>t</italic>-statistics than other sequences; warm colors where it showed higher <italic>t</italic>-statistics than other sequences. Highlighted are primary auditory cortex (green circles) and inferior temporal/fusiform gyrus (magenta circles).</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <p>Summary of EPI sequences tested.</p>
      </caption>
      <table-wrap-foot>
        <fn>
          <p>Note: Sound levels were measured for the Standard and Quiet sequences with an MR-compatible microphone placed inside an empty scanner. Measurements were not taken for the Matched Standard sequence but it was uniformly judged to be louder than the Quiet sequence, and softer than the Standard sequence. During the Sparse sequence the only noise came from ambient scanner noise (e.g., helium pumps).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="tbl2" position="float">
      <label>Table 2</label>
      <caption>
        <p>Maxima of regions showing decreased BOLD sensitivity for the Quiet sequence compared to the Standard sequence.</p>
      </caption>
      <table-wrap-foot>
        <fn>
          <p>Note: When areas of significant difference occur outside the brain (i.e. near brain/air boundaries), these are described by the nearest cortical landmark.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="tbl3" position="float">
      <label>Table 3</label>
      <caption>
        <p>Maxima of regions for Sentences &gt; SCN contrast (averaged across scanning sequence).</p>
      </caption>
    </table-wrap>
    <table-wrap id="tbl4" position="float">
      <label>Table 4</label>
      <caption>
        <p>Maxima of regions showing effects of listening effort (sentence-related neural activity greater in Standard than Quiet sequence).</p>
      </caption>
    </table-wrap>
  </floats-group>
</article>