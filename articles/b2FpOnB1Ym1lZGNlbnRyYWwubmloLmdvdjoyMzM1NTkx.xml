<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="announcement">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Curr Biol</journal-id>
      <journal-title>Current Biology </journal-title>
      <issn pub-type="ppub">0960-9822</issn>
      <issn pub-type="epub">1879-0445</issn>
      <publisher>
        <publisher-name>Cell Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2335591</article-id>
      <article-id pub-id-type="pmid">17600716</article-id>
      <article-id pub-id-type="publisher-id">CURBIO5682</article-id>
      <article-id pub-id-type="doi">10.1016/j.cub.2007.05.061</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Report</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Neural Representation of Auditory Size in the Human Voice and in Sounds from Other Resonant Sources</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>von Kriegstein</surname>
            <given-names>Katharina</given-names>
          </name>
          <email>kkriegs@fil.ion.ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="aff2" ref-type="aff">2</xref>
          <xref rid="cor1" ref-type="corresp">∗</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>David R.R.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">3</xref>
          <xref rid="aff4" ref-type="aff">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Patterson</surname>
            <given-names>Roy D.</given-names>
          </name>
          <xref rid="aff4" ref-type="aff">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ives</surname>
            <given-names>D. Timothy</given-names>
          </name>
          <xref rid="aff4" ref-type="aff">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Griffiths</surname>
            <given-names>Timothy D.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>1</sup>Wellcome Department of Imaging Neuroscience, Institute of Neurology, University College London, London, WC1N 3BG, United Kingdom</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>2</sup>Auditory Group, University of Newcastle upon Tyne Medical School, Newcastle upon Tyne, NE2 4HH, United Kingdom</addr-line>
      </aff>
      <aff id="aff3">
        <addr-line><sup>3</sup>Department of Psychology, University of Hull, Hull, HU6 7RX, United Kingdom</addr-line>
      </aff>
      <aff id="aff4">
        <addr-line><sup>4</sup>Centre for the Neural Basis of Hearing, Department of Physiology, Development and Neuroscience, University of Cambridge, Cambridge, CB2 3EG, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>∗</label>Corresponding author <email>kkriegs@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <day>03</day>
        <month>7</month>
        <year>2007</year>
      </pub-date>
      <volume>17</volume>
      <issue>13-2</issue>
      <fpage>1123</fpage>
      <lpage>1128</lpage>
      <history>
        <date date-type="received">
          <day>26</day>
          <month>3</month>
          <year>2007</year>
        </date>
        <date date-type="rev-recd">
          <day>18</day>
          <month>5</month>
          <year>2007</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>5</month>
          <year>2007</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2007 ELL &amp; Excerpta Medica.</copyright-statement>
        <copyright-year>2007</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Summary</title>
        <p>The size of a resonant source can be estimated by the acoustic-scale information in the sound <xref rid="bib1 bib2 bib3" ref-type="bibr">[1–3]</xref>. Previous studies revealed that posterior superior temporal gyrus (STG) responds to acoustic scale in human speech when it is controlled for spectral-envelope change (unpublished data). Here we investigate whether the STG activity is specific to the processing of acoustic scale in human voice or whether it reflects a generic mechanism for the analysis of acoustic scale in resonant sources. In two functional magnetic resonance imaging (fMRI) experiments, we measured brain activity in response to changes in acoustic scale in different categories of resonant sound (human voice, animal call, and musical instrument). We show that STG is activated bilaterally for spectral-envelope changes in general; it responds to changes in category as well as acoustic scale. Activity in left posterior STG is specific to acoustic scale in human voices and not responsive to acoustic scale in other resonant sources. In contrast, the anterior temporal lobe and intraparietal sulcus are activated by changes in acoustic scale across categories. The results imply that the human voice requires special processing of acoustic scale, whereas the anterior temporal lobe and intraparietal sulcus process auditory size information independent of source category.</p>
      </abstract>
      <kwd-group>
        <kwd>SYSNEURO</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>Results and Discussion</title>
      <p>In the animal world, the analysis of size from the calls of conspecifics is crucial in social communication (deer <xref rid="bib2" ref-type="bibr">[2]</xref>, frogs <xref rid="bib4" ref-type="bibr">[4]</xref>). Monkeys can link the size information in conspecific calls to the actual physical appearance of the caller <xref rid="bib1" ref-type="bibr">[1]</xref>. Humans are also very good at judging size information from auditory cues <xref rid="bib3 bib5 bib6" ref-type="bibr">[3, 5, 6]</xref>. This is not only the case for speech stimuli, but also for musical instruments <xref rid="bib6" ref-type="bibr">[6]</xref>. Furthermore, the perception of acoustic scale does not appear to depend on learned associations based on audio-visual co-occurrence; listeners can extract size information from speech that has been resynthesized to simulate unnatural speaker sizes <xref rid="bib3" ref-type="bibr">[3]</xref>. This suggests that there is a generic mechanism for the analysis of acoustic scale—a mechanism that supports size perception across a range of natural categories and possibly even in novel categories.</p>
      <p>The size of a resonant source affects the sounds it produces; other things being equal, larger resonant sources vibrate more slowly <xref rid="bib7" ref-type="bibr">[7]</xref>. In the case of mammals, birds, and frogs, the vocal folds produce a stream of pulses that are filtered by resonances in the vocal tract. The vocal tract grows with body size, and, as a result, the acoustic scale of the sound increases with the length of the vocal tract <xref rid="bib8 bib9" ref-type="bibr">[8, 9]</xref>. In the case of brass instruments, the lips and mouthpiece produce a stream of pulses that are then filtered by resonances in the tube and bell <xref rid="bib6" ref-type="bibr">[6]</xref>. The effects of the size of the body resonators on the sound are effectively the same for these different sources; the resonant frequencies <italic>increase</italic> in proportion to the <italic>decrease</italic> in the size of the source. The effects of a change in size on a human vowel, a French horn note, and a bullfrog call are illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>, which presents auditory images of the sounds <xref rid="bib6 bib10" ref-type="bibr">[6, 10]</xref>, and in <xref rid="app2" ref-type="sec">Figure S1</xref> (in the <xref rid="app2" ref-type="sec">Supplemental Data</xref> available online), which presents spectrograms. In the auditory images, the shape of the distribution of activity represents the timbre of the source; the distribution moves upward as a unit (on a log-frequency axis) as the size of the source decreases. The spectral profile to the right of each auditory image emphasizes the timbre of the source in the shape of the profile. The acoustic scale of the source is determined by the position of the profile in frequency. The temporal profile at the bottom of each auditory image emphasizes the pitch of the source <xref rid="bib10" ref-type="bibr">[10]</xref>.</p>
      <p>Previous studies revealed that the neural activity associated with the acoustic-scale information in speech sounds is located in posterior superior temporal gyrus (STG) (unpublished data, <xref rid="bib11" ref-type="bibr">[11]</xref>). The activity was independent of the task the subjects were performing, indicating an obligatory processing step for acoustic-scale information (unpublished data). Here we investigated whether this region in posterior STG is also specialized for acoustic-scale information in other communication sounds. Whereas we previously controlled for spectral-envelope change by varying the speech sounds in all conditions, here we contrast two different types of spectral-envelope change: One of these spectral changes (change of category) changes the shape of the spectral distribution; the other (change of acoustic scale) preserves the shape of the spectral distribution and just changes its position on the frequency axis.</p>
      <p>The stimuli for the study were recorded from one speaker (saying /a/), one bullfrog (<italic>Lithobates Catesbeiana</italic>, formerly <italic>Rana catesbeiana</italic>), and one musical instrument (French horn). We analyzed and resynthesized these sounds with a technique that synchronizes the analysis window with the pulse rate of the sound <xref rid="bib12 bib13" ref-type="bibr">[12, 13]</xref>. This allows segregation of the pitch information from the spectral-envelope information that represents the timbre of the source and its size. Once separated, the two forms of information can be manipulated independently and then recombined to simulate variously sized resonant sources with the same pitch (see <xref rid="app2" ref-type="sec">Supplemental Data</xref> for details). Detailed behavioral experiments had been performed with the horn and speech stimuli previously <xref rid="bib3 bib6 bib14" ref-type="bibr">[3, 6, 14]</xref>. We performed a perceptual study with the frog sounds to enable us to match the just-noticeable difference (JND) in size across categories (see <xref rid="app2" ref-type="sec">Supplemental Data</xref> for details). Thus, the three categories we used in the two experiments reported here were human speech, animal call, and musical instrument.</p>
      <p>Neural activity was assessed with functional magnetic resonance imaging (fMRI). The stimuli were concatenated to form blocks of several sounds. The first experiment was performed as a 2 × 2 factorial design with the factors acoustic scale (fixed, varies) and category (fixed, varies). This resulted in four conditions: (1) acoustic scale varies, category varies; (2) acoustic scale varies, category is fixed; (3) acoustic scale is fixed, category varies; and (4) acoustic scale is fixed, category is fixed (see <xref rid="sec2" ref-type="sec">Experimental Procedures</xref> for more details). The first hypothesis was that contrasting conditions in which both types (acoustic scale and category) of spectral envelope vary with those in which both were fixed would reveal areas that are responsive to spectral-envelope change in general. Furthermore, we contrasted the condition in which acoustic scale alone varies with the condition in which category alone varies in order to test the second hypothesis, that there are specific mechanisms—independent of category in posterior STG—for acoustic-scale processing.</p>
      <p>We localized the auditory system functionally by contrasting all experimental conditions against silence (<xref rid="app2" ref-type="sec">Figure S2</xref>). Areas that are responsive to spectral-envelope change in general were sought by contrasting the condition with acoustic-scale and category changes with the condition where both were fixed. This revealed activity that extended from bilateral STG into planum temporale (PT) and down into the superior temporal sulcus (STS) (<xref rid="fig2" ref-type="fig">Figure 2</xref>). To investigate whether acoustic-scale changes in general play a role in the activation of posterior STG, we contrasted the condition where only acoustic scale varies with the condition where only category varies. We did not use the main effect for acoustic scale (<xref rid="app2" ref-type="sec">Figure S3</xref>) to investigate this effect because the perception of size change is relative; if there is a category change within the block, this interferes with the perception of size change. There was no activity in posterior STG (p &gt; 0.05, uncorrected). The contrast revealed activity in left anterior temporal lobe and left intraparietal sulcus (<xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
      <p>Anterior-temporal-lobe activity is expected during the higher-level processing of size information because it lies at a site of convergence between processing mechanisms for human visual and auditory communication signals <xref rid="bib15" ref-type="bibr">[15]</xref>. This does not imply that the anterior temporal area is the only possible site of auditory-visual convergence in size processing. Such convergence (especially during or after bimodal stimulation) probably involves a whole network of interactions from primary to more specialized sensory areas in auditory, as well as visual, cortices <xref rid="bib15 bib16 bib17" ref-type="bibr">[15–17]</xref>.</p>
      <p>Several single-cell studies in animals have shown “numerons” (number-selective neurons) responding to a specific quantity or number independent of its appearance. These are therefore thought to contain an abstract representation of magnitude (for a review, see <xref rid="bib18" ref-type="bibr">[18]</xref>). Size and magnitude are both continuous dimensions, and comparative judgments are possible in both. Human functional-imaging studies have revealed that both dimensions activate the human intraparietal sulcus <xref rid="bib19 bib20" ref-type="bibr">[19, 20]</xref>. These studies investigated abstract representation of size with visual displays including differently sized letters or numbers. We assume that the activity in intraparietal sulcus in response to auditory size in our study is elicited because the abstract representation of size is accessed not only by the visual but also by the auditory modality.</p>
      <p>The fact that size changes in resonant sources do not, in general, activate posterior STG was surprising. In the previous experiments (unpublished data), we did not have conditions where the speech sound was fixed and acoustic scale changed on its own, and vice versa. This might explain the difference because the spectral-envelope changes that code vowel changes were present in all conditions. It could therefore be that the changes in acoustic scale in our previous studies (unpublished data) activate regions implicated in general processing of the spectral envelope. Another possibility is that there is an acoustic-scale-processing region that is specific to the human voice.</p>
      <p>The second experiment was designed to test the hypothesis that there is a specific region specialized for processing acoustic-scale information in the human voice. We used the same stimuli as in the first experiment. The difference was in the design, which was a 2 × 3 factorial, with the factor acoustic scale (varies or fixed) and the factor category (animal, musical instrument, human). The three categories were presented in separate blocks, in which the size of the resonant source either varied or was fixed.</p>
      <p>The main effect of category revealed activity in STG and STS (<xref rid="fig4" ref-type="fig">Figure 4</xref>A). This activity was mostly accounted for by differential activation to frog and human sounds when contrasted with horn sounds (<xref rid="fig4" ref-type="fig">Figure 4</xref>B). The interaction of acoustic scale and category (<xref rid="fig4" ref-type="fig">Figure 4</xref>C, blue) is significant in the left STG. This activity overlaps with activity found in previous experiments (unpublished data) for size changes in speech (<xref rid="fig4" ref-type="fig">Figure 4</xref>C, red). The parameter estimates show that the interaction of acoustic scale and category in the present study is due to the strength of the signal change associated with the size information in the human voice (<xref rid="fig4" ref-type="fig">Figure 4</xref>D). This contrast is controlled for activity in response to spectral-envelope change in general because it contrasts the difference between varying and fixed acoustic scale in the human voice against the difference between varying and fixed acoustic scale in the other categories. Furthermore, the differential activity for size change in vowel sounds cannot be attributed to greater activity for voices in general: The category effect shows that frog sounds activate the STG to a higher extent than vowel sounds (<xref rid="fig4" ref-type="fig">Figure 4</xref>B).</p>
      <p>These results are consistent with the hypothesis that there is a computational mechanism within posterior STG for the analysis of acoustic scale in the human voice. This is not, however, a generic mechanism for the analysis of size in resonant sources. The posterior superior temporal lobe has been implicated as a primary substrate for constructing sound-based representations of speech <xref rid="bib21" ref-type="bibr">[21]</xref>. Differential activation to a vowel sound would be reasonable inasmuch as formant shifts associated with size differences need to be separated from formant shifts associated with linguistic content only in human communication. Normalization for acoustic scale is not required to the same degree for the communication calls of other species. Another possible explanation would be that activity in posterior STG is related to the number of formants that change size, a number that is larger for human speech.</p>
      <p>In summary, our results show that the activity in posterior STG found for acoustic scale in previous studies (unpublished data) reflects processing of acoustic scale specifically for human speech. In contrast, activity in the anterior temporal lobe appears to reflect general processing of acoustic scale. This region constitutes a candidate area for higher-level processing of size information, such as that involved in audio-visual matching of the sound of the source with its physical appearance <xref rid="bib15" ref-type="bibr">[15]</xref>. Furthermore, category-independent activity to acoustic scale in intraparietal sulcus supports the view that intraparietal sulcus is not only accessed by size judgements on visual stimuli <xref rid="bib19 bib20" ref-type="bibr">[19, 20]</xref>, but also entails a general supramodal representation of size.</p>
    </sec>
    <sec sec-type="materials-methods" id="sec2">
      <title>Experimental Procedures</title>
      <sec id="sec2.1">
        <title>Subjects</title>
        <p>Fifteen subjects participated in experiment 1 and 14 in experiment 2. For further details, see the <xref rid="app2" ref-type="sec">Supplemental Data</xref>.</p>
      </sec>
      <sec id="sec2.2">
        <title>Stimuli</title>
        <p>There were three categories of tonal sounds in these experiments: the vowel /a/, a bullfrog croak, and a French-horn note. Each category was constructed from a single prototype sound by manipulation of the fundamental frequency (f0) of the prototype sound and the acoustic scale of the envelope of the sound with the vocoder, STRAIGHT <xref rid="bib12 bib13" ref-type="bibr">[12, 13]</xref>. For further details see <xref rid="app2" ref-type="sec">Supplemental Data</xref>.</p>
      </sec>
      <sec id="sec2.3">
        <title>Experimental Design</title>
        <p>Sets of stimuli were concatenated to form the stimulus block for a trial. For monitoring of alertness, subjects pressed a button at the end of each block.</p>
        <sec id="sec2.3.1">
          <title>Experiment 1</title>
          <p>The design included the four experimental conditions described above and one silent condition. There were three categories (frogs, horns, humans) and three acoustic-scale values (ac2, ac5, ac10). Twelve sounds were concatenated to form a block for one trial. In the conditions where acoustic scale or category was fixed, the scale or category was chosen randomly from the three possible values, and it was fixed during the 12 sounds of the trial. In the conditions where acoustic scale or category (or both) varied, the block still consisted of 12 sounds; however, the scale or category varied (or both varied).</p>
        </sec>
        <sec id="sec2.3.2">
          <title>Experiment 2</title>
          <p>The design included the six experimental conditions and one silent condition. Each of the three stimulus categories (frogs, horns, humans) was represented by a condition with fixed acoustic scale and a condition with varying acoustic scale. There were eleven possible acoustic-scale values in the experiment. The block for a trial consisted of 11 concatenated sounds. In the fixed-acoustic-scale conditions, a scan consisted of one sound repeated 11 times. The acoustic-scale value for a fixed condition was chosen randomly for each scan from the 11 possible values. In the varied-scale conditions, the category was fixed throughout the block of 11 sounds, while the acoustic scale varied randomly from sound to sound, with the restriction that the value must be two or more steps away from the preceding value. This randomization ensured that each stimulus was at least 4 JNDs from its temporal neighbors, and normally more than 4 JNDs.</p>
          <p>The pitch value in both experiments was kept fixed within a trial, and the value was chosen randomly from the 30 possible values.</p>
        </sec>
      </sec>
      <sec id="sec2.4">
        <title>Scanning Procedure</title>
        <p>The stimuli were delivered via a custom electrostatic system at 70 dB sound-pressure level (SPL). Each sound sequence was presented for a period of 9.2 s (experiment 1) or 8.4 s (experiment 2), after which brain activity was estimated by the fMRI blood-oxygen-level-dependent (BOLD) response at 3 T (Siemens Allegra, Erlangen, Germany) by gradient-echo-planar imaging in a sparse acquisition protocol <xref rid="bib22 bib23" ref-type="bibr">[22, 23]</xref> with cardiac gating (time to repeat/time to echo [TR/TE]: 2.73 s + length of stimulus presentation/65 ms; 42 transverse slices; 3 × 3 mm in-plane resolution, ascending axial sequence). Two hundred and fifty-two brain volumes/subject (48 volumes/condition) were acquired for experiment 1 and 240 brain volumes/subject (30 volumes/experimental condition, 60 volumes/silence condition) for experiment 2.</p>
      </sec>
      <sec id="sec2.5">
        <title>Image Analysis</title>
        <p>Imaging data were analyzed with statistical parametric mapping implemented in SPM2/5 software (<ext-link xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" ext-link-type="uri">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>). Scans were realigned, unwarped, spatially normalized <xref rid="bib24" ref-type="bibr">[24]</xref> to Montreal Neurological Institute (MNI) standard stereotactic space <xref rid="bib25" ref-type="bibr">[25]</xref>, and spatially smoothed (8 mm full-width-at-half-maximum).</p>
        <p>Statistical parametric maps were generated by modeling the evoked hemodynamic response as boxcars convolved with a synthetic hemodynamic-response function in the context of the general linear model <xref rid="bib26" ref-type="bibr">[26]</xref>. Population-level inferences concerning BOLD signal changes between conditions of interest were based on a random-effects model. For each contrast, responses were considered significant at p &lt; 0.001, uncorrected, if the localization of activity was in accordance with prior hypothesis.</p>
      </sec>
    </sec>
    <sec id="app1" sec-type="supplementary-material">
      <title>Supplemental Data</title>
      <p>Experimental Procedures and three figures are available at <ext-link xlink:href="http://www.current-biology.com/cgi/content/full/17/13/1123/DC1/" ext-link-type="uri">http://www.current-biology.com/cgi/content/full/17/13/1123/DC1/</ext-link>.</p>
    </sec>
    <sec id="app2" sec-type="supplementary-material">
      <title>Supplemental Data</title>
      <p>
        <supplementary-material content-type="local-data" id="mmc1">
          <caption>
            <title>Document S1. Experimental Procedures and Three Figures</title>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
      </p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>This work was supported by the Volkswagen Stiftung (I/79 783), the Wellcome Trust, and the UK Medical Research Council (G9900362).</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <label>1</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Turesson</surname>
              <given-names>H.K.</given-names>
            </name>
            <name>
              <surname>Maier</surname>
              <given-names>J.X.</given-names>
            </name>
            <name>
              <surname>van Dinther</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>Vocal tract resonances as indexical cues in rhesus monkeys</article-title>
          <source>Curr. Biol.</source>
          <year>2007</year>
          <volume>17</volume>
          <fpage>425</fpage>
          <lpage>430</lpage>
          <pub-id pub-id-type="pmid">17320389</pub-id>
        </citation>
      </ref>
      <ref id="bib2">
        <label>2</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reby</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>McComb</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Cargnelutti</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Darwin</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Clutton-Brock</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Red deer stags use formants as assessment cues during intrasexual agonistic interactions</article-title>
          <source>Proc. Biol. Sci.</source>
          <year>2005</year>
          <volume>272</volume>
          <fpage>941</fpage>
          <lpage>947</lpage>
          <pub-id pub-id-type="pmid">16024350</pub-id>
        </citation>
      </ref>
      <ref id="bib3">
        <label>3</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Irino</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>The processing and perception of size information in speech sounds</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>117</volume>
          <fpage>305</fpage>
          <lpage>318</lpage>
          <pub-id pub-id-type="pmid">15704423</pub-id>
        </citation>
      </ref>
      <ref id="bib4">
        <label>4</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fairchild</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Mate selection and behavioral thermoregulation in Fowler's toads</article-title>
          <source>Science</source>
          <year>1981</year>
          <volume>212</volume>
          <fpage>950</fpage>
          <lpage>951</lpage>
          <pub-id pub-id-type="pmid">17830192</pub-id>
        </citation>
      </ref>
      <ref id="bib5">
        <label>5</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ives</surname>
              <given-names>D.T.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Discrimination of speaker size from syllable phrases</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>3816</fpage>
          <lpage>3822</lpage>
          <pub-id pub-id-type="pmid">16419826</pub-id>
        </citation>
      </ref>
      <ref id="bib6">
        <label>6</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van Dinther</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Perception of acoustic scale and size in musical instrument sounds</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2006</year>
          <volume>120</volume>
          <fpage>2158</fpage>
          <lpage>2176</lpage>
          <pub-id pub-id-type="pmid">17069313</pub-id>
        </citation>
      </ref>
      <ref id="bib7">
        <label>7</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Irino</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Segregating information about the size and shape of the vocal tract using a time-domain auditory model: The stabilised wavelet-Mellin transform</article-title>
          <source>Speech Commun.</source>
          <year>2002</year>
          <volume>36</volume>
          <fpage>181</fpage>
          <lpage>203</lpage>
        </citation>
      </ref>
      <ref id="bib8">
        <label>8</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Giedd</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Morphology and development of the human vocal tract: A study using magnetic resonance imaging</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1999</year>
          <volume>106</volume>
          <fpage>1511</fpage>
          <lpage>1522</lpage>
          <pub-id pub-id-type="pmid">10489707</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <label>9</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
          </person-group>
          <article-title>Vocal tract length and formant frequency dispersion correlate with body size in rhesus macaques</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1997</year>
          <volume>102</volume>
          <fpage>1213</fpage>
          <lpage>1222</lpage>
          <pub-id pub-id-type="pmid">9265764</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <label>10</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Griffiths</surname>
              <given-names>T.D.</given-names>
            </name>
            <name>
              <surname>Buchel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of temporal structure in sound by the human brain</article-title>
          <source>Nat. Neurosci.</source>
          <year>1998</year>
          <volume>1</volume>
          <fpage>422</fpage>
          <lpage>427</lpage>
          <pub-id pub-id-type="pmid">10196534</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <label>11</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Warren</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Ives</surname>
              <given-names>D.T.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Griffiths</surname>
              <given-names>T.D.</given-names>
            </name>
          </person-group>
          <article-title>Processing the acoustic effect of size in speech sounds</article-title>
          <source>Neuroimage</source>
          <year>2006</year>
          <volume>32</volume>
          <fpage>368</fpage>
          <lpage>375</lpage>
          <pub-id pub-id-type="pmid">16644240</pub-id>
        </citation>
      </ref>
      <ref id="bib12">
        <label>12</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Masuda-Kasuse</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>de Cheveigne</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Restructuring speech representations using pitch-adaptive time-frequency smoothing and instantaneous-frequency-based F0 extraction: Possible role of repetitive structure in sounds</article-title>
          <source>Speech Commun.</source>
          <year>1999</year>
          <volume>27</volume>
          <fpage>187</fpage>
          <lpage>207</lpage>
        </citation>
      </ref>
      <ref id="bib13">
        <label>13</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Irino</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Underlying prinicples of a high-quality speech manipulation system STRAIGHT and its application to speech segregation</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Divenyi</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <source>Speech Separation by Humans and Machines</source>
          <year>2004</year>
          <publisher-name>Kluwer Academic</publisher-name>
          <publisher-loc>Massachusetts</publisher-loc>
          <fpage>167</fpage>
          <lpage>180</lpage>
        </citation>
      </ref>
      <ref id="bib14">
        <label>14</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>The interaction of glottal-pulse rate and vocal-tract length in judgements of speaker size, sex and age</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>3177</fpage>
          <lpage>3186</lpage>
          <pub-id pub-id-type="pmid">16334696</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <label>15</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>A.L.</given-names>
            </name>
          </person-group>
          <article-title>Implicit multisensory associations influence voice recognition</article-title>
          <source>PLoS Biol.</source>
          <year>2006</year>
          <volume>4</volume>
          <fpage>e326</fpage>
          <pub-id pub-id-type="pmid">17002519</pub-id>
        </citation>
      </ref>
      <ref id="bib16">
        <label>16</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Maier</surname>
              <given-names>J.X.</given-names>
            </name>
            <name>
              <surname>Hoffman</surname>
              <given-names>K.L.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title>
          <source>J. Neurosci.</source>
          <year>2005</year>
          <volume>25</volume>
          <fpage>5004</fpage>
          <lpage>5012</lpage>
          <pub-id pub-id-type="pmid">15901781</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <label>17</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van Wassenhove</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Grant</surname>
              <given-names>K.W.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Visual speech speeds up the neural processing of auditory speech</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2005</year>
          <volume>102</volume>
          <fpage>1181</fpage>
          <lpage>1186</lpage>
          <pub-id pub-id-type="pmid">15647358</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <label>18</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Walsh</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>A theory of magnitude: Common cortical metrics of time, space and quantity</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2003</year>
          <volume>7</volume>
          <fpage>483</fpage>
          <lpage>488</lpage>
          <pub-id pub-id-type="pmid">14585444</pub-id>
        </citation>
      </ref>
      <ref id="bib19">
        <label>19</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cohen Kadosh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Henik</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rubinsten</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Mohr</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Dori</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>van de Ven</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Zorzi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hendler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Linden</surname>
              <given-names>D.E.</given-names>
            </name>
          </person-group>
          <article-title>Are numbers special? The comparison systems of the human brain investigated by fMRI</article-title>
          <source>Neuropsychologia</source>
          <year>2005</year>
          <volume>43</volume>
          <fpage>1238</fpage>
          <lpage>1248</lpage>
          <pub-id pub-id-type="pmid">15949508</pub-id>
        </citation>
      </ref>
      <ref id="bib20">
        <label>20</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pinel</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Piazza</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Le Bihan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Dehaene</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Distributed and overlapping cerebral representations of number, size, and luminance during comparative judgments</article-title>
          <source>Neuron</source>
          <year>2004</year>
          <volume>41</volume>
          <fpage>983</fpage>
          <lpage>993</lpage>
          <pub-id pub-id-type="pmid">15046729</pub-id>
        </citation>
      </ref>
      <ref id="bib21">
        <label>21</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hickok</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Towards a functional neuroanatomy of speech perception</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2000</year>
          <volume>4</volume>
          <fpage>131</fpage>
          <lpage>138</lpage>
          <pub-id pub-id-type="pmid">10740277</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <label>22</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Haggard</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Akeroyd</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
            <name>
              <surname>Elliott</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Gurney</surname>
              <given-names>E.M.</given-names>
            </name>
            <name>
              <surname>Bowtell</surname>
              <given-names>R.W.</given-names>
            </name>
          </person-group>
          <article-title>“Sparse” temporal sampling in auditory fMRI</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>7</volume>
          <fpage>213</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="pmid">10194620</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <label>23</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Hoge</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Pike</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Event-related fMRI of the auditory cortex</article-title>
          <source>Neuroimage</source>
          <year>1999</year>
          <volume>10</volume>
          <fpage>417</fpage>
          <lpage>429</lpage>
          <pub-id pub-id-type="pmid">10493900</pub-id>
        </citation>
      </ref>
      <ref id="bib24">
        <label>24</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Heather</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
          </person-group>
          <article-title>Spatial registration and normalisation of images</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1995</year>
          <volume>2</volume>
          <fpage>165</fpage>
          <lpage>189</lpage>
        </citation>
      </ref>
      <ref id="bib25">
        <label>25</label>
        <citation citation-type="other">Evans, A.C., Collins, D.L., Mills, S.R., Brown, E.D., Kelly, R.L., and Phinney, R.E. (1993). 3D statistical neuroanatomical models from 305 MRI volumes. Proceedings of IEEE Nuclear Science Symposium and Medical Imaging Conference 3, 1813-1817.</citation>
      </ref>
      <ref id="bib26">
        <label>26</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
          </person-group>
          <article-title>Statistical parametric maps in functional imaging: A general linear approach</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1995</year>
          <volume>2</volume>
          <fpage>189</fpage>
          <lpage>210</lpage>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>Examples of Auditory Stimuli Used in the Experiment</p>
        <p>Sounds emitted by a human (the vowel /a/), a French horn (a sustained note), and a bullfrog (a natural croak) are displayed as auditory images <xref rid="bib6 bib10" ref-type="bibr">[6, 10]</xref> with a spectral profile to the right and a temporal profile at the bottom of each panel. For each category, the left-hand column shows a large source and the right-hand column shows a small source. For each category, the reduction in resonator size shifts the distribution of energy up to higher frequencies in the auditory image, and it decreases the duration of the resonances. The auditory image separates the formant and pitch information graphically. The main formant of each sound is marked by a black arrow on the spectral profile to the right of each auditory image. The first peak in the temporal profile (at the bottom of each auditory image) summarizes the pitch information shown by the vertical ridge in the auditory image above the peak in the profile. The sounds in the figure all have the same pitch (147 Hz). When the pitch increases, the peaks move to the right without altering the position of the activity in the frequency dimension.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Figure 2</label>
      <caption>
        <p>Effect of Changes in Spectral Envelope in Experiment 1</p>
        <p>The group (n = 15) statistical parametric map for the contrast between the condition where acoustic scale and category changes and the condition where both are fixed has been rendered on sections of the group mean, normalized structural-MRI volume (p &lt; 0.001, uncorrected).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Figure 3</label>
      <caption>
        <p>Effect of Acoustic Scale Independent of Category in Experiment 1</p>
        <p>The group (n = 15) statistical parametric map for the contrast between the condition where acoustic scale changes (category fixed) and the condition where category is changing (acoustic scale fixed) has been rendered on sections of the group mean, normalized structural-MRI volume (effects are significant at p &lt; 0.001, uncorrected; visualization threshold is p &lt; 0.003). The plots show the parameter estimates at the maximum of activity in the anterior temporal lobe (MNI coordinate: −46, 0, −42) and the intraparietal sulcus (MNI coordinate: −46, −50, 46) for each condition separately. Note that in the changing-size condition, the main spectral peak ranges over 1.5 kHz, whereas the major peak changes by no more than 1.0 kHz in the changing-category condition. The main peak shifts most in the condition where both size and category are changing; this condition includes a change from a large French horn (main peak around 0.5 kHz) to a small frog (main peak at 3 kHz). The plots show that there is more activation in the condition size changing/category fixed than in the condition where both category and size are changing. Therefore, the effects in anterior temporal lobe and intraparietal sulcus cannot be accounted for by the change of the main peak in the spectrum. Black and gray bars represent the conditions that were contrasted against each other (black = +1, gray = −1). The following abbreviations are used: Var, variable; Fix, fixed; Siz, size (acoustic scale); and Cat, Category. Error bars represent 95% confidence interval of the mean.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Figure 4</label>
      <caption>
        <p>Effects of Category and Acoustic Scale in Experiment 2</p>
        <p>The group (n = 14) statistical parametric maps have been rendered on sections of the group mean, normalized structural-MRI volume.</p>
        <p>(A) Main effect of category.</p>
        <p>(B) Plot of parameter estimates for each category separately, independent of whether acoustic scale changes or not.</p>
        <p>(C) Blue shows interaction of acoustic scale and category; red shows conjunction of the main effects of size change in the two previous experiments (unpublished data).</p>
        <p>(D) Parameter estimates for acoustic scale changing greater than acoustic scale fixed for all source categories separately. All contrasts are significant at p &lt; 0.001, uncorrected; the visualization threshold is p &lt; 0.003. <sup>∗</sup> indicates p &lt; 0.05. <sup>∗∗∗</sup> indicates p &lt; 0.001. Error bars represent 95% confidence interval of the mean.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
  </floats-wrap>
</article>