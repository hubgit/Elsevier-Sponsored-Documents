<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="brief-report">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuron</journal-id>
      <journal-title-group>
        <journal-title>Neuron</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0896-6273</issn>
      <issn pub-type="epub">1097-4199</issn>
      <publisher>
        <publisher-name>Cell Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2852650</article-id>
      <article-id pub-id-type="pmid">20346760</article-id>
      <article-id pub-id-type="publisher-id">NEURON10160</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuron.2010.03.001</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Report</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>The Developmental Origins of Voice Processing in the Human Brain</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Grossmann</surname>
            <given-names>Tobias</given-names>
          </name>
          <email>t.grossmann@bbk.ac.uk</email>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="aff2" ref-type="aff">2</xref>
          <xref rid="cor1" ref-type="corresp">∗</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Oberecker</surname>
            <given-names>Regine</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Koch</surname>
            <given-names>Stefan Paul</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">3</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Friederici</surname>
            <given-names>Angela D.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>1</label>Centre for Brain and Cognitive Development, Birkbeck, University of London, Malet Street, London WC1E 7HX, UK</aff>
      <aff id="aff2"><label>2</label>Department of Neuropsychology, Max Planck Institute for Human Cognitive and Brain Sciences, Stephanstrasse 1a, 04103 Leipzig, Germany</aff>
      <aff id="aff3"><label>3</label>Berlin Neuroimaging Centre, Department of Neurology, Charite Universitätsmedizin, Luisenstrasse 56, 10099 Berlin, Germany</aff>
      <author-notes>
        <corresp id="cor1"><label>∗</label>Corresponding author <email>t.grossmann@bbk.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>25</day>
        <month>3</month>
        <year>2010</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>25</day>
        <month>3</month>
        <year>2010</year>
      </pub-date>
      <volume>65</volume>
      <issue>6</issue>
      <fpage>852</fpage>
      <lpage>858</lpage>
      <history>
        <date date-type="accepted">
          <day>13</day>
          <month>2</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2010 ELL &amp; Excerpta Medica.</copyright-statement>
        <copyright-year>2010</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <title>Summary</title>
        <p>In human adults, voices are processed in specialized brain regions in superior temporal cortices. We examined the development of this cortical organization during infancy by using near-infrared spectroscopy. In experiment 1, 7-month-olds but not 4-month-olds showed increased responses in left and right superior temporal cortex to the human voice when compared to nonvocal sounds, suggesting that voice-sensitive brain systems emerge between 4 and 7 months of age. In experiment 2, 7-month-old infants listened to words spoken with neutral, happy, or angry prosody. Hearing emotional prosody resulted in increased responses in a voice-sensitive region in the right hemisphere. Moreover, a region in right inferior frontal cortex taken to serve evaluative functions in the adult brain showed particular sensitivity to happy prosody. The pattern of findings suggests that temporal regions specialize in processing voices very early in development and that, already in infancy, emotions differentially modulate voice processing in the right hemisphere.</p>
      </abstract>
      <abstract abstract-type="graphical">
        <title>Highlights</title>
        <p>► Temporal cortex specializes in processing human voices during infancy ► Emotion specifically enhances voice processing in the right hemisphere in infants ► Deeper evaluation of happy speech in infants' right inferior frontal cortex</p>
      </abstract>
      <kwd-group>
        <kwd>sysneuro</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>Introduction</title>
      <p>The human voice is clearly one of the most important stimuli in our auditory environment, which not only conveys speech information, but allows us to recognize individuals and their emotional states (<xref rid="bib3" ref-type="bibr">Belin et al., 2004</xref>). In human adults, voices are processed in specialized brain regions located in the upper bank of the superior temporal sulcus (<xref rid="bib2" ref-type="bibr">Belin et al., 2000</xref>). Recently, it has been shown that macaque monkeys have a similar voice-selective region in the superior temporal plane that preferentially responds to conspecific vocalizations, suggesting that recognizing the sound that is the vocalization of a species member is an evolutionarily conserved brain function in primates that is independent of language (<xref rid="bib42 bib43" ref-type="bibr">Petkov et al., 2008, 2009</xref>). These voice-selective areas in auditory cortex, similar to face-selective areas in visual cortex identified in both human adults and monkeys (<xref rid="bib27 bib50" ref-type="bibr">Kanwisher et al., 1997; Tsao et al., 2006</xref>), are thought to bind the processing of crucial socially relevant information to sensory systems.</p>
      <p>In human adults, the voice-sensitive temporal regions not only react to voice-specific information but are moreover sensitive to emotional prosody crucial in social communication (<xref rid="bib20 bib13" ref-type="bibr">Grandjean et al., 2005; Ethofer et al., 2006</xref>). Such a modulation of sensory processing by emotional signals is particularly strong for threat-related emotions, occurs independent of attention, and is thought to be a fundamental neural mechanism both in face- and voice-sensitive brain regions to prioritize the processing of significant stimuli (see <xref rid="bib56" ref-type="bibr">Vuilleumier, 2006</xref>, for a review). Although well described for the adult brain, the developmental origins of the cortical organization underlying voice and emotional prosody processing in the human brain remain unknown. Here we report two experiments with young infants that fill this gap.</p>
      <p>Behavioral work has shown that newborn infants prefer human voices to similar nonsocial auditory stimuli (<xref rid="bib11 bib25" ref-type="bibr">Ecklund-Flores and Turkewitz, 1996; Hutt et al., 1968</xref>) and their mother's voice to the voice of another newborn's mother (<xref rid="bib8" ref-type="bibr">DeCasper and Fifer, 1980</xref>). These postnatal listening preferences are primarily related to infants' sensitivity to prosodic characteristics of speech (<xref rid="bib37 bib39" ref-type="bibr">Mehler et al., 1988; Moon et al., 1993</xref>). The latter finding is relevant insofar as prosodic cues are known to play an essential role in the perception of vocally communicated emotions (<xref rid="bib46" ref-type="bibr">Scherer, 1986</xref>). Indeed, newborns of English- and Spanish-speaking mothers presented with a range of vocal expressions (happy, angry, sad, and neutral) in their native and nonnative language showed an increase in eye-opening responses following the onset of stimuli with happy prosody when compared to the other emotions, but only when they listened to the vocal expression in their native language (<xref rid="bib35" ref-type="bibr">Mastropieri and Turkewitz, 1999</xref>). Despite this very early form of sensitivity to happy prosody in familiar contexts, further behavioral studies show that only from around 5 months of age do infants robustly discriminate between happy, angry, and sad emotional prosody (<xref rid="bib15 bib57" ref-type="bibr">Flom and Bahrick, 2007; Walker-Andrews, 1997</xref>).</p>
      <p>Recent electrophysiological work indicates an early sensitivity to language-specific and emotion-specific prosodic information in the speech signal. The processing of prosodic stress was shown to elicit language-specific event-related brain potentials (ERPs) in 4- to 5-month-old infants (<xref rid="bib18" ref-type="bibr">Friederici et al., 2007</xref>). An ERP study investigating the processing of emotional prosody in 7-month-old infants (<xref rid="bib21" ref-type="bibr">Grossmann et al., 2005</xref>) revealed that infants discriminated between neutral, happy, and angry emotional prosody. As early as 300 ms poststimulus onset, ERPs for angry prosody differed from happy or neutral prosody over frontal and central electrodes, suggesting a greater initial attention to angry voices. Both angry and happy prosody resulted in a greater positive slow wave than neutral prosody at temporal electrodes, pointing toward an enhanced sensory processing of emotionally loaded stimuli. Thus it appears that aspects of the human voice and prosody, be it emotional or intonational, are processed early in life and that the brain reacts quite specifically to these aspects in speech (for reviews of auditory language functions during early infancy, see <xref rid="bib16 bib31" ref-type="bibr">Friederici, 2006; Kuhl, 2004</xref>).</p>
      <p>Although this work has provided important insights, ERP data cannot provide clear information on the exact brain regions that are involved in processing prosody in infancy. Studies investigating the brain substrates of infants' auditory discrimination abilities by measuring their hemodynamic brain responses indicate that, already by the age of 2 months, infants display a left hemispheric advantage for spoken language, whereas music results in bilateral patterns of activation in the planum temporale (<xref rid="bib10" ref-type="bibr">Dehaene-Lambertz et al., 2009</xref>). Furthermore, a right hemispheric advantage for the processing of language prosody in the temporal cortex can be observed by the age of 3 months (<xref rid="bib24" ref-type="bibr">Homae et al., 2006</xref>). These lateralization patterns are quite similar to those seen in adults (for reviews see <xref rid="bib55 bib17 bib29" ref-type="bibr">Vigneau et al., 2006; Friederici and Alter, 2004; Koelsch and Siebel, 2005</xref>). However, despite the similar brain lateralization patterns, 2- to 3-month-old infants do not yet show specificity in their brain responses in temporal cortex. Namely, direct contrasts between speech and music, mother's and stranger's voice (<xref rid="bib10" ref-type="bibr">Dehaene-Lambertz et al., 2009</xref>), and forward and backward speech (<xref rid="bib9" ref-type="bibr">Dehaene-Lambertz et al., 2002</xref>) did not reveal significant differences in 2- to 3-month-olds' temporal cortex responses. This suggests that the specialization of temporal brain regions involved in speech and voice recognition occur after the age of 3 months.</p>
      <p>The present study used near-infrared spectroscopy (NIRS) permitting spatial localization of brain activation by measuring hemodynamic responses to investigate the neurotopography of voice and emotional prosody in young infants (see <xref rid="bib38 bib34" ref-type="bibr">Minagawa-Kawai et al., 2008; Lloyd-Fox et al., 2010</xref> for reviews of this method and its use with infants). Other neuroimaging techniques that are well established in adults are limited in their use with infants because of methodological concerns. For example, positron emission tomography (PET) exposes participants to radioisotopes, and functional magnetic resonance imaging (fMRI) requires the participant to remain very still and exposes them to a noisy environment. Although both PET and fMRI have been used with infants, this work is restricted to the study of sleeping, sedated, or very young infants. NIRS is better suited for infant research because it can accommodate a good degree of movement from the infants, enabling them to sit upright on their parent's lap and behave relatively freely while watching or listening to certain stimuli. In addition, unlike PET and fMRI, NIRS systems are portable. Finally, despite its inferior spatial resolution, NIRS, like fMRI, measures localized patterns of hemodynamic responses, thus allowing for a comparison of infant NIRS data with adult fMRI data (see <xref rid="bib49" ref-type="bibr">Strangman et al., 2002</xref>, for evidence of a strong correlation between the hemodynamic responses measured with fMRI and NIRS).</p>
      <p>We first investigated voice sensitivity in infants, as voices have been shown to be processed in specific temporal brain regions in human adults and nonhuman primates (<xref rid="bib42 bib43" ref-type="bibr">Petkov et al., 2008, 2009</xref>). In experiment 1, we thus presented 4- and 7-month-old infants with vocal and nonvocal sounds, in order to examine when regions in infant temporal cortices become sensitive to the human voice. We decided to study infants of these ages as prior work suggests that speech and specific voices (e.g., mother's voice) do not yet evoke adult-like specialized temporal brain responses in younger infants (<xref rid="bib10" ref-type="bibr">Dehaene-Lambertz et al., 2009</xref>). Second, we assessed whether the voice-sensitive regions as identified in experiment 1 were modulated by emotional prosody (<xref rid="bib20 bib13" ref-type="bibr">Grandjean et al., 2005; Ethofer et al., 2006</xref>). In experiment 2, we therefore presented 7-month-old infants with happy, angry, and neutral prosody while measuring their brain responses.</p>
    </sec>
    <sec id="sec2">
      <title>Results</title>
      <sec id="sec2.1">
        <title>Experiment 1</title>
        <p>Our analysis of 7-month-old infants' brain responses revealed that three channels in posterior temporal cortex, two located in the right hemisphere (channel 17 and 22) and one located in the left hemisphere (channel 3), were sensitive to the human voice (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). These three brain regions showed significant increases in oxygenated hemoglobin (oxyHb) concentration when the vocal condition was compared to the nonvocal condition (left hemisphere: channel 3: F [1, 15] = 4.782, p = 0.045; right hemisphere: channel 17: F [1, 15] = 5.626, p = 0.032 and channel 22: F [1, 15] = 5.797, p = 0.029). Similar increased activation effects were not obtained in our analysis of 4-month-old infants' brain responses (see <xref rid="fig2" ref-type="fig">Figure 2</xref>). Rather, there was one region in the right hemisphere that showed significant increases in oxyHb when the nonvocal condition was compared to the vocal condition (channel 19: F [1, 15] = 5.07, p = 0.04). For the group of 7-month-olds, no brain regions were found in which the oxyHb concentration changes were higher in the nonvocal than in the vocal condition.</p>
        <p>The analysis of deoxygenated hemoglobin (deoxyHb) concentration changes revealed no significant differences between conditions in 4- and 7-month-old infants. The fact that we did not find any significant decreases in deoxyHb that accompanied the increase in oxyHb, as one would expect on the basis of adult work (<xref rid="bib41" ref-type="bibr">Obrig and Villringer, 2003</xref>), is in line with previous infant NIRS work (<xref rid="bib22 bib36 bib40" ref-type="bibr">Grossmann et al., 2008; Meek, 2002; Nakato et al., 2009</xref>). Several infant NIRS studies either failed to find a significant decrease or even observed an increase in deoxyHb concentration. Although a number of factors such as immaturity of the infant brain have been suggested to explain this difference between infants and adults, the exact nature of this difference remains an open question (for a discussion, see <xref rid="bib36 bib40" ref-type="bibr">Meek, 2002; Nakato et al., 2009</xref>).</p>
      </sec>
      <sec id="sec2.2">
        <title>Experiment 2</title>
        <p>Our analysis revealed two channels in the right hemisphere (channels 15 and 17) that were sensitive to emotion in 7-month-old infants (see <xref rid="fig3" ref-type="fig">Figure 3</xref>). These channels showed significant differences in oxyHb concentration when emotion (happy, angry, and neutral prosody) was assessed as a within-subjects factor in repeated-measures ANOVAs (right hemisphere: channel 15: F [2, 34] = 7.245, p = 0.002; channel 17: F [2, 34] = 4.977, p = 0.013). Of these two channels, channel 17 (located in posterior temporal cortex) had been identified as voice sensitive in experiment 1. This channel showed a significant increase in oxyHb when the angry condition was compared to the happy condition (t [1, 17] = 2.165, p = 0.045) and when the angry condition was compared to the neutral condition (t [1, 17] = 2.289, p = 0.035) using a post-hoc paired t test. Furthermore, channel 17 also showed an increase in oxyHb that was marginally significant when the happy condition was compared to the neutral condition (t [1, 17] = 2.052, p = 0.056). Moreover, channel 15, located in the right inferior frontal cortex, showed a significant increase in oxyHb when the happy condition was compared to the angry condition (t [1, 17] = 2.943, p = 0.009) and when the happy condition was compared to the neutral condition (t [1, 17] = 2.765, p = 0.013), whereas the angry condition was not statistically different from the neutral condition (t [1, 17] = 0.102, p = 0.92). As in experiment 1, the analysis of deoxy concentration changes revealed no significant differences between conditions.</p>
      </sec>
    </sec>
    <sec id="sec3">
      <title>Discussion</title>
      <p>The present study investigated the processing of voice specificity and prosody specificity in the infant brain.</p>
      <sec id="sec3.1">
        <title>Voice Processing</title>
        <p>In experiment 1, we found that 7-month-old infants showed significantly increased hemodynamic responses in left and right superior temporal cortex to the human voice when compared to nonvocal sounds. This suggests that voices, as a class of auditory objects with high occurrence and ecological interest, are processed in a fairly specialized brain region by 7 months of age. Strikingly, 4-month-old infants' temporal regions did not show a similar voice-sensitive responding in experiment 1, indicating that voice sensitivity in the posterior temporal cortex emerges between 4 and 7 months of age. The finding that the group of younger infants did not show voice-sensitive responding is in line with earlier fMRI work in which 2- to 3-month-olds failed to show adult-like increased temporal cortex responses when speech was compared to backward speech (<xref rid="bib9" ref-type="bibr">Dehaene-Lambertz et al., 2002</xref>) or music (<xref rid="bib10" ref-type="bibr">Dehaene-Lambertz et al., 2009</xref>). Infants by the age of 4 months rather showed an increased hemodynamic response to nonvocal stimuli in one region in right temporal cortex located more anterior then the region identified as voice sensitive in 7-month-olds. This finding suggests that 4-month-olds' brains are able to discriminate between the two kinds of auditory stimuli but they seem to be using different (immature) brain mechanisms for this discrimination, since only 7-month-olds show adult-like increased responses to the human voice.</p>
        <p>The brain region identified as voice sensitive in 7-month-olds appears to be localized in similar portions of the superior temporal cortex as in adults (see <xref rid="bib2" ref-type="bibr">Belin et al., 2000</xref>, and <xref rid="app2" ref-type="sec">Figure S1</xref> for comparison of localization in adults), indicating developmental continuity in voice processing between 7-month-old infants and adults. In adults, the voice-sensitive regions for stimulus material identical to that used in the present experiment 1 were found in the upper bank of the superior temporal sulcus (<xref rid="bib2" ref-type="bibr">Belin et al., 2000</xref>). However, the spatial precision in localizing cortical responses achieved with NIRS in infants is more coarse than the excellent spatial resolution obtained by fMRI used in previous adult studies (see <xref rid="bib1 bib34" ref-type="bibr">Aslin and Mehler, 2005; Lloyd-Fox et al., 2010</xref> for a discussion of the advantages and limitations of using NIRS with infants). Furthermore, our current measurement technique did not provide us with information of the depth at which the source of this activation is located (see <xref rid="bib4" ref-type="bibr">Blasi et al., 2007</xref>, for NIRS methodology that allows for the measurement of depth-dependent hemodynamic responses in infants). Therefore, we cannot assess whether the voice-sensitive regions identified in 7-month-old infants are located in the sulcus or the gyrus of the superior temporal cortex. Nevertheless, the functionally similar brain responses in superior temporal cortex in infants and adults suggest that the current infant NIRS results and previous fMRI results with adults represent homologous brain processes. Taken together, in conjunction with earlier work with nonhuman primates (<xref rid="bib42" ref-type="bibr">Petkov et al., 2008</xref>), by demonstrating that this brain specialization emerges early during human postnatal development, the results of experiment 1 provide further support for the notion that sensitive responding to the vocalizations of conspecifics is an evolutionarily important brain function in primates.</p>
      </sec>
      <sec id="sec3.2">
        <title>Processing Emotional Prosody</title>
        <p>The brain responses to emotional prosody as obtained in experiment 2 are in line with previous adult studies (<xref rid="bib20 bib13" ref-type="bibr">Grandjean et al., 2005; Ethofer et al., 2006</xref>). Hearing emotional prosody (happy and angry) but not neutral prosody evoked an increased response in a right temporal region in 7-month-old infants that was identified as voice sensitive in experiment 1. This result indicates that the enhancement of sensory processing by emotional signals is a fundamental and early developing neural mechanism engaged to prioritize the processing of significant stimuli (<xref rid="bib56" ref-type="bibr">Vuilleumier, 2006</xref>). It is interesting to note that the brain response in right temporal cortex was larger to angry prosody when compared to happy prosody, indicating that threatening signals have a particularly strong impact on voice processing (see also <xref rid="bib21" ref-type="bibr">Grossmann et al., 2005</xref>). This heightened sensitivity to negative information is in accordance with the notion of a negativity bias, which is proposed to be an evolutionarily driven propensity to attend and react more strongly to negative information (<xref rid="bib6" ref-type="bibr">Cacioppo and Berntson, 1999</xref>) that appears to emerge in the second half of the first year of life (see <xref rid="bib51" ref-type="bibr">Vaish et al., 2008</xref>).</p>
        <p>In experiment 2, hearing happy prosody but not angry or neutral prosody, evoked an increased response in a region in right inferior frontal cortex in 7-month-olds that did not show voice sensitivity in experiment 1. Greater activation to happy voices than angry voices in right inferior frontal cortex has also been observed in adults (<xref rid="bib26" ref-type="bibr">Johnstone et al., 2006</xref>), suggesting developmental continuity in how the human brain processes happy prosody. Current models of prosody processing in adults (<xref rid="bib47 bib59" ref-type="bibr">Schirmer and Kotz, 2006; Wiethoff et al., 2008</xref>) hold that, following the acoustic analysis in temporal cortices, information is passed on to the inferior frontal regions for further and more detailed evaluation. The finding that 7-month-olds engage right inferior frontal cortex when listening to happy prosody might therefore indicate that speech characterized by positive vocal affect undergoes a more explicit evaluation than speech with neutral or angry affect.</p>
        <p>This finding might also relate to a number of behavioral findings suggesting that infants show strong perferences for infant-directed speech (so-called motherese). Motherese compared to adult-directed speech posseses unique acoustic characteristics: it is generally slower and contains exaggerated pitch contours, hyperarticulation of vowels, and (critical for the interpretation of the current findings) positive prosody (<xref rid="bib14 bib32 bib7" ref-type="bibr">Fernald, 1985; Kuhl et al., 1997; Cooper and Aslin, 1990</xref>). It is also interesting to note that motherese with its happy prosody has been found to facilitate learning, specifically language and word learning in the developing infant (<xref rid="bib31 bib33 bib48 bib52" ref-type="bibr">Kuhl, 2004; Liu et al., 2003; Singh et al., 2002; Vallabha et al., 2007</xref>). Therefore, in conjunction with these behavioral findings, the inferior frontal response to happy prosody observed in 7-month-old infants in experiment 2 may constitute the neural basis for a more detailed cognitive evaluation of infant-directed happy speech.</p>
      </sec>
      <sec id="sec3.3">
        <title>Role of the Right Hemisphere</title>
        <p>Even though voice-sensitive responses were observed in both hemispheres in 7-month-olds in experiment 1, the right hemisphere seemed to be more interested in voice compared to other sounds. While only one NIRS channel showed a voice-sensitive response in the left hemisphere, two adjacent voice-sensitive channels were found in the right hemisphere. Moreover, the overall magnitude of the responses to voices in the two channels in the right hemisphere was larger than that in the left hemisphere. The finding that the spatial extent and the magnitude of the voice-sensitive response were larger in the right hemisphere is in line with adult imaging findings suggesting that the voice-sensitive responses are predominant in the right hemisphere (<xref rid="bib2" ref-type="bibr">Belin et al., 2000</xref>). The modulation of infant brain responses by emotion observed in experiment 2 was restricted to the right hemisphere. Similarly, in adult neuroimaging studies, responses in temporal cortex showed strongest effects of emotion in the right hemisphere (<xref rid="bib20 bib13" ref-type="bibr">Grandjean et al., 2005; Ethofer et al., 2006</xref>). In conjunction with some lesion work (<xref rid="bib5" ref-type="bibr">Borod et al., 2002</xref>), this has led to the suggestion that the right hemisphere plays a predominant role in processing emotional prosody (<xref rid="bib60" ref-type="bibr">Wildgruber et al., 2002</xref>). However, in adults, lesion studies have also discussed the contribution of the left hemisphere for the understanding of emotional prosody (<xref rid="bib30 bib44 bib53" ref-type="bibr">Kucharska-Pietura et al., 2003; Ross et al., 1997; Van Lancker and Sidtis, 1992</xref>). But this can be explained by the fact that in these adult lesion studies meaningful speech stimuli were used, and the left hemisphere is thought to be involved in the recognition of emotion conveyed through meaningful speech (<xref rid="bib30" ref-type="bibr">Kucharska-Pietura et al., 2003</xref>). The right hemisphere shows a clear dominance for prosodic information once any lexical information is absent in the acoustic stimuli (for a review, see <xref rid="bib17" ref-type="bibr">Friederici and Alter, 2004</xref>). The current data from 7-month-old infants together with those from adults suggest that voice-sensitive regions in the right hemisphere play an important role in processing emotional prosody.</p>
      </sec>
      <sec id="sec3.4">
        <title>Implications for Neurodevelopmental Disorders</title>
        <p>Finally, these findings might also have important implications for neurodevelopmental disorders such as autism. Adult participants with autism fail to activate voice-sensitive regions in temporal cortex (<xref rid="bib19" ref-type="bibr">Gervais et al., 2004</xref>). Furthermore, older children and adults with autism are impaired in identifying emotion expressed through tone of voice (<xref rid="bib23 bib45 bib54" ref-type="bibr">Hobson et al., 1989; Rutherford et al., 2002; Van Lancker et al., 1989</xref>). Our findings demonstrating that voice-sensitive brain regions are already specialized and modulated by emotional information by the age of 7 months raise the possibility that the critical neurodevelopmental processes underlying impaired voice processing in autism might occur before 7 months. Therefore, in future work the current approach could be used to assess individual differences in infants' responses to voices and emotional prosody and might thus serve as one of potentially multiple markers that can help with an early identification of infants at risk for a neurodevelopmental disorder (for example, see <xref rid="bib12" ref-type="bibr">Elsabbagh and Johnson, 2007</xref>).</p>
      </sec>
    </sec>
    <sec sec-type="methods" id="sec4">
      <title>Experimental Procedures</title>
      <sec id="sec4.1">
        <title>Participants</title>
        <p>The final sample in experiment 1 consisted of 16 7-month-old infants (eight girls) aged between 201 and 217 days (M = 210.2 days) and 16 4-month-old infants (seven girls) aged between 108 and 135 days (M = 123.1 days). The final sample in experiment 2 consisted of 18 7-month-old infants (eight girls) aged between 199 and 216 days (M = 211.8 days). An additional 26 were tested for experiment 1 (4 months: n = 6; 7 months: n = 8) and experiment 2 (7 months: n = 12) but not included in the final sample because they had too many motion artifacts resulting in too few usable trials for analysis (minimum number of five trials per condition) (n = 18) or because of technical failure (n = 2). Note that an attrition rate at this level is within the normal range for an infant NIRS study (<xref rid="bib38 bib34" ref-type="bibr">Minagawa-Kawai et al., 2008; Lloyd-Fox et al., 2010</xref>). All infants were born full-term (37–42 weeks gestation) and with normal birthweight (&gt;2500 g). All parents gave informed consent before the study.</p>
      </sec>
      <sec id="sec4.2">
        <title>Stimuli</title>
        <p>For experiment 1, stimulus material consisted of 40 8 s long trials of vocal and nonvocal sounds (16 bit/22 KHz sampling rate). Vocal trials included speech (words and nonwords) as well as nonspeech vocalizations, and nonvocal trials consisted of sounds from nature, animals, modern human environment (cars, telephone, airplanes), and musical instruments (for more detail, see <xref rid="bib2" ref-type="bibr">Belin et al. [2000]</xref> and <ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk">http://vnl.psy.gla.ac.uk</ext-link>). For experiment 2, the stimulus material consisted of 74 semantically neutral German verbs previously validated and used with adults (<xref rid="bib47" ref-type="bibr">Schirmer and Kotz, 2006</xref>) and with infants (<xref rid="bib21" ref-type="bibr">Grossmann et al., 2005</xref>). A female speaker produced all words with happy, angry, and neutral prosody. Words were taped with a DAT recorder and digitized at a 16 bit/44.1 kHz sampling rate. The three emotions did not differ with respect to their mean intensity (for further acoustic analysis, see <xref rid="bib21" ref-type="bibr">Grossmann et al., 2005</xref>).</p>
      </sec>
      <sec sec-type="methods" id="sec4.3">
        <title>Procedures</title>
        <p>Infants were seated on their parent's lap in a dimly lit and sound-attenuated room. Stimuli were presented via loudspeaker (SPL = 70 dB). In experiment 1, the experimental sessions consisted of 8 s long trials during which various vocal or nonvocal sound stimuli were presented consecutively. Voices and nonvocal sounds were randomly distributed over the session with no more than two trials of the same category occurring in a row. The intertrial interval was 12 s. In experiment 2, the experimental session consisted of 5 s long trials during which five words of one emotion category (happy, angry, or neutral) were presented consecutively. Trials from the different emotional categories were randomly distributed over the session with no more than two trials of the same category occurring consecutively. The intertrial interval was 15 s. During the presentation of the acoustic stimuli, a cartoon was presented to the infants on a computer screen placed at a 60 cm distance in order to keep their attention and reduce motion artifacts. The experimental session lasted on average 7 min, 20 s (average number of trials = 22).</p>
        <p>Data acquisition and analysis. In both experiments, cortical activation was measured using a Hitachi ETG-4000 NIRS system. The multichannel system uses two wavelengths at 695 nm and 830 nm. Two custom-built arrays consisting of nine optodes (five sources, four detectors) in a 12 channel (source-detector pairs) arrangement with an interoptode separation of 20 mm were placed over temporal and inferior frontal brain regions on each hemisphere (see <xref rid="fig1 fig2 fig3" ref-type="fig">Figures 1–3</xref>) using an Easycap (Falk Minow). The NIRS method relies on the optical determination of changes in oxygenated (oxyHb) and deoxygenated (deoxyHb) hemoglobin concentrations in cerebral cortex, which result from increased regional cerebral blood flow (<xref rid="bib41" ref-type="bibr">Obrig and Villringer, 2003</xref>). NIRS data were continuously sampled at 10 Hz. For analysis, after calculation of the hemoglobin concentration changes, pulse-related signal changes and overall trends were eliminated by low-pass filtering (Butterworth, 5<sup>th</sup> order, lower cutoff 0.5 Hz). Movement artifacts were corrected by an established procedure (see <xref rid="bib28 bib58" ref-type="bibr">Koch et al., 2006; Wartenburger et al., 2007</xref>), which allows marking of artifacts and then padding the contaminated data segments by linear interpolation. Cortical activations were assessed statistically by comparing average concentration changes (oxyHb and deoxyHb) within trials (20 s after stimulus onset) between the experimental conditions by using repeated-measures ANOVAs.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aslin</surname>
              <given-names>R.N.</given-names>
            </name>
            <name>
              <surname>Mehler</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Near-infrared spectroscopy for functional studies of brain activity in human infants: promise, prospects, and challenges</article-title>
          <source>J. Biomed. Opt.</source>
          <volume>10</volume>
          <year>2005</year>
          <fpage>11009</fpage>
          <pub-id pub-id-type="pmid">15847575</pub-id>
        </element-citation>
      </ref>
      <ref id="bib2">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Lafaille</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ahad</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pike</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Voice-selective areas in human auditory cortex</article-title>
          <source>Nature</source>
          <volume>403</volume>
          <year>2000</year>
          <fpage>309</fpage>
          <lpage>312</lpage>
          <pub-id pub-id-type="pmid">10659849</pub-id>
        </element-citation>
      </ref>
      <ref id="bib3">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Fecteau</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bédard</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Thinking the voice: neural correlates of voice perception</article-title>
          <source>Trends Cogn. Sci.</source>
          <volume>8</volume>
          <year>2004</year>
          <fpage>129</fpage>
          <lpage>135</lpage>
          <pub-id pub-id-type="pmid">15301753</pub-id>
        </element-citation>
      </ref>
      <ref id="bib4">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Blasi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Fox</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Everdell</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Volein</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tucker</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Csibra</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Gibson</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Hebden</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Elwell</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <article-title>Investigation of depth dependent changes in cerebral haemodynamics during face perception in infants</article-title>
          <source>Phys. Med. Biol.</source>
          <volume>52</volume>
          <year>2007</year>
          <fpage>6849</fpage>
          <lpage>6864</lpage>
          <pub-id pub-id-type="pmid">18029979</pub-id>
        </element-citation>
      </ref>
      <ref id="bib5">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Borod</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Bloom</surname>
              <given-names>R.L.</given-names>
            </name>
            <name>
              <surname>Brickman</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Nakhutina</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Curko</surname>
              <given-names>E.A.</given-names>
            </name>
          </person-group>
          <article-title>Emotional processing deficits in individuals with unilateral brain damage</article-title>
          <source>Appl. Neuropsychol.</source>
          <volume>9</volume>
          <year>2002</year>
          <fpage>23</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="pmid">12173747</pub-id>
        </element-citation>
      </ref>
      <ref id="bib6">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cacioppo</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Berntson</surname>
              <given-names>G.G.</given-names>
            </name>
          </person-group>
          <article-title>The affect system: architecture and operating characteristics</article-title>
          <source>Curr. Dir. Psychol. Sci.</source>
          <volume>8</volume>
          <year>1999</year>
          <fpage>133</fpage>
          <lpage>137</lpage>
        </element-citation>
      </ref>
      <ref id="bib7">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cooper</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Aslin</surname>
              <given-names>R.N.</given-names>
            </name>
          </person-group>
          <article-title>Preference for infant-directed speech in the first month after birth</article-title>
          <source>Child Dev.</source>
          <volume>61</volume>
          <year>1990</year>
          <fpage>1584</fpage>
          <lpage>1595</lpage>
          <pub-id pub-id-type="pmid">2245748</pub-id>
        </element-citation>
      </ref>
      <ref id="bib8">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>DeCasper</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Fifer</surname>
              <given-names>W.P.</given-names>
            </name>
          </person-group>
          <article-title>Of human bonding: Newborns prefer their mothers' voices</article-title>
          <source>Science</source>
          <volume>280</volume>
          <year>1980</year>
          <fpage>1174</fpage>
          <lpage>1176</lpage>
          <pub-id pub-id-type="pmid">7375928</pub-id>
        </element-citation>
      </ref>
      <ref id="bib9">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dehaene-Lambertz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Dehaene</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hertz-Pannier</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Functional neuroimaging of speech perception in infants</article-title>
          <source>Science</source>
          <volume>208</volume>
          <year>2002</year>
          <fpage>2013</fpage>
          <lpage>2015</lpage>
          <pub-id pub-id-type="pmid">12471265</pub-id>
        </element-citation>
      </ref>
      <ref id="bib10">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dehaene-Lambertz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Montavont</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jobert</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Allirol</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Dubois</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hertz-Pannier</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Dehaene</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Language or music, mother or Mozart? Structural and environmental influences on infants' language networks</article-title>
          <source>Brain Lang.</source>
          <year>2009</year>
          <comment>in press. Published online October 27, 2009</comment>
        </element-citation>
      </ref>
      <ref id="bib11">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ecklund-Flores</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Turkewitz</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Asymmetric headturning to speech and nonspeech in human newborns</article-title>
          <source>Dev. Psychobiol.</source>
          <volume>29</volume>
          <year>1996</year>
          <fpage>205</fpage>
          <lpage>217</lpage>
          <pub-id pub-id-type="pmid">8666129</pub-id>
        </element-citation>
      </ref>
      <ref id="bib12">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Elsabbagh</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>M.H.</given-names>
            </name>
          </person-group>
          <article-title>Infancy and autism: progress, prospects, and challenges</article-title>
          <source>Prog. Brain Res.</source>
          <volume>164</volume>
          <year>2007</year>
          <fpage>355</fpage>
          <lpage>383</lpage>
          <pub-id pub-id-type="pmid">17920442</pub-id>
        </element-citation>
      </ref>
      <ref id="bib13">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ethofer</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Anders</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wiethoff</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Erb</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Herbert</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Saur</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Grodd</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Effects of prosodic emotional intensity on activation of associative auditory cortex</article-title>
          <source>Neuroreport</source>
          <volume>17</volume>
          <year>2006</year>
          <fpage>249</fpage>
          <lpage>253</lpage>
          <pub-id pub-id-type="pmid">16462592</pub-id>
        </element-citation>
      </ref>
      <ref id="bib14">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fernald</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Four-month-olds prefer to listen to motherese</article-title>
          <source>Infant Behav. Dev.</source>
          <volume>8</volume>
          <year>1985</year>
          <fpage>181</fpage>
          <lpage>195</lpage>
        </element-citation>
      </ref>
      <ref id="bib15">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Flom</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bahrick</surname>
              <given-names>L.E.</given-names>
            </name>
          </person-group>
          <article-title>The development of infant discrimination of affect in multimodal and unimodal stimulation: The role of intersensory redundancy</article-title>
          <source>Dev. Psychol.</source>
          <volume>43</volume>
          <year>2007</year>
          <fpage>238</fpage>
          <lpage>252</lpage>
          <pub-id pub-id-type="pmid">17201522</pub-id>
        </element-citation>
      </ref>
      <ref id="bib16">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
          </person-group>
          <article-title>The neural basis of language development and its impairment</article-title>
          <source>Neuron</source>
          <volume>52</volume>
          <year>2006</year>
          <fpage>941</fpage>
          <lpage>952</lpage>
          <pub-id pub-id-type="pmid">17178399</pub-id>
        </element-citation>
      </ref>
      <ref id="bib17">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Alter</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Lateralization of auditory language functions: a dynamic dual pathway model</article-title>
          <source>Brain Lang.</source>
          <volume>89</volume>
          <year>2004</year>
          <fpage>267</fpage>
          <lpage>276</lpage>
          <pub-id pub-id-type="pmid">15068909</pub-id>
        </element-citation>
      </ref>
      <ref id="bib18">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Friedrich</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Christophe</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Brain responses in 4-month-old infants are already language specific</article-title>
          <source>Curr. Biol.</source>
          <volume>17</volume>
          <year>2007</year>
          <fpage>1208</fpage>
          <lpage>1211</lpage>
          <pub-id pub-id-type="pmid">17583508</pub-id>
        </element-citation>
      </ref>
      <ref id="bib19">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gervais</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Boddaert</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Leboyer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Coez</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sfaello</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Barthélémy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Brunelle</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Samson</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zilbovicius</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Abnormal cortical voice processing in autism</article-title>
          <source>Nat. Neurosci.</source>
          <volume>7</volume>
          <year>2004</year>
          <fpage>801</fpage>
          <lpage>802</lpage>
          <pub-id pub-id-type="pmid">15258587</pub-id>
        </element-citation>
      </ref>
      <ref id="bib20">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grandjean</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sander</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Pourtois</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Seghier</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Scherer</surname>
              <given-names>K.R.</given-names>
            </name>
            <name>
              <surname>Vuilleumier</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>The voices of wrath: brain responses to angry prosody in meaningless speech</article-title>
          <source>Nat. Neurosci.</source>
          <volume>8</volume>
          <year>2005</year>
          <fpage>145</fpage>
          <lpage>146</lpage>
          <pub-id pub-id-type="pmid">15665880</pub-id>
        </element-citation>
      </ref>
      <ref id="bib21">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grossmann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Striano</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
          </person-group>
          <article-title>Infants' electric brain responses to emotional prosody</article-title>
          <source>Neuroreport</source>
          <volume>16</volume>
          <year>2005</year>
          <fpage>1825</fpage>
          <lpage>1828</lpage>
          <pub-id pub-id-type="pmid">16237335</pub-id>
        </element-citation>
      </ref>
      <ref id="bib22">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grossmann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Lloyd-Fox</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Blasi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Deligianni</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Elwell</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Csibra</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Early cortical specialization for face-to-face communication in human infants</article-title>
          <source>Proc. R. Soc. Lond. B. Biol. Sci.</source>
          <volume>275</volume>
          <year>2008</year>
          <fpage>2803</fpage>
          <lpage>2811</lpage>
        </element-citation>
      </ref>
      <ref id="bib23">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hobson</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Ouston</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Naming emotion in faces and voices: abilities and disabilities in autism and mental retardation</article-title>
          <source>Br. J. Dev. Psychol.</source>
          <volume>7</volume>
          <year>1989</year>
          <fpage>237</fpage>
          <lpage>250</lpage>
        </element-citation>
      </ref>
      <ref id="bib24">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Homae</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Watanabe</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Nakano</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Asakawa</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Taga</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>The right hemisphere of sleeping infant perceives sentential prosody</article-title>
          <source>Neurosci. Res.</source>
          <volume>54</volume>
          <year>2006</year>
          <fpage>276</fpage>
          <lpage>280</lpage>
          <pub-id pub-id-type="pmid">16427714</pub-id>
        </element-citation>
      </ref>
      <ref id="bib25">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hutt</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Hutt</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Leonard</surname>
              <given-names>H.G.</given-names>
            </name>
            <name>
              <surname>von Bermuth</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Muntjewerff</surname>
              <given-names>W.F.</given-names>
            </name>
          </person-group>
          <article-title>Auditory responsivity in the human neonate</article-title>
          <source>Nature</source>
          <volume>218</volume>
          <year>1968</year>
          <fpage>888</fpage>
          <lpage>890</lpage>
        </element-citation>
      </ref>
      <ref id="bib26">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Johnstone</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>van Reekum</surname>
              <given-names>C.M.</given-names>
            </name>
            <name>
              <surname>Oakes</surname>
              <given-names>T.R.</given-names>
            </name>
            <name>
              <surname>Davidson</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>The voice of emotion: an FMRI study of neural responses to angry and happy vocal expressions</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <volume>1</volume>
          <year>2006</year>
          <fpage>242</fpage>
          <lpage>249</lpage>
          <pub-id pub-id-type="pmid">17607327</pub-id>
        </element-citation>
      </ref>
      <ref id="bib27">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>McDermott</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chun</surname>
              <given-names>M.M.</given-names>
            </name>
          </person-group>
          <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>
          <source>J. Neurosci.</source>
          <volume>17</volume>
          <year>1997</year>
          <fpage>4302</fpage>
          <lpage>4311</lpage>
          <pub-id pub-id-type="pmid">9151747</pub-id>
        </element-citation>
      </ref>
      <ref id="bib28">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Koch</surname>
              <given-names>S.P.</given-names>
            </name>
            <name>
              <surname>Steinbrink</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Villringer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Obrig</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Synchronization between background activity and visually evoked potential is not mirrored by focal hyperoxygenation: implications for the interpretation of vascular brain imaging</article-title>
          <source>J. Neurosci.</source>
          <volume>26</volume>
          <year>2006</year>
          <fpage>4940</fpage>
          <lpage>4948</lpage>
          <pub-id pub-id-type="pmid">16672669</pub-id>
        </element-citation>
      </ref>
      <ref id="bib29">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Koelsch</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Siebel</surname>
              <given-names>W.A.</given-names>
            </name>
          </person-group>
          <article-title>Towards a neural basis of music perception</article-title>
          <source>Trends Cogn. Sci.</source>
          <volume>9</volume>
          <year>2005</year>
          <fpage>578</fpage>
          <lpage>584</lpage>
          <pub-id pub-id-type="pmid">16271503</pub-id>
        </element-citation>
      </ref>
      <ref id="bib30">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kucharska-Pietura</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Phillips</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Gernand</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
          </person-group>
          <article-title>Perception of emotions from faces and voices following unilateral brain damage</article-title>
          <source>Neuropsychologia</source>
          <volume>41</volume>
          <year>2003</year>
          <fpage>963</fpage>
          <lpage>970</lpage>
        </element-citation>
      </ref>
      <ref id="bib31">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kuhl</surname>
              <given-names>P.K.</given-names>
            </name>
          </person-group>
          <article-title>Early language acquisition: cracking the speech code</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <volume>5</volume>
          <year>2004</year>
          <fpage>831</fpage>
          <lpage>843</lpage>
          <pub-id pub-id-type="pmid">15496861</pub-id>
        </element-citation>
      </ref>
      <ref id="bib32">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kuhl</surname>
              <given-names>P.K.</given-names>
            </name>
            <name>
              <surname>Andruski</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Chistovich</surname>
              <given-names>I.A.</given-names>
            </name>
            <name>
              <surname>Chistovich</surname>
              <given-names>L.A.</given-names>
            </name>
            <name>
              <surname>Kozhevnikova</surname>
              <given-names>E.V.</given-names>
            </name>
            <name>
              <surname>Ryskina</surname>
              <given-names>V.L.</given-names>
            </name>
            <name>
              <surname>Stolyarova</surname>
              <given-names>E.I.</given-names>
            </name>
            <name>
              <surname>Sundberg</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Lacerda</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Cross-language analysis of phonetic units in language addressed to infants</article-title>
          <source>Science</source>
          <volume>277</volume>
          <year>1997</year>
          <fpage>684</fpage>
          <lpage>686</lpage>
          <pub-id pub-id-type="pmid">9235890</pub-id>
        </element-citation>
      </ref>
      <ref id="bib33">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>H.M.</given-names>
            </name>
            <name>
              <surname>Kuhl</surname>
              <given-names>P.K.</given-names>
            </name>
            <name>
              <surname>Tsao</surname>
              <given-names>F.M.</given-names>
            </name>
          </person-group>
          <article-title>An association between mothers' speech clarity and infants' speech discrimination skills</article-title>
          <source>Dev. Sci.</source>
          <volume>6</volume>
          <year>2003</year>
          <fpage>1</fpage>
          <lpage>10</lpage>
        </element-citation>
      </ref>
      <ref id="bib34">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lloyd-Fox</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Blasi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Elwell</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <article-title>Illuminating the developing brain: The past, present and future of functional near-infrared spectroscopy</article-title>
          <source>Neurosci. Biobehav. Rev.</source>
          <volume>34</volume>
          <year>2010</year>
          <fpage>269</fpage>
          <lpage>284</lpage>
          <pub-id pub-id-type="pmid">19632270</pub-id>
        </element-citation>
      </ref>
      <ref id="bib35">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mastropieri</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Turkewitz</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Prenatal experience and neonatal responsiveness to vocal expressions of emotion</article-title>
          <source>Dev. Psychobiol.</source>
          <volume>35</volume>
          <year>1999</year>
          <fpage>204</fpage>
          <lpage>214</lpage>
          <pub-id pub-id-type="pmid">10531533</pub-id>
        </element-citation>
      </ref>
      <ref id="bib36">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meek</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Basic principles of optical imaging and application to the study of infant development</article-title>
          <source>Dev. Sci.</source>
          <volume>5</volume>
          <year>2002</year>
          <fpage>371</fpage>
          <lpage>380</lpage>
        </element-citation>
      </ref>
      <ref id="bib37">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mehler</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jusczyk</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Lambertz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Halsted</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Bertoncini</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Amiel-Tison</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>A precursor of language acquisition in young infants</article-title>
          <source>Cognition</source>
          <volume>29</volume>
          <year>1988</year>
          <fpage>143</fpage>
          <lpage>178</lpage>
          <pub-id pub-id-type="pmid">3168420</pub-id>
        </element-citation>
      </ref>
      <ref id="bib38">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Minagawa-Kawai</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Mori</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hebden</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Dupoux</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Optical imaging of infants' neurocognitive development: recent advances and perspectives</article-title>
          <source>Dev. Neurobiol.</source>
          <volume>68</volume>
          <year>2008</year>
          <fpage>712</fpage>
          <lpage>728</lpage>
          <pub-id pub-id-type="pmid">18383545</pub-id>
        </element-citation>
      </ref>
      <ref id="bib39">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moon</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Cooper</surname>
              <given-names>R.P.</given-names>
            </name>
            <name>
              <surname>Fifer</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Two-day-olds prefer their native language</article-title>
          <source>Infant Behav. Dev.</source>
          <volume>16</volume>
          <year>1993</year>
          <fpage>495</fpage>
          <lpage>500</lpage>
        </element-citation>
      </ref>
      <ref id="bib40">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nakato</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Otsuka</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kanazawa</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yamaguchi</surname>
              <given-names>M.K.</given-names>
            </name>
            <name>
              <surname>Watanabe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kakigi</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>When do infants differentiate profile face from frontal face? A near-infrared spectroscopic study</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>30</volume>
          <year>2009</year>
          <fpage>462</fpage>
          <lpage>472</lpage>
          <pub-id pub-id-type="pmid">18095284</pub-id>
        </element-citation>
      </ref>
      <ref id="bib41">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Obrig</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Villringer</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Beyond the visible—imaging the human brain with light</article-title>
          <source>J. Cereb. Blood Flow Metab.</source>
          <volume>23</volume>
          <year>2003</year>
          <fpage>1</fpage>
          <lpage>18</lpage>
          <pub-id pub-id-type="pmid">12500086</pub-id>
        </element-citation>
      </ref>
      <ref id="bib42">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Petkov</surname>
              <given-names>C.I.</given-names>
            </name>
            <name>
              <surname>Kayser</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Steudel</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Whittingstall</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Augath</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>A voice region in the monkey brain</article-title>
          <source>Nat. Neurosci.</source>
          <volume>11</volume>
          <year>2008</year>
          <fpage>367</fpage>
          <lpage>374</lpage>
          <pub-id pub-id-type="pmid">18264095</pub-id>
        </element-citation>
      </ref>
      <ref id="bib43">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Petkov</surname>
              <given-names>C.I.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
            <name>
              <surname>Obleser</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Where are the human speech and voice regions, and do other animals have anything like them?</article-title>
          <source>Neuroscientist</source>
          <volume>15</volume>
          <year>2009</year>
          <fpage>419</fpage>
          <lpage>429</lpage>
          <pub-id pub-id-type="pmid">19516047</pub-id>
        </element-citation>
      </ref>
      <ref id="bib44">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ross</surname>
              <given-names>E.D.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Yenkosky</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Lateralization of affective prosody in brain and the callosal integration of hemispheric language functions</article-title>
          <source>Brain Lang.</source>
          <volume>56</volume>
          <year>1997</year>
          <fpage>27</fpage>
          <lpage>54</lpage>
          <pub-id pub-id-type="pmid">8994697</pub-id>
        </element-citation>
      </ref>
      <ref id="bib45">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rutherford</surname>
              <given-names>M.D.</given-names>
            </name>
            <name>
              <surname>Baron-Cohen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wheelwright</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Reading the mind in the voice: a study with normal adults and adults with Asperger syndrome and high functioning autism</article-title>
          <source>J. Autism Dev. Disord.</source>
          <volume>32</volume>
          <year>2002</year>
          <fpage>189</fpage>
          <lpage>194</lpage>
          <pub-id pub-id-type="pmid">12108620</pub-id>
        </element-citation>
      </ref>
      <ref id="bib46">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scherer</surname>
              <given-names>K.R.</given-names>
            </name>
          </person-group>
          <article-title>Vocal affect expression: a review and a model for future research</article-title>
          <source>Psychol. Bull.</source>
          <volume>99</volume>
          <year>1986</year>
          <fpage>143</fpage>
          <lpage>165</lpage>
          <pub-id pub-id-type="pmid">3515381</pub-id>
        </element-citation>
      </ref>
      <ref id="bib47">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schirmer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kotz</surname>
              <given-names>S.A.</given-names>
            </name>
          </person-group>
          <article-title>Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing</article-title>
          <source>Trends Cogn. Sci.</source>
          <volume>10</volume>
          <year>2006</year>
          <fpage>24</fpage>
          <lpage>30</lpage>
          <pub-id pub-id-type="pmid">16321562</pub-id>
        </element-citation>
      </ref>
      <ref id="bib48">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Morgan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Best</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Infants' listening preferences: baby talk or happy talk</article-title>
          <source>Infancy</source>
          <volume>3</volume>
          <year>2002</year>
          <fpage>365</fpage>
          <lpage>394</lpage>
        </element-citation>
      </ref>
      <ref id="bib49">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Strangman</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Culver</surname>
              <given-names>J.P.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>J.H.</given-names>
            </name>
            <name>
              <surname>Boas</surname>
              <given-names>D.A.</given-names>
            </name>
          </person-group>
          <article-title>A quantitative comparison of simultaneous BOLD fMRI and NIRS recordings during functional brain activation</article-title>
          <source>Neuroimage</source>
          <volume>17</volume>
          <year>2002</year>
          <fpage>719</fpage>
          <lpage>731</lpage>
          <pub-id pub-id-type="pmid">12377147</pub-id>
        </element-citation>
      </ref>
      <ref id="bib50">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tsao</surname>
              <given-names>D.Y.</given-names>
            </name>
            <name>
              <surname>Freiwald</surname>
              <given-names>W.A.</given-names>
            </name>
            <name>
              <surname>Tootell</surname>
              <given-names>R.B.H.</given-names>
            </name>
            <name>
              <surname>Livingstone</surname>
              <given-names>M.S.</given-names>
            </name>
          </person-group>
          <article-title>A cortical region consisting entirely of face-selective cells</article-title>
          <source>Science</source>
          <volume>311</volume>
          <year>2006</year>
          <fpage>670</fpage>
          <lpage>674</lpage>
          <pub-id pub-id-type="pmid">16456083</pub-id>
        </element-citation>
      </ref>
      <ref id="bib51">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vaish</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Grossmann</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Woodward</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Not all emotions are created equal: the negativity bias in social-emotional development</article-title>
          <source>Psychol. Bull.</source>
          <volume>134</volume>
          <year>2008</year>
          <fpage>383</fpage>
          <lpage>403</lpage>
          <pub-id pub-id-type="pmid">18444702</pub-id>
        </element-citation>
      </ref>
      <ref id="bib52">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vallabha</surname>
              <given-names>G.K.</given-names>
            </name>
            <name>
              <surname>McClelland</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Pons</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Werker</surname>
              <given-names>J.F.</given-names>
            </name>
            <name>
              <surname>Amano</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Unsupervised learning of vowel categories from infant-directed speech</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <volume>104</volume>
          <year>2007</year>
          <fpage>13273</fpage>
          <lpage>13278</lpage>
          <pub-id pub-id-type="pmid">17664424</pub-id>
        </element-citation>
      </ref>
      <ref id="bib53">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sidtis</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>The identification of affective-prosodic stimuli by left- and right-hemisphere-damaged subjects: all errors are not created equal</article-title>
          <source>J. Speech Hear. Res.</source>
          <volume>35</volume>
          <year>1992</year>
          <fpage>963</fpage>
          <lpage>970</lpage>
          <pub-id pub-id-type="pmid">1447930</pub-id>
        </element-citation>
      </ref>
      <ref id="bib54">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Cornelius</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Recognition of emotional-prosodic meanings in speech by autistic, schizophrenic, and normal children</article-title>
          <source>Dev. Neuropsychol.</source>
          <volume>5</volume>
          <year>1989</year>
          <fpage>207</fpage>
          <lpage>226</lpage>
        </element-citation>
      </ref>
      <ref id="bib55">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vigneau</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Beaucousin</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Hervé</surname>
              <given-names>P.Y.</given-names>
            </name>
            <name>
              <surname>Duffau</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Crivello</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Houdé</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Mazoyer</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Tzourio-Mazoyer</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing</article-title>
          <source>Neuroimage</source>
          <volume>30</volume>
          <year>2006</year>
          <fpage>1414</fpage>
          <lpage>1432</lpage>
          <pub-id pub-id-type="pmid">16413796</pub-id>
        </element-citation>
      </ref>
      <ref id="bib56">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vuilleumier</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>How brains beware: neural mechanisms of emotional attention</article-title>
          <source>Trends Cogn. Sci.</source>
          <volume>9</volume>
          <year>2006</year>
          <fpage>585</fpage>
          <lpage>594</lpage>
          <pub-id pub-id-type="pmid">16289871</pub-id>
        </element-citation>
      </ref>
      <ref id="bib57">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Walker-Andrews</surname>
              <given-names>A.S.</given-names>
            </name>
          </person-group>
          <article-title>Infants' perception of expressive behaviors: Differentiation of multimodal information</article-title>
          <source>Psychol. Bull.</source>
          <volume>121</volume>
          <year>1997</year>
          <fpage>1</fpage>
          <lpage>20</lpage>
        </element-citation>
      </ref>
      <ref id="bib58">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wartenburger</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Steinbrink</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Telkemeyer</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Friedrich</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Obrig</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>The processing of prosody: Evidence of interhemispheric specialization at the age of four</article-title>
          <source>Neuroimage</source>
          <volume>34</volume>
          <year>2007</year>
          <fpage>416</fpage>
          <lpage>425</lpage>
          <pub-id pub-id-type="pmid">17056277</pub-id>
        </element-citation>
      </ref>
      <ref id="bib59">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wiethoff</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kreifelts</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Herbert</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Grodd</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ethofer</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Cerebral processing of emotional prosody—influence of acoustic parameters and arousal</article-title>
          <source>Neuroimage</source>
          <volume>39</volume>
          <year>2008</year>
          <fpage>885</fpage>
          <lpage>893</lpage>
          <pub-id pub-id-type="pmid">17964813</pub-id>
        </element-citation>
      </ref>
      <ref id="bib60">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wildgruber</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Pihan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ackermann</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Erb</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Grodd</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic brain activation during processing of emotional intonation: Influence of acoustic parameters, emotional valence, and sex</article-title>
          <source>Neuroimage</source>
          <volume>4</volume>
          <year>2002</year>
          <fpage>856</fpage>
          <lpage>859</lpage>
          <pub-id pub-id-type="pmid">11906226</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="app2" sec-type="supplementary-material">
      <title>Supplemental Information</title>
      <p>
        <supplementary-material content-type="local-data" id="mmc1">
          <caption>
            <title>Document S1. Supplemental Figure</title>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
      </p>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>T.G. was supported by a Sir Henry Wellcome Postdoctoral Fellowship awarded by the Wellcome Trust (082659/Z/07/Z).</p>
    </ack>
  </back>
  <floats-group>
    <fig id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>Voice-Sensitive Brain Regions Identified in 7-Month-Old Infants in Experiment 1</p>
        <p>This graph depicts mean oxygenated hemoglobin concentration changes (±SEM) for vocal and other sounds measured from 24 NIRS channels. Channels that showed significant increases for vocal compared to other sounds are marked in red on the head model.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Figure 2</label>
      <caption>
        <p>Brain Responses in 4-Month-Old Infants in Experiment 1</p>
        <p>This graph depicts mean oxygenated hemoglobin concentration changes (±SEM) for vocal and other sounds measured from 24 NIRS channels. The channel that showed a significant increase for other sounds compared to vocal sounds is marked in blue on the head model.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Figure 3</label>
      <caption>
        <p>Brain Regions Modulated by Emotional Prosody in Experiment 2</p>
        <p>This graph depicts mean oxygenated hemoglobin concentration changes (±SEM) for happy, angry, and neutral prosody measured from 24 NIRS channels. The channel that showed an increased sensitivity to angry prosody is marked in magenta, and the channel that showed increased sensitivity to happy prosody is marked in blue on the head model.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
  </floats-group>
</article>