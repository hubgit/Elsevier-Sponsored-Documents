<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="announcement">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Curr Biol</journal-id>
      <journal-title>Current Biology </journal-title>
      <issn pub-type="ppub">0960-9822</issn>
      <issn pub-type="epub">1879-0445</issn>
      <publisher>
        <publisher-name>Cell Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2361420</article-id>
      <article-id pub-id-type="pmid">17320389</article-id>
      <article-id pub-id-type="publisher-id">CURBIO5385</article-id>
      <article-id pub-id-type="doi">10.1016/j.cub.2007.01.029</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Report</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Vocal-Tract Resonances as Indexical Cues in Rhesus Monkeys</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Ghazanfar</surname>
            <given-names>Asif A.</given-names>
          </name>
          <email>asifg@princeton.edu</email>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="fn1" ref-type="fn">3</xref>
          <xref rid="cor1" ref-type="corresp">∗</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Turesson</surname>
            <given-names>Hjalmar K.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="fn1" ref-type="fn">3</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Maier</surname>
            <given-names>Joost X.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>van Dinther</surname>
            <given-names>Ralph</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Patterson</surname>
            <given-names>Roy D.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Logothetis</surname>
            <given-names>Nikos K.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>1</sup>Max Planck Institute for Biological Cybernetics, 72076 Tuebingen, Germany</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>2</sup>Centre for the Neural Basis of Hearing, Department of Physiology, University of Cambridge, CB2 3EG Cambridge, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>∗</label>Corresponding author <email>asifg@princeton.edu</email></corresp>
        <fn id="fn1">
          <label>3</label>
          <p>Present address: Program in Neuroscience, Department of Psychology, Green Hall, Princeton University, Princeton New Jersey, 08540.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="ppub">
        <day>06</day>
        <month>3</month>
        <year>2007</year>
      </pub-date>
      <volume>17</volume>
      <issue>5-2</issue>
      <fpage>425</fpage>
      <lpage>430</lpage>
      <history>
        <date date-type="received">
          <day>6</day>
          <month>11</month>
          <year>2006</year>
        </date>
        <date date-type="rev-recd">
          <day>7</day>
          <month>1</month>
          <year>2007</year>
        </date>
        <date date-type="accepted">
          <day>8</day>
          <month>1</month>
          <year>2007</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2007 ELL &amp; Excerpta Medica.</copyright-statement>
        <copyright-year>2007</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Summary</title>
        <p>Vocal-tract resonances (or <italic>formants</italic>) are acoustic signatures in the voice and are related to the shape and length of the vocal tract. Formants play an important role in human communication, helping us not only to distinguish several different speech sounds <xref rid="bib1" ref-type="bibr">[1]</xref>, but also to extract important information related to the physical characteristics of the speaker, so-called <italic>indexical cues</italic>. How did formants come to play such an important role in human vocal communication? One hypothesis suggests that the ancestral role of formant perception—a role that might be present in extant nonhuman primates—was to provide indexical cues <xref rid="bib2 bib3 bib4 bib5" ref-type="bibr">[2–5]</xref>. Although formants are present in the acoustic structure of vowel-like calls of monkeys <xref rid="bib3 bib4 bib5 bib6 bib7 bib8" ref-type="bibr">[3–8]</xref> and implicated in the discrimination of call types <xref rid="bib8 bib9 bib10" ref-type="bibr">[8–10]</xref>, it is not known whether they use this feature to extract indexical cues. Here, we investigate whether rhesus monkeys can use the formant structure in their “coo” calls to assess the age-related body size of conspecifics. Using a preferential-looking paradigm <xref rid="bib11 bib12" ref-type="bibr">[11, 12]</xref> and synthetic coo calls in which formant structure simulated an adult/large- or juvenile/small-sounding individual, we demonstrate that untrained monkeys attend to formant cues and link large-sounding coos to large faces and small-sounding coos to small faces—in essence, they can, like humans <xref rid="bib13" ref-type="bibr">[13]</xref>, use formants as indicators of age-related body size.</p>
      </abstract>
      <kwd-group>
        <kwd>SYSNEURO</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>Results</title>
      <p>Though the whole acoustic spectrum of vowel sounds is ideal for our categorization of speech <xref rid="bib14" ref-type="bibr">[14]</xref>, the lowest-dimensional representations rely on vocal-tract resonances, or <italic>formants</italic> <xref rid="bib15" ref-type="bibr">[15]</xref>. Formants are not only important phonetic elements of speech—allowing us to distinguish different vowel sounds—but also carry important information related to the physical characteristics of the particular speaker. In humans, both statistical pattern recognition <xref rid="bib16 bib17" ref-type="bibr">[16, 17]</xref> and psychophysics <xref rid="bib13 bib18 bib19 bib20 bib21 bib22 bib23" ref-type="bibr">[13, 18–23]</xref> have suggested that formants are significant contributors to these indexical cues. It is likely, then, that detecting formants could have provided ancestral primates with indexical cues necessary for navigating the complex social interactions that are the essence of primate societies. One important indexical cue is body size. Formant cues related to body size could be used by monkeys to determine the sex (in sexually dimorphic species), degree of potential threat (e.g., whether a competitor is larger or smaller), and/or age of an individual, as such cues do for human listeners <xref rid="bib13 bib18 bib20 bib21" ref-type="bibr">[13, 18, 20, 21]</xref>.</p>
      <p>Formants are the result of acoustic filtering by the supralaryngeal vocal tract—the nasal and oral cavities above the vocal folds. During vocal production, pulses of air generated by the rapid movement of the vocal folds produce an acoustic signal. The frequency of these pulses—the glottal-pulse rate—determines the fundamental frequency of the signal, which in turn is perceived as pitch. As the signal passes through the supralaryngeal vocal tract, it excites resonances, resulting in the enhancement of particular frequency bands; these are the formants. The length of the vocal tract determines, in part, which frequency bands are enhanced <xref rid="bib2 bib15" ref-type="bibr">[2, 15]</xref>: The frequency of, and the spacing between, successive formants decreases with <italic>increasing</italic> vocal-tract length. Because the vocal-tract length scales with body size in humans <xref rid="bib24" ref-type="bibr">[24]</xref>, formants are often reliable cues to this physical feature <xref rid="bib13 bib18 bib20 bib21" ref-type="bibr">[13, 18, 20, 21]</xref>.</p>
      <p>Acoustic analyses of rhesus monkey vocalizations reveal that these calls also have prominent formant structure <xref rid="bib3 bib5 bib25" ref-type="bibr">[3, 5, 25]</xref> and that this spectral structure could, in theory, provide monkeys with indexical cues about their conspecifics, including information about their body size <xref rid="bib3" ref-type="bibr">[3]</xref>. Here, we explicitly test the hypothesis that rhesus monkeys use formants as salient acoustic cues to assess the age-related body-size differences of conspecifics. A direct, experimental approach for assessing the role of formants includes the use of vocal synthesis methods in which the formant frequencies of a call can be manipulated independently of other acoustic cues (e.g., the fundamental frequency [glottal-pulse rate]) <xref rid="bib26" ref-type="bibr">[26]</xref>. Only recently have such synthetic vocalizations been used successfully in animal playback experiments (whooping cranes <xref rid="bib27" ref-type="bibr">[27]</xref>, red deer <xref rid="bib28" ref-type="bibr">[28]</xref>, and rhesus monkeys <xref rid="bib29" ref-type="bibr">[29]</xref>). Along similar lines, we used naturally produced rhesus monkey “coo” calls as models and a speech vocoder <xref rid="bib30" ref-type="bibr">[30]</xref> to synthesize versions of these calls in which the glottal-pulse rate and all other acoustic variables (e.g., duration and amplitude envelope) were held constant while the formant frequencies were shifted up or down. <xref rid="fig1" ref-type="fig">Figure 1</xref>A shows the spectrograms of a single coo synthesized with two different vocal-tract lengths (10 cm and 5.5 cm). Note how the formants shift down and become more concentrated for large vocal-tract lengths and shift up and spread out for short vocal-tract lengths, whereas the overall shape of the amplitude envelope remains unchanged. The shift in formant spacing is also evident in the power and linear prediction spectra for the two vocal-tract lengths (<xref rid="fig1" ref-type="fig">Figure 1</xref>B).</p>
      <p>To determine whether rhesus monkeys use formant cues to assess age-related differences in conspecific body sizes, we adopted a preferential-looking paradigm. Previous work has established that, like human infants (e.g., <xref rid="bib31 bib32" ref-type="bibr">[31, 32]</xref>), rhesus monkeys naturally prefer to look at a visual stimulus that corresponds to the auditory stimulus that they hear <xref rid="bib11 bib12" ref-type="bibr">[11, 12]</xref>. In the present context, we tested whether our monkey subjects would preferentially attend to a video display showing a large, older monkey (sexually mature, 13-yr-old) versus a small, younger monkey (juvenile, 6-yr-old) producing a coo vocalization (<xref rid="fig1" ref-type="fig">Figure 1</xref>C) when they heard a coo produced from a simulated long vocal tract, and vice versa (<xref rid="fig1" ref-type="fig">Figures 1</xref>A and 1B). Monkeys were seated in front of two LCD monitors and a hidden speaker located between them and at the same height. One monitor displayed a video of the face of the large monkey producing a coo call, and the other monitor displayed the face of the small monkey producing a coo call. We counter-balanced all pertinent variables in the experiment. Both videos were played synchronously in a continuous loop for 60 s. Videos were edited such that the onset and offset of each monkey's mouth movements was synchronous. Synchronously with the videos, the subjects heard a coo that was from a long vocal tract (10 cm) or a short vocal tract (5.5 cm) and was based on a call from a third individual (<xref rid="fig1" ref-type="fig">Figures 1</xref>A and 1B). The call of this third individual was based on one of two coo calls from other individuals of different ages (a sexually mature, 11-yr-old adult and a juvenile 6-yr-old) to eliminate any chance that the subjects could match the call with the dynamic faces by using some other individual-specific articulatory cue or some age-related acoustic cue(s) independent of formants.</p>
      <p>Although only the heads were visible in these videos, subjects could putatively assess overall size by features of the face (their size or the relative positions of facial features) or by comparing the head size relative to parts of the chair in which the vocalizing monkeys were seated (<xref rid="fig1" ref-type="fig">Figure 1</xref>C). Head size can be used as a proxy for overall body size because there is a strong correlation between skull size and body size (as measured by either weight or length) and thus with vocal-tract length and formant spacing <xref rid="bib3" ref-type="bibr">[3]</xref>. Because all visual and auditory components were synchronized and identical in both duration and overall amplitude, amodal cues could not be used to make a match. Two sets of such audiovisual stimuli were generated and used in these experiments; that is, there were two coo calls that were from two differently aged and sized individuals and were manipulated to sound large and small and then paired to the videos. Thus, our paradigm addressed whether monkeys would preferentially attend to the dynamic face that was approximately matched in size to the coo call that simulated that body size.</p>
      <p>Monkeys looked at the matching screen for 58.4% of the total time they spent looking at either screen (match: 13.08 ± 1.45 s; nonmatch: 10.26 ± 1.49 s); this proportion differed significantly from chance [one-sample t test, t(23) = 2.67, p = 0.014] (<xref rid="fig2" ref-type="fig">Figure 2</xref>A). This ∼3 s difference, although seemingly small, is robust in the context of the preferential-looking method and is similar to differences reported for similar experiments in both humans <xref rid="bib31 bib32" ref-type="bibr">[31, 32]</xref> and monkeys <xref rid="bib11 bib12" ref-type="bibr">[11, 12]</xref>. With the percentage of total looking time to the match screen used as a dependent variable, an ANOVA was conducted to explore any possible interactions among four primary variables (side of screen [left versus right], vocalizer [acoustic signal of monkey 1 versus monkey 2], face [visual signal of monkey 1 versus monkey 2], and vocal tract length [long versus short]). All main effects or interactions were nonsignificant. Thus, there were no response biases toward the left or right screen, the stimulus exemplars (the calls or the faces used), or the size of the monkey on the matching screen (e.g., monkeys did not look longer overall when the matching screen showed a large monkey). Nineteen out of twenty-four monkeys in the present experiment preferentially attended to the dynamic face that best matched the body size simulated by the coo vocalization played through the speaker (<xref rid="fig2" ref-type="fig">Figure 2</xref>B, sign test, p = 0.003). These results demonstrate that rhesus monkeys can, without any training whatsoever, use formant structure to assess the age-related body size of conspecific individuals.</p>
    </sec>
    <sec id="sec2">
      <title>Discussion</title>
      <p>Previous behavioral studies demonstrated that trained baboons <xref rid="bib33" ref-type="bibr">[33]</xref> and macaques <xref rid="bib34 bib35 bib36" ref-type="bibr">[34–36]</xref> can discriminate different human vowel sounds presumably on the basis of formant-frequency differences. Recently, Fitch and Fritz <xref rid="bib29" ref-type="bibr">[29]</xref> have significantly extended these findings by showing that rhesus monkeys can, without training, discriminate differences in the formant structure of their own conspecific calls. However, a demonstration that particular sorts of features appear in species-typical vocalizations or that animals can attend to such features is (though of great importance) not equivalent to showing them to be functionally significant to the animals in question. The functional significance of formants in monkey vocalizations was first suggested by the study of Owren <xref rid="bib9 bib10" ref-type="bibr">[9, 10]</xref>, who showed that trained vervet monkeys could use formants to distinguish between their alarm calls (akin to the way in which humans may discriminate speech sounds). The results of our experiments suggest that rhesus monkeys can not only spontaneously discriminate changes in formant structure within a call type (à la <xref rid="bib29" ref-type="bibr">[29]</xref>), but can also use these differences in formant structure as indexical cues—to assess the age-related size of a conspecific individual. Although body size is just one indexical cue among many that may be encoded in the formant frequencies of monkeys, our data show that, as in humans <xref rid="bib13 bib18 bib20 bib21" ref-type="bibr">[13, 18, 20, 21]</xref>, acoustic cues that are the product of vocal-tract length can be used to estimate body size. These data are the first direct evidence for the hypothesis that formants embedded in the acoustic structure of nonhuman primate calls provide cues to the physical characteristics of the vocalizer <xref rid="bib3 bib4 bib5 bib6 bib7" ref-type="bibr">[3–7]</xref>.</p>
      <p>Rhesus monkeys and humans are not alone in this regard. One other nonhuman species perceives a link between formant structure and body size: red deer, <italic>Cervus elaphus</italic>. Recent studies of red deer males during their mating season show that not only do red deer roars contain formant structures that are indicators of a male's body size and fitness <xref rid="bib37" ref-type="bibr">[37]</xref>, but male red deer are also more attentive and, in some cases, will reply with more roars when they hear synthetic male roars with lower formant frequencies (simulating a large stag) <xref rid="bib28" ref-type="bibr">[28]</xref>. Indeed, red deer are able to “exaggerate” their apparent size by actively lowering their larynx during vocal production, thereby creating a longer vocal tract (and thus lower formant frequencies) <xref rid="bib38" ref-type="bibr">[38]</xref>. Nonhuman primates are not known to be able to actively lower their larynx in this manner during vocal production. Taken together, the fact that rhesus monkeys and red deer can both use formant cues to assess body size begs the question: Is their common perceptual ability the result of convergent evolution (i.e., they evolved independently) or common ancestry (i.e., all or most mammals share this capacity)?</p>
      <p>If all mammals were endowed with the capacity to assess body-size cues (age-related or otherwise) via formant frequencies, then it would suggest that even in mammals whose own vocalizations lack formant structure, formant discrimination would still be evident. For example, in small mammals (including small primates, such as New World marmosets or squirrel monkeys) that have short vocal tracts and high frequency calls, formant structure is simply not present in their vocalizations (see <xref rid="bib29" ref-type="bibr">[29]</xref> for details regarding why this is so) and thus formant perception in these animals would exist without purpose, perhaps as the nonadaptive by-product of other auditory mechanisms. The alternative evolutionary scenario would suggest that the link between formant perception and indexical cuing arose in parallel, possibly multiple times during the course of mammalian evolution. Indeed, the divergent vocal <italic>production</italic> apparatuses between primates and red deer suggest that the evolution of vocal communication among mammals did not take a linear, unbranching path. Naturally, a direct test of either of these hypotheses would entail exploring formant perception in untrained animals that lack formant structure in their own vocalizations.</p>
      <p>Regardless of the evolutionary origins of acoustic body-size perception via formants, the link between rhesus monkey perception and human perception is likely to be direct because they are closely related species. However, in human speech perception, indexical cues are coupled with phonetic cues. Humans are able to identify vowel sounds across a wide range of speaker body sizes and ages (and thus different formant-frequency positions), though it is not a feature we consistently attend to. Nevertheless, recent human psychophysical studies revealed that humans, when asked, can make accurate judgments of a speaker's body size by using the formant structure embedded in speech sounds <xref rid="bib13 bib21" ref-type="bibr">[13, 21]</xref> and can recognize vowel sounds even when the simulated vocal-tract length is extended beyond the species-typical range <xref rid="bib21" ref-type="bibr">[21]</xref>. Thus, assessing speaker size through formants may be an automatic, unconscious process that the human auditory system does in everyday speech communication. Even more pertinent to the current findings with rhesus monkeys, humans can use formant frequencies to determine the age category of speakers (juvenile versus adult) <xref rid="bib13" ref-type="bibr">[13]</xref>, and when fundamental frequency is put into conflict with formant information, human listeners rely on the formants to make age judgments <xref rid="bib13" ref-type="bibr">[13]</xref>.</p>
      <p>A question that remains open is whether monkeys and/or humans <italic>within</italic> the category of adults can use formant cues to assess body size. Theoretically, such an assessment could be useful in male-male competition or mate attraction (as in the red deer, described above). Behavioral and acoustic evidence for either scenario, however, remains somewhat ambiguous. For example, in humans, acoustic measurements reveal a relationship only between adult-male height and formant spacing <xref rid="bib17 bib22" ref-type="bibr">[17, 22]</xref>, whereas others find a significant correlation only between female height and formant spacing <xref rid="bib23 bib39" ref-type="bibr">[23, 39]</xref>. At the behavioral level, these cues may not be sufficient for assessing speaker size <xref rid="bib22" ref-type="bibr">[22]</xref>. The reasons for these apparent inconsistencies across studies are multifarious and possibly include differences in body-size variables measured and speech tokens used, and/or large variation in vocal-tract morphology. Similarly, acoustic measures of formant spacing in the grunt calls of adult-female baboons reveal that it is not reliably correlated with many different measures of body size <xref rid="bib6" ref-type="bibr">[6]</xref>, and no behavioral tests of adult body-size perception via formants in monkeys have been forthcoming. Thus, although formant spacing may be a reliable perceptual cue to body size <italic>across</italic> age classes (as in the present study), this may not be true within an age class.</p>
      <p>Given that neither the vocal apparatuses nor brains of human ancestors fossilize, the comparative method is the only way to investigate the evolution of primate communication <xref rid="bib40 bib41" ref-type="bibr">[40, 41]</xref>. By comparing the vocal behavior of extant primates with human communication, one can deduce the behavioral and neural capacities of extinct common ancestors, allowing the identification of homologies and providing clues as to the adaptive functions of such behaviors. The close relationship between Old World macaques and humans allows for putative homologous brain mechanisms related to formant perception to be explored and compared between these species. Our data show that rhesus monkeys can intermodally match the auditory size embedded in their coo calls with the appropriately sized visual image of a vocalizing monkey's face; this ability is independent of the identity of the seen and heard monkey. Thus, monkeys are extracting a size cue from auditory structure alone and subsequently matching it to an appropriately sized visual signal. Could auditory cortex integrate such “high-level” bimodal signals <xref rid="bib42" ref-type="bibr">[42]</xref>, perhaps on the basis of implicit multisensory associations formed during everyday social interactions <xref rid="bib43" ref-type="bibr">[43]</xref>? A first step would be to demonstrate that particular regions of auditory cortex are sensitive to formant structure relative to other acoustic parameters. A recent human neuroimaging paper revealed that regions adjacent to, but not within, Heschl's gyrus are sensitive to formant differences related to speaker size <xref rid="bib44" ref-type="bibr">[44]</xref>, and we have preliminary neurophysiological data that some cortical sites in the lateral belt (putatively a homologous area) of monkeys are also sensitive to vocal-tract-length-related changes in formant spacing relative to changes in fundamental frequencies (C.F. Chandrasekaran, R.V.D., R.D.P., N.K.L., and A.A.G., unpublished data). It is not known whether neurons in these areas integrate auditory and visual size information; if so, it would be strong evidence that these neurons encode ethologically relevant size information.</p>
      <p>It is a long trajectory from body-size perception to speech perception via formant cues. There are many aspects of vocal production that are unique to humans and allow us to produce a broader range of sounds with greater complexity <xref rid="bib15" ref-type="bibr">[15]</xref>. Our data suggest that the use of formant cues in the perception of vowel sounds by humans in a linguistic context emerged gradually, perhaps for other functional reasons, over the course of human evolution. Perception of indexical cues, such as age-related body size, via formants in vocalizations may be one functional link between the vocalizations of human and nonhuman primates.</p>
    </sec>
    <sec sec-type="materials-methods" id="sec3">
      <title>Experimental Procedures</title>
      <sec id="sec3.1">
        <title>Subjects</title>
        <p>We tested male rhesus macaques (n = 24; age range 4–14 yr) from a large colony housed at the Max Planck Institute for Biological Cybernetics. Animals are socially housed and provided with enrichment objects (toys, hammocks, ropes, etc.). All experimental procedures were in accordance with the local authorities (Regierungspraesidium) and the European Community (EUVD 86/609/EEC) standards for the care and use of laboratory animals. For the purposes of the current experiments, subjects were free-fed food and water.</p>
      </sec>
      <sec id="sec3.2">
        <title>Stimuli</title>
        <p>The stimuli were digital-video recordings of seated rhesus monkeys spontaneously producing coo vocalizations in a sound attenuated room (<xref rid="fig1" ref-type="fig">Figures 1</xref>A and 1B). The stimulus set was based on 3-yr-old digital videos of now-deceased male monkeys from the Max Planck Institute for Biological Cybernetics. These videos were then acquired onto a computer and manipulated as needed in Adobe Premiere 6.0 (<ext-link xlink:href="http://www.adobe.com" ext-link-type="uri">www.adobe.com</ext-link>). We extracted the audio track from the digital-video samples. Calls were acquired at 32 kHz and then upsampled to 44.1 kHz to allow playback on our hardware.</p>
        <p>To generate synthetic rhesus monkey coo calls, we used computational algorithms previously used in similar studies with human speech sounds <xref rid="bib13 bib21" ref-type="bibr">[13, 21]</xref>. The stimuli used in the present experiments were based on natural rhesus monkey coo calls that had been scaled with STRAIGHT, a speech-processing routine that dissects and analyzes an utterance with glottal-cycle resolution. STRAIGHT produces a pitch-independent spectral envelope that represents the vocal-tract information independent from the source (the glottal pulse or vocal-fold vibrations) <xref rid="bib30" ref-type="bibr">[30]</xref>. Once STRAIGHT has segregated a coo call into source (the glottal-pulse rate component) and vocal-tract information (the spectral envelope), the coo can then be resynthesized with the spectral envelope contracted or expanded (simulating increases or decreases in vocal-tract length, respectively) or the source information expanded or contracted. The two operations are largely independent. Thus, coo calls produced by a small monkey can be transformed to sound like those of a large monkey, and vice versa, by manipulating the apparent size of the vocal tract while keeping the source constant.</p>
        <p>For the experimental paradigms described below, we used two different coo-call exemplars from two differently aged monkeys—a 6-yr-old juvenile monkey, weighing 5.8 kg, and a sexually mature 11-yr-old adult monkey, weighing 10.0 kg. These were our base stimuli. This was done to control for any cues that may be related to body size beyond the resonance frequencies of different vocal-tract lengths. For both calls, we then normalized the glottal-pulse rate to 420 Hz. This was done to control for any acoustic cues to body size that may be related to vocal-fold thickness and glottal-pulse rate. For each of the two vocalizations, we then manipulated its spectral envelope to create two synthetic versions for each call. One version simulated a large monkey with a vocal-tract length of 10 cm, and the other simulated a small monkey with a vocal-tract length of 5.5 cm. These vocal-tract lengths are within the species-typical range for rhesus monkeys <xref rid="bib3" ref-type="bibr">[3]</xref>. All vocal stimuli were calibrated to the same average root-mean square (RMS) power with Adobe Audition.</p>
      </sec>
      <sec id="sec3.3">
        <title>Preferential-Looking Paradigm</title>
        <p>Two videos were edited, one of which showed a large monkey (13-yr-old, 9.0 kg) producing a coo vocalization and the other showed a small monkey (6-yr-old, 5.9 kg) producing the same call. One of the synthetic coo vocalizations (see above) simulating either a short or long vocal-tract length was then used to replace the original sound track. The videos were edited in Adobe Premiere such that the onset and offset of mouth movements occurred at exactly the same time. Thus, from spatiotemporal point of view, both monkeys appeared to be producing the same coo call. Two sets of such videos were made for each of the two coo calls used.</p>
        <p>The “big monkey” versus “small monkey” visual stimuli were played simultaneously on side-by-side 15 inch LCD monitors (Acer FP559, <ext-link xlink:href="http://www.global.acer.com" ext-link-type="uri">www.global.acer.com</ext-link>). Audio tracks were synchronized with both videos and played through a hidden speaker (same as above) placed directly between and slightly behind the monitors. The RadLight 3.03 Special Edition software video player (<ext-link xlink:href="http://www.radlight.net" ext-link-type="uri">www.radlight.net</ext-link>) was used to play the videos in synchrony. Sounds were presented at an intensity of 72–75 dB (A-weighted) sound-pressure level (SPL) as measured with a Brüel &amp; Kjær 2238 Mediator sound-level meter (<ext-link xlink:href="http://www.bksv.com" ext-link-type="uri">www.bksv.com</ext-link>) at a distance of 72 cm. For testing, a subject was brought to the testing room and placed in front of the two monitors at a distance of 72 cm. The monitors were 65 cm apart (center-to-center distance) and at eye level with the subject. All trials were videotaped by a digital-video camera placed above and between the monitors. All equipment was concealed by a thick black curtain except for the monitor screens and the lens of the camera. The experimenter monitored subject activity from outside of the room. During this time, the subject's attention was directed to the center by the flashing of a 1.2W light placed centrally between the two monitors. A test session began when the subject looked centrally. A trial consisted of the two videos and one of the auditory stimuli played in a continuous loop for 60 s. The left-right position of the two videos was counter balanced. Each subject was only tested once, and all trials were recorded on digital video. We used a between-groups design because, as in all studies that examine the spontaneous behavior of animals and prelinguistic human infants, the subjects often quickly habituate to the testing environment. No reward or training was provided.</p>
        <p>We collected high-quality, close-up digital videos of the subjects' behavior with a JVC GR-DVL805 digital camera (<ext-link xlink:href="http://www.jvc.com" ext-link-type="uri">www.jvc.com</ext-link>). Videos were acquired at 30 frames/s (frame size: 720 × 480 pixels) onto a PC by using an IEEE 1394a input and Adobe Premiere 6.0 software (<ext-link xlink:href="http://www.adobe.com" ext-link-type="uri">www.adobe.com</ext-link>). Clips for analysis were edited down to 60 s, starting with the onset of the auditory track. The total duration of a subject's looking toward each video (left or right) was recorded and expressed as the proportion of total time spent looking at either screen. Scoring which of the screens the monkey subjects were looking toward was unambiguous. The screens are far apart in the horizontal dimension, fairly close to the monkey's face, and at eye level. Thus, the monkey has to make large eye and head movements to look to one screen or the other, and it is similarly clear when he is not looking at either screen. To validate this, we had all the videos scored by a second observer blind to the experimental condition in order to determine interobserver reliability, which was 0.938 (p &lt; 0.0001) as measured by a Pearson <italic>r</italic> test. The statistical tests and plotted data are derived from the blind observer's video scores.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>We thank Jonathan Leong and Chris Darwin for earlier efforts in the formulation and design of these experiments, Chand Chandrasekaran for help with <xref rid="fig1" ref-type="fig">Figure 1</xref>, and Tecumseh Fitch, Marc Hauser, Cory Miller, and Laurie Santos for their excellent comments and discussion. This work was supported by the Max Planck Society (A.A.G., H.K.T., J.X.M., and N.K.L.) and the UK Medical Research Council (G9901257; G9900369) and the German Volkswagen Foundation (VWF 1/79 783) (R.V.D. and R.D.P.).</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <label>1</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Lieberman</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Blumstein</surname>
              <given-names>S.E.</given-names>
            </name>
          </person-group>
          <article-title>Speech Physiology, Speech Perception, and Acoustic Phonetics</article-title>
          <year>1988</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
        </citation>
      </ref>
      <ref id="bib2">
        <label>2</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Hauser</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>Vocal production in nonhuman-primates: Acoustics, physiology, and functional constraints on honest advertisement</article-title>
          <source>Am. J. Primatol.</source>
          <year>1995</year>
          <volume>37</volume>
          <fpage>191</fpage>
          <lpage>219</lpage>
        </citation>
      </ref>
      <ref id="bib3">
        <label>3</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
          </person-group>
          <article-title>Vocal tract length and formant frequency dispersion correlate with body size in rhesus macaques</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1997</year>
          <volume>102</volume>
          <fpage>1213</fpage>
          <lpage>1222</lpage>
          <pub-id pub-id-type="pmid">9265764</pub-id>
        </citation>
      </ref>
      <ref id="bib4">
        <label>4</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Owren</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Seyfarth</surname>
              <given-names>R.M.</given-names>
            </name>
            <name>
              <surname>Cheney</surname>
              <given-names>D.L.</given-names>
            </name>
          </person-group>
          <article-title>The acoustic features of vowel-like grunt calls in chacma baboons (Papio cyncephalus ursinus): Implications for production processes and functions</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1997</year>
          <volume>101</volume>
          <fpage>2951</fpage>
          <lpage>2963</lpage>
          <pub-id pub-id-type="pmid">9165741</pub-id>
        </citation>
      </ref>
      <ref id="bib5">
        <label>5</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rendall</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Owren</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Rodman</surname>
              <given-names>P.S.</given-names>
            </name>
          </person-group>
          <article-title>The role of vocal tract filtering in identity cueing in rhesus monkey (Macaca mulatta) vocalizations</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1998</year>
          <volume>103</volume>
          <fpage>602</fpage>
          <lpage>614</lpage>
          <pub-id pub-id-type="pmid">9440345</pub-id>
        </citation>
      </ref>
      <ref id="bib6">
        <label>6</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pfefferle</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Fischer</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Sounds and size: Identification of acoustic variables that reflect body size in hamadryas baboons, Papio hamadryas</article-title>
          <source>Anim. Behav.</source>
          <year>2006</year>
          <volume>72</volume>
          <fpage>43</fpage>
          <lpage>51</lpage>
        </citation>
      </ref>
      <ref id="bib7">
        <label>7</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rendall</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic correlates of caller identity and affect intensity in the vowel-like grunt vocalizations of baboons</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2003</year>
          <volume>113</volume>
          <fpage>3390</fpage>
          <lpage>3402</lpage>
          <pub-id pub-id-type="pmid">12822809</pub-id>
        </citation>
      </ref>
      <ref id="bib8">
        <label>8</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Riede</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zuberbuhler</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>The relationship between acoustic structure and semantic information in Diana monkey alarm vocalization</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2003</year>
          <volume>114</volume>
          <fpage>1132</fpage>
          <lpage>1142</lpage>
          <pub-id pub-id-type="pmid">12942990</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <label>9</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Owren</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic classification of alarm calls by vervet monkeys (Cercopithecus aethiops) and humans (Homo sapiens). 1. Natural Calls</article-title>
          <source>J. Comp. Psychol.</source>
          <year>1990</year>
          <volume>104</volume>
          <fpage>20</fpage>
          <lpage>28</lpage>
          <pub-id pub-id-type="pmid">2354626</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <label>10</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Owren</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic classification of alarm calls by vervet monkeys (Cercopithecus aethiops) and humans (Homo sapiens). 2. Synthetic calls</article-title>
          <source>J. Comp. Psychol.</source>
          <year>1990</year>
          <volume>104</volume>
          <fpage>29</fpage>
          <lpage>40</lpage>
          <pub-id pub-id-type="pmid">2354627</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <label>11</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>Facial expressions linked to monkey calls</article-title>
          <source>Nature</source>
          <year>2003</year>
          <volume>423</volume>
          <fpage>937</fpage>
          <lpage>938</lpage>
          <pub-id pub-id-type="pmid">12827188</pub-id>
        </citation>
      </ref>
      <ref id="bib12">
        <label>12</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maier</surname>
              <given-names>J.X.</given-names>
            </name>
            <name>
              <surname>Neuhoff</surname>
              <given-names>J.G.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
          </person-group>
          <article-title>Multisensory integration of looming signals by Rhesus monkeys</article-title>
          <source>Neuron</source>
          <year>2004</year>
          <volume>43</volume>
          <fpage>177</fpage>
          <lpage>181</lpage>
          <pub-id pub-id-type="pmid">15260954</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <label>13</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>The interaction of glottal-pulse rate and vocal-tract length in judgement of speaker size, sex and age</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>3177</fpage>
          <lpage>3186</lpage>
          <pub-id pub-id-type="pmid">16334696</pub-id>
        </citation>
      </ref>
      <ref id="bib14">
        <label>14</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mollis</surname>
              <given-names>M.R.</given-names>
            </name>
          </person-group>
          <article-title>Evaluating models of vowel perception</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>1062</fpage>
          <lpage>1071</lpage>
          <pub-id pub-id-type="pmid">16158661</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <label>15</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lieberman</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Laitman</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Reidenberg</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Gannon</surname>
              <given-names>P.J.</given-names>
            </name>
          </person-group>
          <article-title>The anatomy, physiology, acoustics and perception of speech: Essential elements in analysis of the evolution of human speech</article-title>
          <source>J. Hum. Evol.</source>
          <year>1992</year>
          <volume>23</volume>
          <fpage>447</fpage>
          <lpage>467</lpage>
        </citation>
      </ref>
      <ref id="bib16">
        <label>16</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bachorowski</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Owren</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic correlates of talker sex and individual talker identity are present in a short vowel segment produced in running speech</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1999</year>
          <volume>106</volume>
          <fpage>1054</fpage>
          <lpage>1063</lpage>
          <pub-id pub-id-type="pmid">10462810</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <label>17</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rendall</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kollias</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ney</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lloyd</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Pitch (F-0) and formant profiles of human vowels and vowel-like baboon grunts: The role of vocalizer body size and voice-acoustic allometry</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>117</volume>
          <fpage>944</fpage>
          <lpage>955</lpage>
          <pub-id pub-id-type="pmid">15759713</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <label>18</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
          </person-group>
          <article-title>Vocal Tract Length Perception and the Evolution of Language</article-title>
          <year>1994</year>
          <publisher-name>Brown University</publisher-name>
          <publisher-loc>Providence, RI</publisher-loc>
        </citation>
      </ref>
      <ref id="bib19">
        <label>19</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Feinberg</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>B.C.</given-names>
            </name>
            <name>
              <surname>Little</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Burt</surname>
              <given-names>D.M.</given-names>
            </name>
            <name>
              <surname>Perrett</surname>
              <given-names>D.I.</given-names>
            </name>
          </person-group>
          <article-title>Manipulations of fundamental and formant frequencies influence the attractiveness of human male voices</article-title>
          <source>Anim. Behav.</source>
          <year>2005</year>
          <volume>69</volume>
          <fpage>561</fpage>
          <lpage>568</lpage>
        </citation>
      </ref>
      <ref id="bib20">
        <label>20</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ives</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Discrimination of speaker size from syllable phrases</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>3816</fpage>
          <lpage>3822</lpage>
          <pub-id pub-id-type="pmid">16419826</pub-id>
        </citation>
      </ref>
      <ref id="bib21">
        <label>21</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Irino</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>The processing and perception of size information in speech sounds</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2005</year>
          <volume>117</volume>
          <fpage>305</fpage>
          <lpage>318</lpage>
          <pub-id pub-id-type="pmid">15704423</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <label>22</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruckert</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lienard</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Lacroix</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kreutzer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Leboucher</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Women use voice parameters to assess men's characteristics</article-title>
          <source>Proc. Biol. Sci.</source>
          <year>2006</year>
          <volume>273</volume>
          <fpage>83</fpage>
          <lpage>89</lpage>
          <pub-id pub-id-type="pmid">16519239</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <label>23</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Collins</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Missing</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Vocal and visual attractiveness are related in women</article-title>
          <source>Anim. Behav.</source>
          <year>2003</year>
          <volume>65</volume>
          <fpage>997</fpage>
          <lpage>1004</lpage>
        </citation>
      </ref>
      <ref id="bib24">
        <label>24</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Giedd</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Morphology and development of the human vocal tract: A study using magnetic resonance imaging</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1999</year>
          <volume>106</volume>
          <fpage>1511</fpage>
          <lpage>1522</lpage>
          <pub-id pub-id-type="pmid">10489707</pub-id>
        </citation>
      </ref>
      <ref id="bib25">
        <label>25</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hauser</surname>
              <given-names>M.D.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>C.S.</given-names>
            </name>
            <name>
              <surname>Marler</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>The Role of Articulation in the Production of Rhesus-Monkey, Macaca Mulatta, Vocalizations</article-title>
          <source>Anim. Behav.</source>
          <year>1993</year>
          <volume>45</volume>
          <fpage>423</fpage>
          <lpage>433</lpage>
        </citation>
      </ref>
      <ref id="bib26">
        <label>26</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
          </person-group>
          <article-title>Primate vocal production and its implications for auditory research</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
          </person-group>
          <source>Primate Audition: Ethology and Neurobiology</source>
          <year>2003</year>
          <publisher-name>CRC Press</publisher-name>
          <publisher-loc>Boca Raton</publisher-loc>
          <fpage>87</fpage>
          <lpage>108</lpage>
        </citation>
      </ref>
      <ref id="bib27">
        <label>27</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Kelley</surname>
              <given-names>J.P.</given-names>
            </name>
          </person-group>
          <article-title>Perception of vocal tract resonances by whooping cranes, Grus americana</article-title>
          <source>Ethology</source>
          <year>2000</year>
          <volume>106</volume>
          <fpage>559</fpage>
          <lpage>574</lpage>
        </citation>
      </ref>
      <ref id="bib28">
        <label>28</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reby</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>McComb</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Cargnelutti</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Darwin</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Clutton-Brock</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Red deer stags use formants as assessment cues during intrasexual agonistic interactions</article-title>
          <source>Proc. Biol. Sci.</source>
          <year>2005</year>
          <volume>272</volume>
          <fpage>941</fpage>
          <lpage>947</lpage>
          <pub-id pub-id-type="pmid">16024350</pub-id>
        </citation>
      </ref>
      <ref id="bib29">
        <label>29</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Fritz</surname>
              <given-names>J.B.</given-names>
            </name>
          </person-group>
          <article-title>Rhesus monkeys spontaneously perceive formants in conspecific vocalizations</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2006</year>
          <volume>120</volume>
          <fpage>2132</fpage>
          <lpage>2141</lpage>
          <pub-id pub-id-type="pmid">17069311</pub-id>
        </citation>
      </ref>
      <ref id="bib30">
        <label>30</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Masuda-Kasuse</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>de Cheveigne</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Restructuring speech representations using pitch-adaptive time-frequency smoothing and instantaneous-frequency-based F0 extraction: A possible role of repetitive structure in sounds</article-title>
          <source>Speech Commun.</source>
          <year>1999</year>
          <volume>27</volume>
          <fpage>187</fpage>
          <lpage>207</lpage>
        </citation>
      </ref>
      <ref id="bib31">
        <label>31</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kuhl</surname>
              <given-names>P.K.</given-names>
            </name>
            <name>
              <surname>Meltzoff</surname>
              <given-names>A.N.</given-names>
            </name>
          </person-group>
          <article-title>The intermodal representation of speech in infants</article-title>
          <source>Infant Behav. Dev.</source>
          <year>1984</year>
          <volume>7</volume>
          <fpage>361</fpage>
          <lpage>381</lpage>
        </citation>
      </ref>
      <ref id="bib32">
        <label>32</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Patterson</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Werker</surname>
              <given-names>J.F.</given-names>
            </name>
          </person-group>
          <article-title>Matching phonetic information in lips and voice is robust in 4.5-month-old infants</article-title>
          <source>Infant Behav. Dev.</source>
          <year>1999</year>
          <volume>22</volume>
          <fpage>237</fpage>
          <lpage>247</lpage>
        </citation>
      </ref>
      <ref id="bib33">
        <label>33</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heinz</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>J.V.</given-names>
            </name>
          </person-group>
          <article-title>The acquisition of vowel discriminations by nonhuman primates</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1988</year>
          <volume>84</volume>
          <fpage>186</fpage>
          <lpage>194</lpage>
          <pub-id pub-id-type="pmid">3411047</pub-id>
        </citation>
      </ref>
      <ref id="bib34">
        <label>34</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sommers</surname>
              <given-names>M.S.</given-names>
            </name>
            <name>
              <surname>Moody</surname>
              <given-names>D.B.</given-names>
            </name>
            <name>
              <surname>Prosen</surname>
              <given-names>C.A.</given-names>
            </name>
            <name>
              <surname>Stebbins</surname>
              <given-names>W.C.</given-names>
            </name>
          </person-group>
          <article-title>Formant frequency discrimination by Japanese macaques (Macaca fuscata)</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1992</year>
          <volume>91</volume>
          <fpage>3499</fpage>
          <lpage>3510</lpage>
          <pub-id pub-id-type="pmid">1619126</pub-id>
        </citation>
      </ref>
      <ref id="bib35">
        <label>35</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Le Prell</surname>
              <given-names>C.G.</given-names>
            </name>
            <name>
              <surname>Niemiec</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Moody</surname>
              <given-names>D.B.</given-names>
            </name>
          </person-group>
          <article-title>Macaque thresholds for detecting increases in intensity: Effects of formant structure</article-title>
          <source>Hear. Res.</source>
          <year>2001</year>
          <volume>162</volume>
          <fpage>29</fpage>
          <lpage>42</lpage>
          <pub-id pub-id-type="pmid">11707349</pub-id>
        </citation>
      </ref>
      <ref id="bib36">
        <label>36</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sinnott</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>Detection and discrimination of synthetic English vowels by Old-World monkeys (Cercopithecus, Macaca) and humans</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1989</year>
          <volume>86</volume>
          <fpage>557</fpage>
          <lpage>565</lpage>
          <pub-id pub-id-type="pmid">2768672</pub-id>
        </citation>
      </ref>
      <ref id="bib37">
        <label>37</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reby</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>McComb</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Anatomical constraints generate honesty: Acoustic cues to age and weight in the roars of red deer stags</article-title>
          <source>Anim. Behav.</source>
          <year>2003</year>
          <volume>65</volume>
          <fpage>519</fpage>
          <lpage>530</lpage>
        </citation>
      </ref>
      <ref id="bib38">
        <label>38</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
            <name>
              <surname>Reby</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>The descended larynx is not uniquely human</article-title>
          <source>Proc. R. Soc. Lond. B. Biol. Sci.</source>
          <year>2001</year>
          <volume>268</volume>
          <fpage>1669</fpage>
          <lpage>1675</lpage>
        </citation>
      </ref>
      <ref id="bib39">
        <label>39</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gonzalez</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Formant frequencies and body size of speaker: A weak relationship in adult humans</article-title>
          <source>J. Phonetics</source>
          <year>2004</year>
          <volume>32</volume>
          <fpage>277</fpage>
          <lpage>287</lpage>
        </citation>
      </ref>
      <ref id="bib40">
        <label>40</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fitch</surname>
              <given-names>W.T.</given-names>
            </name>
          </person-group>
          <article-title>The evolution of speech: A comparative review</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2000</year>
          <volume>4</volume>
          <fpage>258</fpage>
          <lpage>267</lpage>
          <pub-id pub-id-type="pmid">10859570</pub-id>
        </citation>
      </ref>
      <ref id="bib41">
        <label>41</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Hauser</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>The neuroethology of primate vocal communication: Substrates for the evolution of speech</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>1999</year>
          <volume>3</volume>
          <fpage>377</fpage>
          <lpage>384</lpage>
          <pub-id pub-id-type="pmid">10498928</pub-id>
        </citation>
      </ref>
      <ref id="bib42">
        <label>42</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ghazanfar</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Maier</surname>
              <given-names>J.X.</given-names>
            </name>
            <name>
              <surname>Hoffman</surname>
              <given-names>K.L.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title>
          <source>J. Neurosci.</source>
          <year>2005</year>
          <volume>25</volume>
          <fpage>5004</fpage>
          <lpage>5012</lpage>
          <pub-id pub-id-type="pmid">15901781</pub-id>
        </citation>
      </ref>
      <ref id="bib43">
        <label>43</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>A.-L.</given-names>
            </name>
          </person-group>
          <article-title>Implicit multisensory associations influence voice recognition</article-title>
          <source>PLoS Biol.</source>
          <year>2006</year>
          <volume>4</volume>
          <fpage>1809</fpage>
          <lpage>1820</lpage>
        </citation>
      </ref>
      <ref id="bib44">
        <label>44</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Warren</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Ives</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
            <name>
              <surname>Griffiths</surname>
              <given-names>T.D.</given-names>
            </name>
          </person-group>
          <article-title>Processing the acoustic effect of size in speech sounds</article-title>
          <source>Neuroimage</source>
          <year>2006</year>
          <volume>32</volume>
          <fpage>368</fpage>
          <lpage>375</lpage>
          <pub-id pub-id-type="pmid">16644240</pub-id>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>Auditory and Visual Stimuli Used in the Current Experiments</p>
        <p>(A) Resynthesized coo vocalizations based on one of the two coo exemplars used in the preferential-looking paradigm. Diagram shows the spectrograms and waveforms of a coo vocalization resynthesized with two different vocal-tract lengths. The arrow in the spectrogram indicates the position of an individual formant, which increases in frequency as the apparent vocal-tract length decreases.</p>
        <p>(B) Power spectra (black line) and linear predictive coding spectra (gray lines) for the long vocal-tract length (10 cm, top panel) and short vocal-tract length (5.5 cm, bottom panel) used in the experiment and seen in (A).</p>
        <p>(C) Still frames extracted from the videos used in the preferential-looking experiments. The top row shows frames from the large monkey. Videos were synchronized and edited so that they appeared to be synchronously producing the coo vocalization shown in (A).</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Figure 2</label>
      <caption>
        <p>Monkeys Match the Acoustic Size Extracted from Formant Frequencies to the Matching Face</p>
        <p>(A) The mean percentage of total looking time spent looking at the matching video display; the dotted line indicates chance expectation (n = 24). Error bars represent the standard error of the mean.</p>
        <p>(B) A significant proportion of subjects looked longer at the match than the nonmatch screen.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
  </floats-wrap>
</article>