<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuropsychologia</journal-id>
      <journal-title>Neuropsychologia</journal-title>
      <issn pub-type="ppub">0028-3932</issn>
      <publisher>
        <publisher-name>Pergamon Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2394569</article-id>
      <article-id pub-id-type="pmid">18249420</article-id>
      <article-id pub-id-type="publisher-id">NSY2778</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.11.026</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Cortical circuits for silent speechreading in deaf and hearing people</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Capek</surname>
            <given-names>Cheryl M.</given-names>
          </name>
          <email>c.capek@ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>MacSweeney</surname>
            <given-names>Mairéad</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Woll</surname>
            <given-names>Bencie</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Waters</surname>
            <given-names>Dafydd</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>McGuire</surname>
            <given-names>Philip K.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>David</surname>
            <given-names>Anthony S.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Brammer</surname>
            <given-names>Michael J.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Campbell</surname>
            <given-names>Ruth</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>a</sup>Deafness, Cognition and Language Research Centre, Division of Psychology and Language Sciences, University College London, 49 Gordon Square, London WC1H 0PD, United Kingdom</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>b</sup>Behavioural and Brain Sciences Unit, UCL Institute of Child Health, University College London, 30 Guilford Street, London WC1N 1EH, United Kingdom</addr-line>
      </aff>
      <aff id="aff3">
        <addr-line><sup>c</sup>Institute of Psychiatry, Kings College London, De Crespigny Park, London SE5 8AF, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. Tel.: +44 207 679 8673; fax: +44 207 679 8691. <email>c.capek@ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <year>2008</year>
      </pub-date>
      <volume>46</volume>
      <issue>5</issue>
      <fpage>1233</fpage>
      <lpage>1241</lpage>
      <history>
        <date date-type="received">
          <day>1</day>
          <month>3</month>
          <year>2007</year>
        </date>
        <date date-type="rev-recd">
          <day>8</day>
          <month>10</month>
          <year>2007</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>11</month>
          <year>2007</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2008 Elsevier Ltd.</copyright-statement>
        <copyright-year>2007</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Abstract</title>
        <p>This fMRI study explored the functional neural organisation of seen speech in congenitally deaf native signers and hearing non-signers. Both groups showed extensive activation in perisylvian regions for speechreading words compared to viewing the model at rest. In contrast to earlier findings, activation in left middle and posterior portions of superior temporal cortex, including regions within the lateral sulcus and the superior and middle temporal gyri, was greater for deaf than hearing participants. This activation pattern survived covarying for speechreading skill, which was better in deaf than hearing participants. Furthermore, correlational analysis showed that regions of activation related to speechreading skill varied with the hearing status of the observers. Deaf participants showed a positive correlation between speechreading skill and activation in the middle/posterior superior temporal cortex. In hearing participants, however, more posterior and inferior temporal activation (including fusiform and lingual gyri) was positively correlated with speechreading skill. Together, these findings indicate that activation in the left superior temporal regions for silent speechreading can be modulated by both hearing status and speechreading skill.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Deafness</kwd>
        <kwd>Brain</kwd>
        <kwd>Language</kwd>
        <kwd>Sign language</kwd>
        <kwd>Speechreading</kwd>
        <kwd>fMRI</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <label>1</label>
      <title>Introduction</title>
      <p>Auditory speech processing reliably engages perisylvian regions, particularly in the left hemisphere (e.g., <xref rid="bib35" ref-type="bibr">Scott &amp; Johnsrude, 2003</xref>). In hearing people, perisylvian regions are also recruited for silent speechreading. In particular, silent speechreading elicits activation in superior temporal regions, including middle and posterior portions of the superior temporal gyrus, its dorsal and ventral surfaces (i.e., lateral sulcus and superior temporal sulcus or STS, respectively) and the middle temporal gyrus (<xref rid="bib3 bib11 bib10" ref-type="bibr">Bernstein et al., 2002; Calvert et al., 1997; Calvert et al., 1999</xref>; <xref rid="bib12" ref-type="bibr">Calvert, Campbell, &amp; Brammer, 2000</xref>; <xref rid="bib22 bib23 bib29 bib30" ref-type="bibr">Ludman et al., 2000; MacSweeney et al., 2000; Paulesu et al., 2003; Pekkola et al., 2005</xref>; <xref rid="bib33" ref-type="bibr">Ruytjens, Albers, van Dijk, Wit, &amp; Willemsen, 2006</xref>), and inferior frontal regions (<xref rid="bib6 bib13" ref-type="bibr">Buccino et al., 2004; Campbell et al., 2001</xref>; <xref rid="bib28" ref-type="bibr">Nishitani &amp; Hari, 2002</xref>; <xref rid="bib29" ref-type="bibr">Paulesu et al., 2003</xref>; <xref rid="bib37" ref-type="bibr">Watkins, Strafella, &amp; Paus, 2003</xref>). Generally, seen speech appears to engage similar circuits to those activated when speech is heard. This includes portions of the superior temporal cortex reliably involved in processing auditory information. Activation in this region also appears to be modulated by speechreading skill. <xref rid="bib21" ref-type="bibr">Hall, Fussell, and Summerfield (2005)</xref> did not find marked activation in the superior temporal gyrus at the group level when hearing adults observed silently spoken sentences, as compared to viewing facial gurning. However, their participants varied greatly in their ability to speechread, and a positive correlation was found between activation in the left posterior superior temporal gyrus and speechreading skill.</p>
      <p>Deaf people can outperform hearing people in comprehending seen speech (<xref rid="bib4" ref-type="bibr">Bernstein, Demorest, &amp; Tucker, 2000</xref>; <xref rid="bib27" ref-type="bibr">Mohammed, Campbell, MacSweeney, Barry, &amp; Coleman, 2006</xref>). Nevertheless, earlier reports suggested that superior temporal activation for speechreading was less reliably observed in deaf than in hearing people (<xref rid="bib25 bib24" ref-type="bibr">MacSweeney et al., 2001; MacSweeney et al., 2002</xref>). However, the group size for these studies was small (<italic>n</italic> = 6), and so there may not have been sufficient statistical power to detect activation in this region. Furthermore, while the speechreading task in <xref rid="bib24" ref-type="bibr">MacSweeney et al.'s (2002)</xref> study was easy (identify spoken numbers between 1 and 9), it was compared with a relatively high-level task—counting numbers of meaningless mouth actions. In contrast, a separate study by <xref rid="bib34" ref-type="bibr">Sadato et al. (2005)</xref>, reported activation in superior temporal regions in both hearing and deaf participants viewing speech-like actions. Here, the stimulus was a cartoon avatar opening and closing its mouth to form different vowel-like patterns, which participants may have interpreted as phonological gestures.</p>
      <p>The present study is the first to examine patterns of activation in deaf people who are proficient speechreaders while they searched for a speechread target embedded in lists of unrelated words. We anticipated that both hearing status and speechreading ability, measured outside the scanner, may determine the extent of activation in perisylvian regions. This was explored in two complementary ways. First, the group comparison between deaf and hearing activation patterns was assessed with speechreading skill entered into the analysis as a covariate. Speechreading skill was assessed using the Test of Adult Speechreading (TAS, <xref rid="bib27" ref-type="bibr">Mohammed et al., 2006</xref>). By ‘partialling out’ individual differences in speechreading ability, we hoped to establish whether activation in brain regions could be modulated as a function of hearing status, irrespective of speechreading skill. Second, we used correlational analysis to establish, for each group in turn, which regions were sensitive to variations in speechreading skill.</p>
      <p>To summarise, this study examines cortical correlates for the perception of lists of speechread words under lexical target detection conditions. We aimed to identify regions that may be activated during observation of silently spoken lexical items that are not drawn from a closed set, and when the contrast (baseline) condition was a speaker at rest. The questions posed were: (1) To what extent do prelingually deaf people who are proficient signers and speechreaders show activation in superior temporal regions, including auditory cortical processing regions? (2) Are the patterns of activation different in deaf and hearing people? (3) In which regions is speechreading ability positively correlated with activation?</p>
    </sec>
    <sec sec-type="materials-methods">
      <label>2</label>
      <title>Method</title>
      <sec>
        <label>2.1</label>
        <title>Participants</title>
        <p>Thirteen (six female; mean age: 27.4; age range: 18–49) deaf adults were tested. All were congenitally (severely or profoundly) deaf (81 dB mean loss or greater in the better ear over four octaves, spanning 500–4000 Hz). Across the group, the mean hearing loss in the better ear was 103 dB. They were all native signers, having acquired British Sign Language (BSL) from their deaf signing parents. Thirteen (six female; mean age: 29.4; age range: 18–43) hearing, monolingual speakers of English were also tested. All participants were right-handed with no known neurological or behavioural abnormalities. Non-verbal IQ was measured using the Block Design subtest of the WAIS-R. Speechreading was measured using the Test of Adult Speechreading (TAS). The TAS comprises three subtests of silent speechreading in English: word identification, sentence identification, and short story identification (<xref rid="bib27" ref-type="bibr">Mohammed et al., 2006</xref>). Independent-samples <italic>t</italic>-tests showed that deaf and hearing participants did not differ on non-verbal IQ (<italic>p</italic> &gt; 0.1). However, deaf participants scored significantly higher than hearing non-signers on the TAS (<italic>t</italic> (24) = 4.779, <italic>p</italic> &lt; 0.001), confirming earlier findings (<xref rid="bib27" ref-type="bibr">Mohammed et al., 2006</xref>) with an independent sample of participants. Participant characteristics are summarised in <xref rid="tbl1" ref-type="table">Table 1</xref>. Standard scores (TAS <italic>z</italic>-scores) were derived from the populations reported in <xref rid="bib27" ref-type="bibr">Mohammed et al.'s (2006)</xref> study, together with those for the present study. These scores were calculated separately for deaf and for hearing groups. Standard scores were used in order to correct for the differences in statistical distribution of scores within the deaf and the hearing groups, and were used in the correlational analyses (see <xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
        <p>All participants gave written informed consent to participate in the study according to the Declaration of Helsinki (BMJ 1991; 302: 1194) and the study was approved by the Institute of Psychiatry/South London and Maudsley NHS Trust Research Ethics Committee.</p>
      </sec>
      <sec>
        <label>2.2</label>
        <title>Stimuli</title>
        <p>Stimuli were full-colour motion video of silently mouthed English words. Stimuli were modelled by a deaf native signer of BSL, who spoke English fluently (i.e., a BSL-English bilingual). The model was viewed full-face and torso. The words to be speechread were piloted on adult hearing volunteers who were not scanned. The final stimuli comprised only those words that were speechreadable by the hearing pilots. Stimuli consisted of both content words (nouns) and descriptive terms (both adjectival and adverbial).</p>
      </sec>
      <sec>
        <label>2.3</label>
        <title>fMRI experimental design and task</title>
        <p>The speechreading task was one of four conditions presented to participants. The other three conditions comprised signed language (BSL) material (not reported here). The speech stimuli were presented in blocks, alternating with blocks of the other three experimental conditions (30-s blocks for each condition), and with a 15-s baseline condition. The total run duration for all four conditions and baseline was 15 min. Both deaf and hearing participants were given the same target-detection task and instructions. During the speechreading condition, participants were instructed to watch the speech patterns produced by the model and to try to understand them. They were required to make a push-button response whenever the model was seen to be saying ‘yes’. This relatively passive task was chosen in preference to a ‘deeper’ processing task (such as semantic classification) for several reasons. First, it allowed for relatively automatic processing of non-target items to occur (as confirmed in post-scan tests). Second, it ensured similar difficulty of the task across stimulus conditions. As hearing non-signers would not be able to perform a semantic task on the sign stimuli, using a sparse target detection task enabled all participants to perform the same task during all experimental conditions. Over the course of the experiment, participants viewed 96 stimulus items, 24 in each of the four experimental conditions. Items were not repeated within the same block and were pseudorandomised to ensure that repeats were not clustered at the end of the experiment. Each participant saw five blocks of the speechreading condition.</p>
        <p>The baseline condition comprised video of the model at rest. The model's face and torso were shown, as in the experimental conditions. During the baseline condition, participants were directed to press a button when a grey fixation cross, digitally superimposed on the face region of the resting model, turned red. To maintain vigilance, targets in both the experimental and baseline conditions occurred randomly at a rate of one per block. Prior to the scan, participants practiced the tasks and were shown examples of the ‘yes’ targets outside the scanner using video of a model and words that were similar but not identical to those used in the experiment. Following the experiment, a sample of the hearing participants (8 of 13) and all of the deaf participants were asked to identify the items they had seen.</p>
        <p>Stimuli in the experimental conditions appeared at a rate of 15 items per block. The rate of articulation across all experimental conditions, including the speechreading blocks, was approximately one item every 2 s. All stimuli were projected onto a screen located at the base of the scanner table via a Sanyo XU40 LCD projector and then projected to a mirror angled above the participant's head.</p>
      </sec>
      <sec>
        <label>2.4</label>
        <title>Imaging parameters</title>
        <p>Gradient echoplanar MRI data were acquired with a 1.5-T General Electric Signa Excite (Milwaukee, WI, USA) with TwinSpeed gradients and fitted with an 8-channel quadrature head coil. Three hundred <mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math>-weighted images depicting BOLD contrast were acquired at each of the 40 near-axial 3 mm thick planes parallel to the intercommissural (AC-PC) line (0.3 mm interslice gap; TR = 3 s, TE = 40 ms, flip angle = 90°). The field of view for the fMRI runs was 240 mm, and the matrix size was 64 × 64, with a resultant in-plane voxel size of 3.75 mm. High-resolution EPI scans were acquired to facilitate registration of individual fMRI datasets to Talairach space (<xref rid="bib36" ref-type="bibr">Talairach &amp; Tournoux, 1988</xref>). These comprised 40 near-axial 3 mm slices (0.3 mm gap), which were acquired parallel to the AC-PC line. The field of view for these scans was matched to that of the fMRI scans, but the matrix size was increased to 128 × 128, resulting in an in-plane voxel size of 1.875 mm. Other scan parameters (TR = 3 s, TE = 40 ms, flip angle = 90°) were, where possible, matched to those of the main EPI run, resulting in similar image contrast.</p>
      </sec>
      <sec>
        <label>2.5</label>
        <title>Data analysis</title>
        <p>Following computation of the model fit, a goodness of fit statistic was derived by calculating the ratio between the sum of squares due to the model fit and the residual sum of squares (SSQ ratio) at each voxel. Permutation testing, as well as its freedom from many of the distributional assumptions of parametric tests, also offers the possibility of testing a number of statistics that are not easily testable parametrically. The SSQ ratio is such a statistic and is a simplified substitute for the <italic>F</italic> statistic suggested by <xref rid="bib16" ref-type="bibr">Edgington (1995)</xref> that avoids the necessity of calculating the residual degrees of freedom of the time series following model fitting.</p>
        <p>The data were permuted by the wavelet-based method described by <xref rid="bib8" ref-type="bibr">Bullmore et al. (2001)</xref> with the exception that, prior to permutation, any wavelet coefficients exceeding the calculated threshold (as described by <xref rid="bib15" ref-type="bibr">Donoho and Johnstone (1994)</xref>) were removed. These were replaced by the threshold value. This step reduces the likelihood of refitting large, experimentally unrelated components of the signal following permutation.</p>
        <p>Significant values of the SSQ were identified by comparing this statistic with the null distribution, determined by repeating the fitting procedure 20 times at each voxel and combining data over all intracerebral voxels. This procedure preserves the noise characteristics of the time series during the permutation process, and the global assessment of the null distribution performed in this way provides good control of Type I error rates (<xref rid="bib8" ref-type="bibr">Bullmore et al., 2001</xref>). The voxel-wise SSQ ratios were calculated for each subject from the observed data and, following time series permutation, were transformed into standard space (<xref rid="bib36" ref-type="bibr">Talairach &amp; Tournoux, 1988</xref>) as described previously (<xref rid="bib5 bib7" ref-type="bibr">Brammer et al., 1997; Bullmore et al., 1996</xref>). The Talairach transformation stage was performed in two parts. First, each participant's fMRI data was realigned with their own high resolution <mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math>-weighted images using a rigid body transformation. Second, an affine transformation to the Talairach template was computed. The cost function for both transformations was the maximization of the correlation between the images. Voxel size in Talairach space was 3 mm × 3 mm × 3 mm.</p>
      </sec>
      <sec>
        <label>2.6</label>
        <title>Group analysis</title>
        <p>Identification of active 3-D clusters was performed by first thresholding the median voxel-level SSQ ratio maps at the false positive probability of 0.05. The activated voxels were assembled into 3-D connected clusters and the sum of the SSQ ratios (statistical cluster mass) was determined for each cluster. This procedure was repeated for the median SSQ ratio maps obtained from the wavelet-permuted data to compute the null distribution of statistical cluster masses under the null hypothesis. The cluster-wise false positive threshold was then set using this distribution to give an expected false positive rate of &lt;1 cluster per brain (<xref rid="bib9" ref-type="bibr">Bullmore et al., 1999</xref>).</p>
      </sec>
      <sec>
        <label>2.7</label>
        <title>ANOVA</title>
        <p>Differences between the groups were calculated by fitting the data at each voxel in which all participants had non-zero data using the following linear model, <italic>Y</italic> = <italic>a</italic> + <italic>bX</italic> + <italic>e</italic>, where <italic>Y</italic> is the vector of BOLD effect sizes for each individual, <italic>X</italic> is the contrast matrix for the particular inter-group contrast required, <italic>a</italic> is the mean effect across all individuals in the groups, <italic>b</italic> is the computed group difference and <italic>e</italic> is a vector of residual errors. The model is fitted by minimising the sum of absolute deviations rather than the sums of squares to reduce outlier effects. The null distribution of <italic>b</italic> is computed by permuting data between groups (assuming the null hypothesis of no effect of group) and refitting the above model. This permutation method thus gives an exact test (for this set of data) of the probability of the value of <italic>b</italic> in the unpermuted data under the null hypothesis. The permutation process permits estimation of the distribution of <italic>b</italic> under the null hypothesis of no mean difference. Identification of significantly activated clusters was performed by using the cluster-wise false positive threshold that yielded an expected false positive rate of &lt;1 cluster per brain (<xref rid="bib9" ref-type="bibr">Bullmore et al., 1999</xref>).</p>
      </sec>
      <sec>
        <label>2.8</label>
        <title>ANCOVA</title>
        <p>Analysis of covariance was used to address behavioural differences between the deaf and hearing participants in relation to the patterns of activation for the speechreading condition (see <xref rid="tbl1" ref-type="table">Table 1</xref>). Differences in responses (<italic>R</italic>) were inferred at each voxel using the linear model, <italic>R</italic> = <italic>a</italic>0 + <italic>a</italic>1<italic>H</italic> + <italic>a</italic>2<italic>X</italic> + <italic>e</italic>, where <italic>H</italic> codes the contrast(s) of interest between groups, <italic>X</italic> is a covariate and <italic>e</italic> is the residual error. Maps of the standardized coefficient (size of group difference) (<italic>a</italic>1), were tested for significance against the null distribution of <italic>a</italic>1 (no effect of group membership) generated by repeatedly refitting the above model at each voxel following randomization of group membership (<italic>H</italic>).</p>
      </sec>
      <sec>
        <label>2.9</label>
        <title>Correlational analysis</title>
        <p>In order to examine the relationship between brain activation and speechreading skill, correlational analysis was performed between the BOLD effect data for each individual and the Test of Adult Speechreading (TAS) <italic>z</italic>-score. These were calculated separately for each group. Pearson product–moment correlation coefficients were calculated between the observed behavioural and BOLD effect data. The null distribution of correlation coefficients was then computed by permuting the BOLD data 100 times per voxel and then combining the data over all voxels. Median voxel-level maps were computed at the false probability of 0.05 and cluster-level maps, where <italic>r</italic> was significant, were computed such that the expected false positive rate was &lt;1 cluster per brain.</p>
      </sec>
    </sec>
    <sec>
      <label>3</label>
      <title>Results</title>
      <sec>
        <label>3.1</label>
        <title>Behavioural data</title>
        <p>All participants completed the behavioural (target detection) task in the scanner reasonably accurately. Deaf participants identified the speechreading targets more accurately than hearing participants (mean accuracy (max = 5), deaf = 4.69, hearing = 3.85, <italic>t</italic>(24) = 2.99, <italic>p</italic> = 0.007). Speechreading target identification was slower in deaf than hearing participants (mean RT, deaf = 1192.63 ms, hearing = 920.08 ms, <italic>t</italic>(24) = 4.15, <italic>p</italic> &lt; 0.001). Following scanning, participants were presented with the experimental stimuli. The deaf participants identified more words than the hearing participants (mean percent correct identification, deaf = 69%, hearing = 46%), <italic>t</italic>(19) = 4.11, <italic>p</italic> = 0.001). The behavioural data suggest that deaf participants’ greater accuracy in identification of non-target items (as indicated by the post-scan test) may have interfered with their processing of the target (as indicated by the relatively slow reaction times to targets in the scanner).</p>
      </sec>
      <sec>
        <label>3.2</label>
        <title>fMRI data</title>
        <sec>
          <label>3.2.1</label>
          <title>Speechreading vs. baseline</title>
          <p>In both deaf and hearing groups, extensive activation was observed in fronto-temporal cortices, bilaterally (<xref rid="tbl2" ref-type="table">Table 2</xref>, <xref rid="fig1" ref-type="fig">Fig. 1</xref>). In deaf participants, activation in the left superior temporal cortex was focused at the border between the posterior superior temporal gyrus and the transverse temporal gyrus (BA 42/41) and extended to the middle (BA 21) and inferior (BAs 37, 19) temporal gyri and the supramarginal gyrus (BA 40). This cluster of activation also extended to inferior (BAs 44, 45) and middle (BAs 6, 9) frontal gyri and precentral gyrus (BA 4). In the right hemisphere, a cluster of activation focused in the superior/middle temporal gyri (BA 22/21) extended to BAs 42 and 41 and posterior inferior temporal gyrus (BAs 37, 19). Activation in the right frontal cortex was focused in the precentral gyrus (BA 6) and extended to the inferior (BAs 44, 45) and middle (BAs 46, 9) frontal gyri. Additional activation was observed at the border of the medial frontal gyrus and the anterior cingulate gyrus (BA 6/32).</p>
        </sec>
        <sec>
          <label>3.2.2</label>
          <title>Deaf vs. hearing</title>
          <p>Deaf native signers displayed significantly greater activation in left and right superior temporal cortices than hearing non-signers. In the left hemisphere, the cluster of activation (116 voxels) was focused at the border between the posterior superior temporal gyrus (i.e., planum temporale) and the transverse temporal (i.e., Heschl's) gyrus (BA 42/41; <italic>x</italic> = −54, <italic>y</italic> = −22, <italic>z</italic> = 10). In the right hemisphere, the cluster (61 voxels) was focused at the border between the superior and middle temporal gyri (BA 22/21; <italic>x</italic> = 51 <italic>y</italic> = −7 <italic>z</italic> = −3). Hearing non-signers showed greater activation than deaf signers in the right prefrontal cortex (128 voxels, focused in BA 44; <italic>x</italic> = 40, <italic>y</italic> = 11, <italic>z</italic> = 26).</p>
          <p>When speechreading performance, as indicated by individual TAS <italic>z</italic>-score, was entered as a covariate into this analysis, deaf participants displayed greater activation than hearing participants in the left temporal cortex. The cluster of activation (120 voxels) was focused at the border between the posterior superior temporal gyrus (i.e., planum temporale) and the transverse temporal (i.e., Heschl's) gyrus (BA 42/41; <italic>x</italic> = −54, <italic>y</italic> = −22, <italic>z</italic> = 10). The focus of this cluster was verified using probabilistic maps provided by <xref rid="bib31" ref-type="bibr">Penhune, Zatorre, MacDonald, and Evans (1996)</xref> (25–50% probability of Heschl's gyrus) and <xref rid="bib38" ref-type="bibr">Westbury, Zatorre, and Evans (1999)</xref> (26–45% probability of planum temporale). Based on these probability maps, 15 voxels within this cluster, displayed ≥50% probability of being located in Heschl's gyrus, and five voxels showed ≥46% probability of being in planum temporale. This cluster also extended into the posterior lateral portion of the superior temporal gyrus (BA 22) and the middle and posterior portions of the superior temporal sulcus and middle temporal gyrus (BA 21; see <xref rid="fig2" ref-type="fig">Fig. 2</xref>). No brain regions were significantly more active in hearing than deaf participants when speechreading was a covariate in the analysis.</p>
        </sec>
        <sec>
          <label>3.2.3</label>
          <title>Cortical activation for speechreading: correlations with speechreading skill</title>
          <p>Speechreading skill, as measured by performance on the Test of Adult Speechreading (TAS), varied considerably across participants (<xref rid="tbl1" ref-type="table">Table 1</xref>). Several brain regions were significantly positively associated with TAS <italic>z</italic>-scores in both deaf and hearing groups.</p>
        </sec>
        <sec>
          <label>3.2.4</label>
          <title>Deaf group</title>
          <p>In the deaf group, ten clusters of activation (≥5 voxels) were positively associated with speechreading skill (see <xref rid="tbl3" ref-type="table">Table 3</xref>). In the temporal lobe, clusters in the superior temporal cortex were focused in the lateral portion of the transverse temporal gyrus (BA 41) in the right hemisphere, and in the superior temporal gyrus (BA 42) in the left hemisphere. However, although the <xref rid="bib36" ref-type="bibr">Talairach and Tournoux (1988)</xref> atlas suggests that this cluster incorporates the transverse temporal gyrus, the probability map of this region provided by <xref rid="bib31" ref-type="bibr">Penhune et al. (1996)</xref> suggests otherwise. In fact, only one voxel (in the left hemisphere cluster) displayed a ≥50% probability of being located in this region (<xref rid="bib31" ref-type="bibr">Penhune et al., 1996</xref>). Both clusters extended to include the posterior superior temporal gyrus (BAs 42, 22). Additional areas showing significant correlation included the middle portion of the right middle temporal gyrus (BA 21). In the frontal cortex, correlations were observed in the middle frontal gyri of both hemispheres (BA 6). In the right hemisphere, correlations were also observed in the dorsolateral prefrontal cortex (BA 46), precentral gyrus (BA 6/4) and in the anterior insula. Additional correlations were observed in the anterior cingulate gyrus (BA 32/24) and the cerebellum.</p>
        </sec>
        <sec>
          <label>3.2.5</label>
          <title>Hearing group</title>
          <p>In the hearing group, clusters of activation that were positively correlated with TAS <italic>z</italic>-scores included the fusiform (BA 37) and lingual (BA 18) gyri of the right hemisphere and the right postcentral gyrus (BA 4). Additional positive correlations were observed in the posterior cingulate gyrus (BA 23).</p>
        </sec>
      </sec>
    </sec>
    <sec>
      <label>4</label>
      <title>Discussion</title>
      <p>Deaf participants were better speechreaders than hearing participants, both in terms of their TAS performance (<xref rid="tbl1" ref-type="table">Table 1</xref>) and, when tested post-scan at identifying the words presented in the scanner. The finding that deaf people can be better speechreaders than hearing individuals is not new (<xref rid="bib4 bib27" ref-type="bibr">Bernstein et al., 2000; Mohammed et al., 2006</xref>). Deaf people, including deaf people who use a signed language, rely on speechreading, whether hearing-aid supported or un-aided, to communicate in the wider hearing community. In contrast, in hearing people, where the auditory channel dominates for speech identification, reliance on silent seen speech is generally unfamiliar and unpractised. In the present study most participants, whether deaf or hearing, could speechread much of the spoken material, and it can be assumed, therefore, that some of what they were shown in the scanner was lexically processed—albeit more in deaf than in hearing participants. Interpretation of the imaging data must bear these considerations in mind. Covariance and correlational analyses allow the behavioural and neuroimaging results to be aligned.</p>
      <p>The group-level analyses, conducted separately for the deaf and hearing groups, contrasted silent speechreading with a low-level target detection task. As such, these analyses cannot allow unambiguous interpretation of the specificity of such activation in relation to speechreading alone, but they do suggest a general pattern against which the group differences can be explored. In hearing people, the pattern of activation replicates that which has been observed in many previous studies, showing extensive activation across the temporal cortex. While some of this activation must relate to visual movement detection and to the perception of biological motion, especially in posterior and inferior regions (see, for example, <xref rid="bib39" ref-type="bibr">Zeki et al., 1991</xref>), it is likely that much of the activation in superior temporal regions relates to speechreading, since several studies contrasting speechreading with a higher-level baseline, such as observing non-speech-like mouth movements, report enhanced activation in this region (e.g., <xref rid="bib11 bib29" ref-type="bibr">Calvert et al., 1997; Paulesu et al., 2003</xref>). The present study found that, in both hearing and deaf participants, activation associated with speechreading words included the dorsal surface of the superior temporal cortex including the junction of the superior temporal gyrus and the lateral portion of the transverse temporal (Heschl's) gyrus (BA 42/41). Spatial smoothing intrinsic to transforming data into standard brain space may limit the spatial resolution in this study. Thus the finding that activation for silent speechreading included the lateral portion of Heschl's gyrus must be interpreted with caution. Nevertheless, this finding is consistent with previous neuroimaging research that delineated this region on individual brains (<xref rid="bib30" ref-type="bibr">Pekkola et al., 2005</xref>). In addition, left inferior frontal regions were activated when observing speech silently. This has also been observed where the contrasts were with higher-level conditions such as watching non-vocal mouth actions (<xref rid="bib6 bib13 bib29 bib37" ref-type="bibr">Buccino et al., 2004; Campbell et al., 2001; Paulesu et al., 2003; Watkins et al., 2003</xref>) and may reflect the operation of mirror neuron systems in the observation of speech actions.</p>
      <p>The finding of superior temporal activation for speechreading in deaf people extends earlier studies exploring the neural organisation of processing a variety of oral gestures in hearing people. This pattern of superior temporal activation found in the present study is consistent with the findings recently reported by <xref rid="bib34" ref-type="bibr">Sadato et al. (2005)</xref>, who presented deaf participants with simple segmental utterances including vowel-like lip shapes. At first sight, the present results do not fit with those we have previously reported using a closed stimulus set, covert articulation and a gurning control condition conducted with a small group of deaf people (<xref rid="bib25 bib24" ref-type="bibr">MacSweeney et al., 2001; MacSweeney et al., 2002</xref>). However, we did report activation within right superior temporal regions, when analysis combining the data from two experiments allowed for an increase in power (<xref rid="bib24" ref-type="bibr">MacSweeney et al., 2002</xref>). A further study involving a larger group of deaf participants, and manipulating task, baseline condition and stimuli, will help establish whether our previous studies simply lacked power or whether task and stimulus factors systematically affect the extent to which superior temporal regions are recruited during silent speechreading in those born profoundly deaf.</p>
      <sec>
        <label>4.1</label>
        <title>Deaf vs. hearing</title>
        <p>When hearing non-signers were compared with deaf signers, and speechreading skill (which differed between the groups) was entered as a covariate (<xref rid="fig2" ref-type="fig">Fig. 2</xref>) greater activation was observed for the deaf than hearing group in left middle-posterior superior temporal regions. This cluster of activation was focused at the border between the posterior and transverse temporal gyri (BA 42/41) and extended to the middle and posterior portions of the superior temporal gyrus and sulcus, and middle temporal gyrus. No regions showed greater activation in hearing than deaf participants. In hearing people, the role of the posterior superior temporal sulcus (p-STS) has been proposed as a key ‘binding site’, responsible for cross- and supra-modal processing of co-incident auditory and visual streams in audiovisual speech processing (<xref rid="bib10 bib12" ref-type="bibr">Calvert et al., 1999; Calvert et al., 2000</xref>). However, in deaf people, p-STS cannot play this role, since the association between seen and heard speech in deaf people is variable and relatively unsystematic. In the present study, not only was activation in this region observed in the absence of audition; it was <italic>greater</italic> in deaf than hearing people. One possibility is that activation by seen speech in p-STS is sensitive to the dominant speech modality within this multimodal region. That is, activation by silent speech in this region may be greater in deaf people because the region has developed to be sensitive to visual speech, while for hearing people it has developed to be sensitive to auditory speech characteristics, with visual speech as a secondary function. Structural imaging of the connections between p-STS and visual and auditory cortices in deaf and hearing individuals could be employed to test this hypothesis.</p>
        <p>A non-mutually exclusive possibility is that greater activation in superior temporal regions for deaf than hearing individuals reflects a more general plasticity of these regions in deaf people. Several studies suggest that brain regions considered specialised for audition can be recruited for processing stimuli from other modalities in deaf people (e.g., <xref rid="bib17" ref-type="bibr">Fine, Finney, Boynton, &amp; Dobkins, 2005</xref>; <xref rid="bib18" ref-type="bibr">Finney, Fine, &amp; Dobkins, 2001</xref>; <xref rid="bib34" ref-type="bibr">Sadato et al., 2005</xref>). While the extent and specificity of primary auditory cortex recruitment by visual events remains unclear (<xref rid="bib1" ref-type="bibr">Bavelier, Dye, &amp; Hauser, 2006</xref>), some studies (e.g., <xref rid="bib26" ref-type="bibr">MacSweeney et al., 2004</xref>) suggest that perception of signed language, and even of non-linguistic biological movement, can recruit regions within superior temporal cortex to a greater extent in deaf native signers than in hearing people exposed to a signed language from birth (hearing native signers).</p>
      </sec>
      <sec>
        <label>4.2</label>
        <title>Correlations of activation with individual differences in speechreading skill</title>
        <p>TAS speechreading scores and post-scan speechreading of the items seen in the scanner were positively correlated (deaf: <italic>r</italic> = 0.476, <italic>p</italic>(1-tailed) = 0.05; hearing: <italic>r</italic> = 0.673, <italic>p</italic>(1-tailed) = 0.034); thus we can infer that the higher the TAS score, the more likely it is that participants would have processed the speechread material lexically. However, TAS scores were not normally distributed across the two groups. For this reason, standard scores (TAS-<italic>z</italic>) derived for each group formed the basis for exploring the relationship between speechreading skill and cortical activation. Within each group, different patterns of association were observed. In deaf participants, the correlational analyses showed that activation in the posterior portion of the superior temporal gyri (as well as middle temporal and middle frontal gyri) was positively associated with speechreading.</p>
        <p>In the hearing participants, who were less able and more varied speechreaders than the deaf participants, speechreading skill was positively associated with activation in the right lingual and posterior cingulate gyri, which is consistent with findings from <xref rid="bib21" ref-type="bibr">Hall et al. (2005)</xref>. Additional activations displaying a positive correlation with speechreading skill included the right postcentral and inferior temporal (fusiform) gyri, perhaps suggesting relatively greater involvement of articulatory skill and face processing in hearing individuals’ speechreading, respectively.</p>
        <p>Taken together, these data show that hearing status is an important determinant of activation in left superior temporal regions when words are speechread. In particular, silent speechreading elicits greater activation in the left middle and posterior portions of the superior temporal cortex, including the superior and middle temporal gyri and the lateral portion of the transverse temporal gyrus in deaf than hearing people, even when speechreading skill is held constant. However, speechreading skill can moderate this activation, showing a positive relationship in deaf but not hearing participants. The relatively small group sizes used in the correlational analysis (<italic>n</italic> = 13 in each group), however, require that this interpretation should be provisional. <xref rid="bib21" ref-type="bibr">Hall et al. (2005)</xref> did not find reliable activation in superior temporal gyrus for silent speechreading in contrast to viewing facial gurning in a group of 33 hearing participants, who also varied widely in speechreading skill. However, they did report a reliable positive correlation between speechreading skill and activation in this region. The inference from that study together with the present one must be that, when speechread material is linguistically processed, superior temporal regions within the left hemisphere are likely to be recruited. Additionally, the present study shows that it was deaf rather than hearing people who showed this relationship most clearly, and where individual differences in speechreading skill made an additional impact, despite the range of speechreading skill being larger in the hearing than the deaf group.</p>
        <p>We have shown that, when auditory regions are not activated by acoustic stimulation, they can nevertheless be activated by silent speech in the form of speechreading. This finding may have some practical as well as theoretical significance. Current practice in relation to speech training for prelingually deaf children preparing for cochlear implantation emphasises acoustic processing. In auditory-verbal training, the speaking model is required to hide her or his lips with the aim of training the child's acoustic skills (e.g., <xref rid="bib14" ref-type="bibr">Chan, Chan, Kwok, &amp; Yu, 2000</xref>; <xref rid="bib32" ref-type="bibr">Rhoades &amp; Chisholm, 2000</xref>). Thus, a neurological hypothesis is being advanced which suggests that the deaf child should not watch spoken (or signed) language since this may adversely affect the sensitivity of auditory brain regions to acoustic activation following cochlear implantation. Such advice may not be warranted if speechreading activates auditory regions in both deaf and hearing individuals.</p>
        <p>Speechreading gives access to spoken language structure by eye. It therefore has the potential to impact positively on the development of auditory speech processing following cochlear implantation. While there are few consistent correlates of improved post-implant speech processing in prelingually deaf cochlear implantees, efficiency in speechreading is implicated. For example, pre-implant silent speechreading skills are positively associated with general speech and language outcomes (<xref rid="bib2" ref-type="bibr">Bergeson, Pisoni, &amp; Davis, 2005</xref>). The possibility that superior temporal regions in deaf individuals, once tuned to visible speech, may then more readily adapt to perceiving speech multimodally should be seriously considered when recommendations concerning pediatric cochlear implantation procedures are being developed.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgements</title>
      <p>This research was supported by the Wellcome Trust (Project Grant 068607/Z/02/Z ‘Imaging the Deaf Brain’). The Wellcome Trust also supports M.M. (Career Development Fellowship). R.C. and B.W. are further supported by Grant RES 620-28-6001 from the ESRC (Deafness, Cognition and Language (DCAL) Research Centre). We are grateful to the following people for their assistance: Jordan Fenlon, Tyron Woolfe, Marc Seal, Cathie Green, Maartje Kouwenberg, Zoë Hunter and Karine Gazarian.</p>
    </ack>
    <fn-group>
      <fn id="fn1">
        <label>1</label>
        <p>The modification of Friman et al. reflects the fact that, when combinations of gamma functions are used to model BOLD responses, some of the combinations that are possible mathematically are unlikely physiologically (i.e., they give rise to BOLD response shapes that are not observed in the brain). After analysing many thousands of BOLD curve shapes, Friman et al.'s solution was to suggest ranges of parameter combinations that were physiologically reasonable. In mathematical terms this consists of replacing the original gamma functions (<italic>y</italic>1(<italic>t</italic>) and <italic>y</italic>2(<italic>t</italic>)) by combinations (<italic>y</italic>1(<italic>t</italic>) + <italic>ay</italic>2(<italic>t</italic>) and <italic>y</italic>1(<italic>t</italic>) − <italic>ay</italic>2(<italic>t</italic>)) where <italic>a</italic> is a constant (0.3) derived from analysis of observed curve shapes.</p>
      </fn>
      <fn id="fn2">
        <label>2</label>
        <p>The extent of activation in the left transverse temporal (i.e., Heschl's) gyrus for the speechreading condition as compared to the baseline condition was verified using the probability map provided by <xref rid="bib31" ref-type="bibr">Penhune et al. (1996)</xref>. Thirty-one voxels in the deaf group and four voxels in hearing group displayed ≥50% probability of being located within this region.</p>
      </fn>
    </fn-group>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <label>Bavelier et al., 2006</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bavelier</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Dye</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Hauser</surname>
              <given-names>P.C.</given-names>
            </name>
          </person-group>
          <article-title>Do deaf individuals see better?</article-title>
          <source>Trends in Cognitive Science</source>
          <year>2006</year>
          <volume>10</volume>
          <fpage>512</fpage>
          <lpage>518</lpage>
        </citation>
      </ref>
      <ref id="bib2">
        <label>Bergeson et al., 2005</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bergeson</surname>
              <given-names>T.R.</given-names>
            </name>
            <name>
              <surname>Pisoni</surname>
              <given-names>D.B.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>R.A.</given-names>
            </name>
          </person-group>
          <article-title>Development of audiovisual comprehension skills in prelingually deaf children with cochlear implants</article-title>
          <source>Ear and Hearing</source>
          <year>2005</year>
          <volume>26</volume>
          <fpage>149</fpage>
          <lpage>164</lpage>
          <pub-id pub-id-type="pmid">15809542</pub-id>
        </citation>
      </ref>
      <ref id="bib3">
        <label>Bernstein et al., 2002</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bernstein</surname>
              <given-names>L.E.</given-names>
            </name>
            <name>
              <surname>Auer</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Moore</surname>
              <given-names>J.K.</given-names>
            </name>
            <name>
              <surname>Ponton</surname>
              <given-names>C.W.</given-names>
            </name>
            <name>
              <surname>Don</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Visual speech perception without primary auditory cortex activation</article-title>
          <source>NeuroReport</source>
          <year>2002</year>
          <volume>13</volume>
          <fpage>311</fpage>
          <lpage>315</lpage>
          <pub-id pub-id-type="pmid">11930129</pub-id>
        </citation>
      </ref>
      <ref id="bib4">
        <label>Bernstein et al., 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bernstein</surname>
              <given-names>L.E.</given-names>
            </name>
            <name>
              <surname>Demorest</surname>
              <given-names>M.E.</given-names>
            </name>
            <name>
              <surname>Tucker</surname>
              <given-names>P.E.</given-names>
            </name>
          </person-group>
          <article-title>Speech perception without hearing</article-title>
          <source>Perception &amp; Psychophysics</source>
          <year>2000</year>
          <volume>62</volume>
          <fpage>233</fpage>
          <lpage>252</lpage>
          <pub-id pub-id-type="pmid">10723205</pub-id>
        </citation>
      </ref>
      <ref id="bib5">
        <label>Brammer et al., 1997</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brammer</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Simmons</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Grasby</surname>
              <given-names>P.M.</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Generic brain activation mapping in functional magnetic resonance imaging: A nonparametric approach</article-title>
          <source>Magnetic Resonance Imaging</source>
          <year>1997</year>
          <volume>15</volume>
          <fpage>763</fpage>
          <lpage>770</lpage>
          <pub-id pub-id-type="pmid">9309607</pub-id>
        </citation>
      </ref>
      <ref id="bib6">
        <label>Buccino et al., 2004</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buccino</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Lui</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Canessa</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Patteri</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Lagravinese</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Benuzzi</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Neural circuits involved in the recognition of actions performed by nonconspecifics: An fMRI study</article-title>
          <source>Journal of Cognitive Neuroscience</source>
          <year>2004</year>
          <volume>16</volume>
          <fpage>114</fpage>
          <lpage>126</lpage>
          <pub-id pub-id-type="pmid">15006041</pub-id>
        </citation>
      </ref>
      <ref id="bib7">
        <label>Bullmore et al., 1996</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Rabe-Hesketh</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Janot</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Statistical methods of estimation and inference for functional MR image analysis</article-title>
          <source>Magnetic Resonance in Medicine</source>
          <year>1996</year>
          <volume>35</volume>
          <fpage>261</fpage>
          <lpage>277</lpage>
          <pub-id pub-id-type="pmid">8622592</pub-id>
        </citation>
      </ref>
      <ref id="bib8">
        <label>Bullmore et al., 2001</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Long</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Suckling</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fadili</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Zelaya</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Colored noise and computational inference in neurophysiological (fMRI) time series analysis: Resampling methods in time and wavelet domains</article-title>
          <source>Human Brain Mapping</source>
          <year>2001</year>
          <volume>12</volume>
          <fpage>61</fpage>
          <lpage>78</lpage>
          <pub-id pub-id-type="pmid">11169871</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <label>Bullmore et al., 1999</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Suckling</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Overmeyer</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rabe-Hesketh</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Taylor</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Global, voxel, and cluster tests, by theory and permutation, for a difference between two groups of structural MR images of the brain</article-title>
          <source>IEEE Transactions on Medical Imaging</source>
          <year>1999</year>
          <volume>18</volume>
          <fpage>32</fpage>
          <lpage>42</lpage>
          <pub-id pub-id-type="pmid">10193695</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <label>Calvert et al., 1999</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Iversen</surname>
              <given-names>S.D.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
          </person-group>
          <article-title>Response amplification in sensory-specific cortices during crossmodal binding</article-title>
          <source>NeuroReport</source>
          <year>1999</year>
          <volume>10</volume>
          <fpage>2619</fpage>
          <lpage>2623</lpage>
          <pub-id pub-id-type="pmid">10574380</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <label>Calvert et al., 1997</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Bullmore</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.C.R.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.K.</given-names>
            </name>
          </person-group>
          <article-title>Activation of auditory cortex during silent lipreading</article-title>
          <source>Science</source>
          <year>1997</year>
          <volume>276</volume>
          <fpage>593</fpage>
          <lpage>596</lpage>
          <pub-id pub-id-type="pmid">9110978</pub-id>
        </citation>
      </ref>
      <ref id="bib12">
        <label>Calvert et al., 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Brammer</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex</article-title>
          <source>Current Biology</source>
          <year>2000</year>
          <volume>10</volume>
          <fpage>649</fpage>
          <lpage>657</lpage>
          <pub-id pub-id-type="pmid">10837246</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <label>Campbell et al., 2001</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Surguladze</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Suckling</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Cortical substrates for the perception of face actions: An fMRI study of the specificity of activation for seen speech and for meaningless lower-face acts (gurning)</article-title>
          <source>Brain Research. Cognitive Brain Research</source>
          <year>2001</year>
          <volume>12</volume>
          <fpage>233</fpage>
          <lpage>243</lpage>
          <pub-id pub-id-type="pmid">11587893</pub-id>
        </citation>
      </ref>
      <ref id="bib14">
        <label>Chan et al., 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Kwok</surname>
              <given-names>I.C.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>H.C.</given-names>
            </name>
          </person-group>
          <article-title>The speech and language rehabilitation program for pediatric cochlear implantees in Hong Kong</article-title>
          <source>Advances in Oto-Rhino-Laryngology</source>
          <year>2000</year>
          <volume>57</volume>
          <fpage>247</fpage>
          <lpage>249</lpage>
          <pub-id pub-id-type="pmid">11892159</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <label>Donoho and Johnstone, 1994</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Donoho</surname>
              <given-names>D.L.</given-names>
            </name>
            <name>
              <surname>Johnstone</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>Ideal spatial adaptation by wavelet shrinkage</article-title>
          <source>Biometrika</source>
          <year>1994</year>
          <volume>81</volume>
          <fpage>425</fpage>
          <lpage>455</lpage>
        </citation>
      </ref>
      <ref id="bib16">
        <label>Edgington, 1995</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Edgington</surname>
              <given-names>E.S.</given-names>
            </name>
          </person-group>
          <edition>third ed.</edition>
          <year>1995</year>
          <publisher-name>Marcel Dekker, Inc.</publisher-name>
          <publisher-loc>New York/Basel</publisher-loc>
        </citation>
      </ref>
      <ref id="bib17">
        <label>Fine et al., 2005</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fine</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Finney</surname>
              <given-names>E.M.</given-names>
            </name>
            <name>
              <surname>Boynton</surname>
              <given-names>G.M.</given-names>
            </name>
            <name>
              <surname>Dobkins</surname>
              <given-names>K.R.</given-names>
            </name>
          </person-group>
          <article-title>Comparing the effects of auditory deprivation and sign language within the auditory and visual cortex</article-title>
          <source>Journal of Cognitive Neuroscience</source>
          <year>2005</year>
          <volume>17</volume>
          <fpage>1621</fpage>
          <lpage>1637</lpage>
          <pub-id pub-id-type="pmid">16269101</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <label>Finney et al., 2001</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Finney</surname>
              <given-names>E.M.</given-names>
            </name>
            <name>
              <surname>Fine</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Dobkins</surname>
              <given-names>K.R.</given-names>
            </name>
          </person-group>
          <article-title>Visual stimuli activate auditory cortex in the deaf</article-title>
          <source>Nature Neuroscience</source>
          <year>2001</year>
          <volume>4</volume>
          <fpage>1171</fpage>
          <lpage>1173</lpage>
        </citation>
      </ref>
      <ref id="bib19">
        <label>Friman et al., 2003</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friman</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Borga</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lundberg</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Knutsson</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Adaptive analysis of fMRI data</article-title>
          <source>Neuroimage</source>
          <year>2003</year>
          <volume>19</volume>
          <fpage>837</fpage>
          <lpage>845</lpage>
          <pub-id pub-id-type="pmid">12880812</pub-id>
        </citation>
      </ref>
      <ref id="bib20">
        <label>Friston et al., 1998</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Josephs</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Turner</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Nonlinear event-related responses in fMRI</article-title>
          <source>Magnetic Resonance in Medicine</source>
          <year>1998</year>
          <volume>39</volume>
          <fpage>41</fpage>
          <lpage>52</lpage>
          <pub-id pub-id-type="pmid">9438436</pub-id>
        </citation>
      </ref>
      <ref id="bib21">
        <label>Hall et al., 2005</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Fussell</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
          </person-group>
          <article-title>Reading fluent speech from talking faces: Typical brain networks and individual differences</article-title>
          <source>Journal of Cognitive Neuroscience</source>
          <year>2005</year>
          <volume>17</volume>
          <fpage>939</fpage>
          <lpage>953</lpage>
          <pub-id pub-id-type="pmid">15969911</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <label>Ludman et al., 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ludman</surname>
              <given-names>C.N.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
            <name>
              <surname>Hall</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Elliott</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Foster</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hykin</surname>
              <given-names>J.L.</given-names>
            </name>
          </person-group>
          <article-title>Lip-reading ability and patterns of cortical activation studied using fMRI</article-title>
          <source>British Journal of Audiology</source>
          <year>2000</year>
          <volume>34</volume>
          <fpage>225</fpage>
          <lpage>230</lpage>
          <pub-id pub-id-type="pmid">10997451</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <label>MacSweeney et al., 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Amaro</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Silent speechreading in the absence of scanner noise: An event-related fMRI study</article-title>
          <source>NeuroReport</source>
          <year>2000</year>
          <volume>11</volume>
          <fpage>1729</fpage>
          <lpage>1733</lpage>
          <pub-id pub-id-type="pmid">10852233</pub-id>
        </citation>
      </ref>
      <ref id="bib24">
        <label>MacSweeney et al., 2002</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.K.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>S.C.</given-names>
            </name>
          </person-group>
          <article-title>Speechreading circuits in people born deaf</article-title>
          <source>Neuropsychologia</source>
          <year>2002</year>
          <volume>40</volume>
          <fpage>801</fpage>
          <lpage>807</lpage>
          <pub-id pub-id-type="pmid">11900730</pub-id>
        </citation>
      </ref>
      <ref id="bib25">
        <label>MacSweeney et al., 2001</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Calvert</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.K.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>Suckling</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Dispersed activation in the left temporal cortex for speechreading in congenitally deaf speechreaders</article-title>
          <source>Proceedings of the Royal Society of London B</source>
          <year>2001</year>
          <volume>268</volume>
          <fpage>451</fpage>
          <lpage>457</lpage>
        </citation>
      </ref>
      <ref id="bib26">
        <label>MacSweeney et al., 2004</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Woll</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Giampietro</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>A.S.</given-names>
            </name>
            <name>
              <surname>McGuire</surname>
              <given-names>P.K.</given-names>
            </name>
          </person-group>
          <article-title>Dissociating linguistic and nonlinguistic gestural communication in the brain</article-title>
          <source>Neuroimage</source>
          <year>2004</year>
          <volume>22</volume>
          <fpage>1605</fpage>
          <lpage>1618</lpage>
          <pub-id pub-id-type="pmid">15275917</pub-id>
        </citation>
      </ref>
      <ref id="bib27">
        <label>Mohammed et al., 2006</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mohammed</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Campbell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>MacSweeney</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Barry</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Coleman</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Speechreading and its association with reading among deaf, hearing and dyslexic individuals</article-title>
          <source>Clinical Linguistics &amp; Phonetics</source>
          <year>2006</year>
          <volume>20</volume>
          <fpage>621</fpage>
          <lpage>630</lpage>
          <pub-id pub-id-type="pmid">17056494</pub-id>
        </citation>
      </ref>
      <ref id="bib28">
        <label>Nishitani and Hari, 2002</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nishitani</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Hari</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Viewing lip forms: Cortical dynamics</article-title>
          <source>Neuron</source>
          <year>2002</year>
          <volume>36</volume>
          <fpage>1211</fpage>
          <lpage>1220</lpage>
          <pub-id pub-id-type="pmid">12495633</pub-id>
        </citation>
      </ref>
      <ref id="bib29">
        <label>Paulesu et al., 2003</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Paulesu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Perani</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Blasi</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Silani</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Borghese</surname>
              <given-names>N.A.</given-names>
            </name>
            <name>
              <surname>De Giovanni</surname>
              <given-names>U.</given-names>
            </name>
          </person-group>
          <article-title>A functional-anatomical model for lipreading</article-title>
          <source>Journal of Neurophysiology</source>
          <year>2003</year>
          <volume>90</volume>
          <fpage>2005</fpage>
          <lpage>2013</lpage>
          <pub-id pub-id-type="pmid">12750414</pub-id>
        </citation>
      </ref>
      <ref id="bib30">
        <label>Pekkola et al., 2005</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pekkola</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ojanen</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Autti</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Jaaskelainen</surname>
              <given-names>I.P.</given-names>
            </name>
            <name>
              <surname>Mottonen</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tarkiainen</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Primary auditory cortex activation by visual speech: an fMRI study at 3 T</article-title>
          <source>NeuroReport</source>
          <year>2005</year>
          <volume>16</volume>
          <fpage>125</fpage>
          <lpage>128</lpage>
          <pub-id pub-id-type="pmid">15671860</pub-id>
        </citation>
      </ref>
      <ref id="bib31">
        <label>Penhune et al., 1996</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penhune</surname>
              <given-names>V.B.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>MacDonald</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>Interhemispheric anatomical differences in human primary auditory cortex: Probabilistic mapping and volume measurement from magnetic resonance scans</article-title>
          <source>Cerebral Cortex</source>
          <year>1996</year>
          <volume>6</volume>
          <fpage>661</fpage>
          <lpage>672</lpage>
          <pub-id pub-id-type="pmid">8921202</pub-id>
        </citation>
      </ref>
      <ref id="bib32">
        <label>Rhoades and Chisholm, 2000</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rhoades</surname>
              <given-names>E.A.</given-names>
            </name>
            <name>
              <surname>Chisholm</surname>
              <given-names>T.H.</given-names>
            </name>
          </person-group>
          <article-title>Global language progress with an auditory-verbal approach for children who are deaf or hard of hearing</article-title>
          <source>Volta Review</source>
          <year>2000</year>
          <volume>102</volume>
          <fpage>5</fpage>
          <lpage>24</lpage>
        </citation>
      </ref>
      <ref id="bib33">
        <label>Ruytjens et al., 2006</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ruytjens</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Albers</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>van Dijk</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wit</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Willemsen</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Neural responses to silent lipreading in normal hearing male and female subjects</article-title>
          <source>The European Journal of Neuroscience</source>
          <year>2006</year>
          <volume>24</volume>
          <fpage>1835</fpage>
          <lpage>1844</lpage>
          <pub-id pub-id-type="pmid">17004947</pub-id>
        </citation>
      </ref>
      <ref id="bib34">
        <label>Sadato et al., 2005</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sadato</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Okada</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Honda</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Matsuki</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yoshida</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kashikura</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Cross-modal integration and plastic changes revealed by lip movement, random-dot motion and sign languages in the hearing and deaf</article-title>
          <source>Cerebral Cortex</source>
          <year>2005</year>
          <volume>15</volume>
          <fpage>1113</fpage>
          <lpage>1122</lpage>
          <pub-id pub-id-type="pmid">15563723</pub-id>
        </citation>
      </ref>
      <ref id="bib35">
        <label>Scott and Johnsrude, 2003</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.S.</given-names>
            </name>
          </person-group>
          <article-title>The neuroanatomical and functional organization of speech perception</article-title>
          <source>Trends in Neuroscience</source>
          <year>2003</year>
          <volume>26</volume>
          <fpage>100</fpage>
          <lpage>107</lpage>
        </citation>
      </ref>
      <ref id="bib36">
        <label>Talairach and Tournoux, 1988</label>
        <citation citation-type="other">Talairach, J., &amp; Tournoux, P. (1988). <italic>Co-planar stereotaxic atlas of the human brain</italic> (M. Rayport, Trans.). New York: Thieme Medical Publishers, Inc.</citation>
      </ref>
      <ref id="bib37">
        <label>Watkins et al., 2003</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watkins</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Strafella</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Paus</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Seeing and hearing speech excites the motor system involved in speech production</article-title>
          <source>Neuropsychologia</source>
          <year>2003</year>
          <volume>41</volume>
          <fpage>989</fpage>
          <lpage>994</lpage>
          <pub-id pub-id-type="pmid">12667534</pub-id>
        </citation>
      </ref>
      <ref id="bib38">
        <label>Westbury et al., 1999</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Westbury</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>Quantifying variability in the planum temporale: A probability map</article-title>
          <source>Cerebral Cortex</source>
          <year>1999</year>
          <volume>9</volume>
          <fpage>392</fpage>
          <lpage>405</lpage>
          <pub-id pub-id-type="pmid">10426418</pub-id>
        </citation>
      </ref>
      <ref id="bib39">
        <label>Zeki et al., 1991</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zeki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Watson</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Lueck</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Kennard</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
          </person-group>
          <article-title>A direct demonstration of functional specialization in human visual cortex</article-title>
          <source>Journal of Neuroscience</source>
          <year>1991</year>
          <volume>11</volume>
          <fpage>641</fpage>
          <lpage>649</lpage>
          <pub-id pub-id-type="pmid">2002358</pub-id>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Activation for speechreading relative to the baseline task for each group. (A) Deaf group (top) and (B) hearing group (bottom); 13 participants per group; voxel-wise <italic>p</italic>-value = 0.05, cluster-wise <italic>p</italic>-value = 0.0025. Activations on lateral renderings are displayed up to 15 mm beneath the cortical surface. Five sequential axial sections, showing activation in superior temporal regions, including the planum temporale (PT) and Heschl's gyrus (HG) are also displayed.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Activation for the speechreading condition as a function of hearing status. The region indicated was more active for deaf than hearing participants, when performance on the TAS (<italic>z</italic>-score) was entered as a covariate (voxel-wise <italic>p</italic>-value = 0.05, cluster-wise <italic>p</italic>-value = 0.01). No regions were more active for hearing than deaf participants. No right hemisphere regions were significantly different across the groups. Five sequential axial sections, showing activation in superior temporal regions, including the planum temporale (PT) and Heschl's gyrus (HG) are also displayed.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <table-wrap position="float" id="tbl1">
      <label>Table 1</label>
      <caption>
        <p>Participant characteristics: mean (S.D.) of age (in years), non-verbal IQ centile and speechreading (TAS) raw and <italic>z</italic>-scores</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Age</th>
            <th align="left">NVIQ centile</th>
            <th align="left">TAS</th>
            <th align="left">TAS <italic>z</italic>-score</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">Deaf (<italic>n</italic> = 13, 6 female)</td>
            <td align="left">27.4 (7.76) range: 18–49</td>
            <td align="left">88.2 (13.3) range: 50–98</td>
            <td align="left">32.54 (3.07) range: 27–37</td>
            <td align="left">0.436 (0.59) range: −0.62–1.29</td>
          </tr>
          <tr>
            <td align="left">Hearing (<italic>n</italic> = 13, 6 female)</td>
            <td align="left">29.4 (6.15) range: 18–43</td>
            <td align="left">83.2 (19.6) range: 25–99</td>
            <td align="left">25.08 (4.89) range: 15–34</td>
            <td align="left">0.183 (1.07) range: −2.02–2.14</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="tbl2">
      <label>Table 2</label>
      <caption>
        <p>Activated regions for the perception of speech compared to baseline (static model) in deaf and hearing participants</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Hemisphere</th>
            <th align="left">Size (voxels)</th>
            <th align="left"><italic>x</italic>, <italic>y</italic>, <italic>z</italic></th>
            <th align="left">BA</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="5" align="left">Deaf group</td>
          </tr>
          <tr>
            <td align="left"> Superior/middle temporal gyrus</td>
            <td align="left">R</td>
            <td align="left">246</td>
            <td align="left">51, −7, −3</td>
            <td align="left">22/21</td>
          </tr>
          <tr>
            <td align="left"> Superior/transverse temporal gyrus</td>
            <td align="left">L</td>
            <td align="left">916</td>
            <td align="left">−54, −22, 10</td>
            <td align="left">42/41</td>
          </tr>
          <tr>
            <td align="left"> Precentral gyrus</td>
            <td align="left">R</td>
            <td align="left">237</td>
            <td align="left">47, −4, 40</td>
            <td align="left">6</td>
          </tr>
          <tr>
            <td align="left"> Medial frontal gyrus/anterior cingulate gyrus</td>
            <td align="left">L</td>
            <td align="left">211</td>
            <td align="left">−4, 15, 43</td>
            <td align="left">6/32</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Hearing group</td>
          </tr>
          <tr>
            <td align="left"> Superior/middle temporal gyrus</td>
            <td align="left">R</td>
            <td align="left">451</td>
            <td align="left">43, −30, 0</td>
            <td align="left">22/21</td>
          </tr>
          <tr>
            <td align="left"> Middle temporo-occipital junction</td>
            <td align="left">L</td>
            <td align="left">493</td>
            <td align="left">−43, −63, 0</td>
            <td align="left">37</td>
          </tr>
          <tr>
            <td align="left"> Supramarginal gyrus</td>
            <td align="left">L</td>
            <td align="left">125</td>
            <td align="left">−33, −52, 43</td>
            <td align="left">40</td>
          </tr>
          <tr>
            <td align="left"> Supramarginal gyrus</td>
            <td align="left">R</td>
            <td align="left">220</td>
            <td align="left">36, −48, 40</td>
            <td align="left">40</td>
          </tr>
          <tr>
            <td align="left"> Precentral gyrus</td>
            <td align="left">L</td>
            <td align="left">457</td>
            <td align="left">−47, −7, 43</td>
            <td align="left">4/6</td>
          </tr>
          <tr>
            <td align="left"> Inferior frontal gyrus</td>
            <td align="left">R</td>
            <td align="left">521</td>
            <td align="left">40, 11, 26</td>
            <td align="left">44</td>
          </tr>
          <tr>
            <td align="left"> Medial frontal gyrus</td>
            <td align="left">R</td>
            <td align="left">218</td>
            <td align="left">4, 4, 50</td>
            <td align="left">6</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Voxel-wise <italic>p</italic>-value = 0.05, cluster-wise <italic>p</italic>-value = 0.0025. Foci correspond to the most activated voxel in each 3-D cluster.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl3">
      <label>Table 3</label>
      <caption>
        <p>Regions positively associated with speechreading skill (TAS <italic>z</italic>-scores) in deaf and hearing participants</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Hemisphere</th>
            <th align="left">Size (voxels)</th>
            <th align="left"><italic>x</italic>, <italic>y</italic>, <italic>z</italic></th>
            <th align="left">BA</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="5" align="left">Deaf group</td>
          </tr>
          <tr>
            <td align="left"> Cerebellum</td>
            <td align="left">L</td>
            <td align="char">8</td>
            <td align="left">−11, −44, −30</td>
            <td align="left">–</td>
          </tr>
          <tr>
            <td align="left"> Middle temporal gyrus</td>
            <td align="left">R</td>
            <td align="char">16</td>
            <td align="left">47, −26, −3</td>
            <td align="left">21</td>
          </tr>
          <tr>
            <td align="left"> Insula</td>
            <td align="left">R</td>
            <td align="char">6</td>
            <td align="left">36, 15, 7</td>
            <td align="left">–</td>
          </tr>
          <tr>
            <td align="left"> Transverse temporal gyrus</td>
            <td align="left">R</td>
            <td align="char">7</td>
            <td align="left">47, −19, 13</td>
            <td align="left">41</td>
          </tr>
          <tr>
            <td align="left"> Superior temporal gyrus</td>
            <td align="left">L</td>
            <td align="char">16</td>
            <td align="left">−54, −26, 13</td>
            <td align="left">42</td>
          </tr>
          <tr>
            <td align="left"> Dorsolateral prefrontal cortex</td>
            <td align="left">R</td>
            <td align="char">10</td>
            <td align="left">47, 22, 26</td>
            <td align="left">46</td>
          </tr>
          <tr>
            <td align="left"> Anterior cingulate gyrus</td>
            <td align="left">–</td>
            <td align="char">7</td>
            <td align="left">0, 15, 33</td>
            <td align="left">32/24</td>
          </tr>
          <tr>
            <td align="left"> Precentral gyrus</td>
            <td align="left">R</td>
            <td align="char">5</td>
            <td align="left">29, −7, 50</td>
            <td align="left">6/4</td>
          </tr>
          <tr>
            <td align="left"> Middle frontal gyrus</td>
            <td align="left">L</td>
            <td align="char">6</td>
            <td align="left">−29, −4, 53</td>
            <td align="left">6</td>
          </tr>
          <tr>
            <td align="left"> Middle frontal gyrus</td>
            <td align="left">R</td>
            <td align="char">6</td>
            <td align="left">33, 0, 53</td>
            <td align="left">6</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Hearing group</td>
          </tr>
          <tr>
            <td align="left"> Fusiform gyrus</td>
            <td align="left">R</td>
            <td align="char">9</td>
            <td align="left">33, −44, −13</td>
            <td align="left">37</td>
          </tr>
          <tr>
            <td align="left"> Lingual gyrus</td>
            <td align="left">R</td>
            <td align="char">8</td>
            <td align="left">11, −81, −7</td>
            <td align="left">18</td>
          </tr>
          <tr>
            <td align="left"> Posterior cingulate gyrus</td>
            <td align="left">–</td>
            <td align="char">6</td>
            <td align="left">0, −33, 23</td>
            <td align="left">23</td>
          </tr>
          <tr>
            <td align="left"> Posterior cingulate gyrus</td>
            <td align="left">L</td>
            <td align="char">5</td>
            <td align="left">−4, −11, 30</td>
            <td align="left">23</td>
          </tr>
          <tr>
            <td align="left"> Postcentral gyrus</td>
            <td align="left">R</td>
            <td align="char">11</td>
            <td align="left">51, −15, 30</td>
            <td align="left">4</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Voxel-wise <italic>p</italic>-value = 0.05, cluster-wise <italic>p</italic>-value = 0.0025. Foci correspond to the most activated voxel in each 3-D cluster.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </floats-wrap>
</article>