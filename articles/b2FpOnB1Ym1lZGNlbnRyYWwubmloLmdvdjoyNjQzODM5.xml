<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title>Neuroimage</journal-title>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2643839</article-id>
      <article-id pub-id-type="pmid">17869542</article-id>
      <article-id pub-id-type="publisher-id">YNIMG4793</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.032</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Diffusion-based spatial priors for imaging</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Harrison</surname>
            <given-names>L.M.</given-names>
          </name>
          <email>l.harrison@fil.ion.ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Penny</surname>
            <given-names>W.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>J.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Trujillo-Barreto</surname>
            <given-names>N.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>K.J.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>a</sup>The Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London, WC1N 3BG, UK</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>b</sup>Cuban Neuroscience Centre, Havana, Cuba</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. Fax: +44 207 813 1445. <email>l.harrison@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <month>12</month>
        <year>2007</year>
      </pub-date>
      <volume>38</volume>
      <issue>4-3</issue>
      <fpage>677</fpage>
      <lpage>695</lpage>
      <history>
        <date date-type="received">
          <day>25</day>
          <month>10</month>
          <year>2006</year>
        </date>
        <date date-type="rev-recd">
          <day>3</day>
          <month>7</month>
          <year>2007</year>
        </date>
        <date date-type="accepted">
          <day>16</day>
          <month>7</month>
          <year>2007</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2007 Elsevier Inc.</copyright-statement>
        <copyright-year>2007</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Abstract</title>
        <p>We describe a Bayesian scheme to analyze images, which uses spatial priors encoded by a diffusion kernel, based on a weighted graph Laplacian. This provides a general framework to formulate a spatial model, whose parameters can be optimized. The application we have in mind is a spatiotemporal model for imaging data. We illustrate the method on a random effects analysis of fMRI contrast images from multiple subjects; this simplifies exposition of the model and enables a clear description of its salient features. Typically, imaging data are smoothed using a fixed Gaussian kernel as a pre-processing step before applying a mass-univariate statistical model (e.g., a general linear model) to provide images of parameter estimates. An alternative is to include smoothness in a multivariate statistical model (Penny, W.D., Trujillo-Barreto, N.J., Friston, K.J., 2005. Bayesian fMRI time series analysis with spatial priors. <italic>Neuroimage</italic> 24, 350–362). The advantage of the latter is that each parameter field is smoothed automatically, according to a measure of uncertainty, given the data. In this work, we investigate the use of diffusion kernels to encode spatial correlations among parameter estimates. Nonlinear diffusion has a long history in image processing; in particular, flows that depend on local image geometry (Romeny, B.M.T., 1994. Geometry-driven Diffusion in Computer Vision. Kluwer Academic Publishers) can be used as adaptive filters. This can furnish a non-stationary smoothing process that preserves features, which would otherwise be lost with a fixed Gaussian kernel. We describe a Bayesian framework that incorporates non-stationary, adaptive smoothing into a generative model to extract spatial features in parameter estimates. Critically, this means adaptive smoothing becomes an integral part of estimation and inference. We illustrate the method using synthetic and real fMRI data.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Diffusion kernel</kwd>
        <kwd>Weighted graph Laplacian</kwd>
        <kwd>Spatial priors</kwd>
        <kwd>Gaussian process model</kwd>
        <kwd>fMRI</kwd>
        <kwd>General linear model</kwd>
        <kwd>Random effects analysis</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <title>Introduction</title>
      <p>Functional MRI data are typically transformed to a three-dimensional regular grid of voxels in anatomical space, each containing a univariate time series of responses to experimental perturbation. The data are then used to invert a statistical model, e.g., general linear model (GLM), after a number of pre-processing steps, which include spatial normalization and smoothing (i.e., convolving the data with a spatial kernel). In mass-univariate approaches (e.g., statistical parametric mapping), a statistical model is used to extract features from the smoothed data by treating each voxel as a separate observation. Model parameters, at each voxel, are estimated (<xref rid="bib15" ref-type="bibr">Friston et al., 2002</xref>) and inference about these parameters proceeds using SPMs or posterior probability maps (<xref rid="bib14" ref-type="bibr">Friston and Penny, 2003</xref>). Smoothing the data ensures the maps of parameter estimates are also smooth. This can be viewed as enforcing a smoothness prior on the parameters. The current paper focuses on incorporating smoothness into the statistical model by making smoothness a hyperparameter of the model and estimating it using empirical Bayes. This optimizes the spatial dependencies among parameter estimates and has the potential to greatly enhance spatial feature detection.</p>
      <p>Recently <xref rid="bib38" ref-type="bibr">Penny et al. (2005)</xref> extended the use of shrinkage priors on parameter estimates (<xref rid="bib37" ref-type="bibr">Penny et al., 2003</xref>), which assume spatial independence, to spatial priors in a statistical model of fMRI time series. They developed an efficient algorithm using a mean-field approximation within a variational Bayes framework. The result is a smoothing process that is incorporated into a generative model of the data, where each parameter is smoothed according to a measure of uncertainty in that parameter. The advantage of a mean-field approximation is that inversion of a requisite spatial precision matrix is avoided. The advantage of a Bayesian framework is that the evidence for different spatial priors can be compared (<xref rid="bib33" ref-type="bibr">MacKay, 2003</xref>). Other Bayesian approaches to spatial priors in fMRI include those of <xref rid="bib19" ref-type="bibr">Gossl et al. (2001)</xref>; <xref rid="bib56" ref-type="bibr">Woolrich et al. (2004)</xref>; and more recently <xref rid="bib12" ref-type="bibr">Flandin and Penny (2007)</xref>.</p>
      <p>There are two main departures from this previous work on spatiotemporal models in the current method. The first is that we use a Gaussian process prior (GPP) over parameter estimates. Spatial correlations are then encoded using a covariance matrix instead of precisions (cf. <xref rid="bib38" ref-type="bibr">Penny et al., 2005</xref>). The second is that the covariance matrix is the Green's function of a diffusive process, i.e., a diffusion kernel, which encodes the solution of a diffusion equation involving a weighted graph Laplacian. This has the advantage of providing a full spatial covariance matrix and enables inference with regards to the spatial extent of activations. This is not possible using a mean-field approximation that factorizes the posterior distribution over voxels. The result is an adaptive smoothing that can be spatially non-stationary, depending on the data. This is achieved by allowing the local geometry of the parameter field to influence the diffusion kernel (smoothing operator). This is important as stationary smoothing reveals underlying spatial signal at the expense of blurring spatial features. Given the convoluted spatial structure of the cortex and patchy functional segregation, it is reasonable to expect variability in the gradient structure of a parameter field. The implication is that the local geometry of activations should be preserved. This can be achieved with a nonlinear smoothing process that adapts to local geometric ‘features’. A disadvantage is the costly operation of evaluating matrix exponentials and inverting potentially large covariance matrices, which the mean-field approach avoids. However, many approximate methods exist (<xref rid="bib33 bib42" ref-type="bibr">MacKay, 2003; Rasmussen and Williams, 2006</xref>) that can ameliorate this problem, e.g., sparse GPPs (see discussion and <xref rid="bib41" ref-type="bibr">Quinonero-Candela and Rasmussen, 2005</xref>).</p>
      <p>The paper is organized as follows. First, we discuss background and related approaches, before giving an outline of the theory of the method. We start with the model, which is a two-level general linear model (GLM) with matrix-variate density priors on GLM parameters. We focus on reducing the model to the specification of covariance components, in particular, the form of covariance and its hyperparameters. We then look at the form of the spatial priors using graph Laplacians and diffusion kernels. We then describe the <bold>EM</bold> algorithm that is used to update hyperparameters of covariance components, which embody empirical spatial priors. The edge preserving quality of diffusion over a weighted graph is demonstrated using synthetic data and then applied to real fMRI data. The illustrations in this paper use 2D spatial images, however, the method can be easily extended to 3D, subject to computational resources, which would be necessary to analyze a volume of brain data. We perform a random effects (between subjects) analysis (<xref rid="bib36" ref-type="bibr">Penny and Holmes, 2003</xref>) on a sample of contrast images from twelve subjects. This means that we consider a scalar field of parameter estimates encoding the population response. However, the nonlinear diffusion kernels described here can be extended to fields of vectors and matrices (<xref rid="bib6 bib60" ref-type="bibr">ChefD'Hotel et al., 2004; Zhang and Hancock, 2006b</xref>). This paper concludes with comments on outstanding issues and future work.</p>
    </sec>
    <sec>
      <title>Background</title>
      <p>The current work draws on two main sources in the literature; diffusion-based methods in image processing and Gaussian process models (GPM). The image processing community has been using diffusion models for many years, e.g., for the restoration of noisy images (<xref rid="bib29" ref-type="bibr">Knutsson et al., 1983</xref>). For overviews, from the perspective of scale-space theories, see <xref rid="bib43 bib44" ref-type="bibr">Romeny (1994, 2003)</xref>. These models rest on the diffusion equation, which is a nonlinear partial differential equation describing the density fluctuations in an ensemble undergoing diffusion; <italic>μ˙</italic> = ∇·<italic>D</italic>(<italic>μ</italic>)∇<italic>μ</italic>, where <italic>μ</italic> can be regarded as the density of the ensemble (e.g., image intensity) and <italic>D</italic> is the diffusion coefficient. Generally, the diffusion coefficient depends on the density, however, if <italic>D</italic> is a constant, the equation reduces to the ‘classical heat equation’; <italic>μ˙</italic> = <italic>D</italic>∇<sup>2</sup><italic>μ</italic>, where ∇<sup>2</sup> ≡ Δ is the Laplacian operator (second-order spatial derivative). A typical use in image processing is to de-noise an image, where the noisy image is the initial condition, <italic>μ</italic>(<italic>t</italic> = 0) and a smoothed, de-noised, image is the result of integrating the heat equation to evaluate the diffused image at some time later; <italic>μ</italic>(<italic>t</italic>). In particular, <xref rid="bib40" ref-type="bibr">Perona and Malik (1990)</xref> used nonlinear diffusion models to preserve the edges of images using an image dependent diffusion term, <italic>D</italic> = <italic>D</italic>(∇<italic>μ</italic>). The dependence on this spatial gradient has the effect of reduced diffusion over regions with high gradient, i.e., edges. Later formulations of nonlinear diffusion methods include those of <xref rid="bib1" ref-type="bibr">Alvarez et al. (1992)</xref> and <xref rid="bib55" ref-type="bibr">Weickert (1996)</xref>. Of particular relevance to the method presented here are graph-theoretic methods, which use graph Laplacians (<xref rid="bib7" ref-type="bibr">Chung, 1991</xref>). These have been used recently to adaptively smooth scalar, vector and matrix-valued images (<xref rid="bib58" ref-type="bibr">Zhang and Hancock, 2005</xref>). Graphical methods provide a general formulation on arbitrary graphs, which is easy to implement. There are also many useful graph-based algorithms in the literature, e.g., image processing on arbitrary graphs (<xref rid="bib20" ref-type="bibr">Grady and Schwartz, 2003</xref>) and, more generally, graph partitioning to sparsify and solve large linear systems (<xref rid="bib50" ref-type="bibr">Spielman and Teng, 2004</xref>).</p>
      <p>Gaussian process models also have a long history. A Gaussian process prior (GPP) is a collection of random variables, any finite number of which have a joint Gaussian distribution (<xref rid="bib33 bib42" ref-type="bibr">MacKay, 2003; Rasmussen and Williams, 2006</xref>). As such it is completely specified by a mean and covariance function. This is a very flexible prior as it is a prior over a function, which can be used to model general data, not just images. Given a function over space, this function is assumed to be a sample from a Gaussian random field specified by a mean and covariance, which can take many forms, as long as it is positive semi-definite.</p>
      <p>Diffusion methods in image processing and covariance functions in GPMs furnish the basis of a spatial smoothing operator; however, the emphasis of each approach is different. One main difference is that a GPM is a statistical model from which inferences and predictions can be made (<xref rid="bib32" ref-type="bibr">MacKay, 1998</xref>). The objective is not solely to smooth data, but to estimate an optimal smoothing operator, which is embedded in a model of how the data were generated. Graphical models in machine learning (<xref rid="bib24" ref-type="bibr">Jordan, 1999</xref>) provide a general and easy formulation of statistical models. The similar benefits of graph-based diffusion methods in image processing further motivates the use of graph-theoretic approaches to represent and estimate statistical images, given functional brain data.</p>
      <p>The relation between models of diffusion and GPPs is seen when considering a random variable as a diffusive process, which locally is a Gaussian process. We can see this by comparing the Green's function of the classical heat equation, used in early diffusion methods in image processing (<xref rid="bib43" ref-type="bibr">Romeny, 1994</xref>) and the squared exponential (SE) covariance function used in GPMs (<xref rid="bib42" ref-type="bibr">Rasmussen and Williams, 2006</xref>). In two dimensions, (<italic>u</italic><sub><italic>k</italic></sub>, <italic>u</italic><sub><italic>l</italic></sub>), where subscripts indicate location in the domain and <italic>D</italic> is a scalar;<disp-formula id="fd1"><label>(1)</label><mml:math id="M1" altimg="si4.gif" overflow="scroll"><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>μ</mml:mi></mml:math></disp-formula><disp-formula><mml:math id="M2" altimg="si5.gif" overflow="scroll"><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M3" altimg="si6.gif" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mtext>;</mml:mtext><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>π</mml:mi><mml:mi>D</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>D</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>K</italic>(<italic>τ</italic>) is the Green's function (solution) of the diffusion equation that represents the evolution of a solution over time. The first line is the special case of constant diffusion coefficient. The solution of this equation is given in the second and third line, where the image at time <italic>t</italic>, <italic>μ</italic>(<italic>t</italic>), is propagated to <italic>t</italic> + <italic>τ</italic> by convolution with the Green's function, or practically by the matrix–vector product using the matrix exponential of the scaled discrete Laplacian. This Green's function is Gaussian with variance 2<italic>Dτ</italic>, meaning that the image at <italic>t</italic> + <italic>τ</italic> is a smoothed version of <italic>μ</italic>(<italic>t</italic>). This is shown explicitly in the last line<xref rid="fn1" ref-type="fn">1</xref>, which has the same form as the SE covariance function, given below, where the squared characteristic length scale is <italic>σ</italic><sup>2</sup> = 2<italic>Dτ</italic>. Typically, a GPP has an additional scale hyperparameter to give<disp-formula><label>(2)</label><mml:math id="M4" altimg="si7.gif" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mtext>;</mml:mtext><mml:mi>λ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>υ</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>λ</italic> = (<italic>υ</italic>, <italic>σ</italic>). A zero mean GPP is then specified, at a set of locations, by the multivariate density, <italic>μ</italic> ∼ <italic>N</italic>(0, <italic>K</italic>) (<xref rid="bib42" ref-type="bibr">Rasmussen and Williams, 2006</xref>). In what follows, we use a diffusion kernel as the covariance of a GPP. This is a spatial prior on model parameter images.</p>
      <p>There are a number of papers applying methods from image processing to anatomical and functional brain images. These include those of <xref rid="bib17" ref-type="bibr">Gerig et al. (1992)</xref>, who applied nonlinear diffusion methods to MRI data and <xref rid="bib8" ref-type="bibr">Chung et al. (2003)</xref> who used the Laplace–Beltrami operator (a generalization of the heat equation to a Riemannian manifold) in a statistical approach to deformation-based morphometry. Nonlinear diffusion methods have been used to adaptively smooth functional images (<xref rid="bib27 bib28" ref-type="bibr">Kim and Cho, 2002; Kim et al., 2005</xref>). Other approaches to adaptive analysis of fMRI include those of <xref rid="bib10" ref-type="bibr">Cosman et al. (2004)</xref>; <xref rid="bib13" ref-type="bibr">Friman et al. (2003)</xref>; and <xref rid="bib51" ref-type="bibr">Teo et al. (1997)</xref>. Graph-theoretic approaches to image processing have been used to regularize diffusion tensor images (DTI) (<xref rid="bib59" ref-type="bibr">Zhang and Hancock, 2006a</xref>). These authors used a weighted graph Laplacian, which is a discrete analogue of the Laplace–Beltrami operator, to adaptively smooth over a field of diffusion tensors, thereby preserving boundaries between regions, e.g., white matter tracts and grey matter.</p>
      <p>The contribution of our work is to combine graph-theoretic methods from image processing and Gaussian process models from machine learning to provide a spatial model of fMRI data. We are essentially constructing a manifold out of the parameter estimates of a linear model of fMRI data and performing isotropic diffusion on the induced manifold, which is anisotropic from the perspective of the domain. In other words, the diffusion is isotropic on the sub-manifold (that represents the surface of the image) embedded in anatomical–feature space (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>), which is anisotropic in anatomical space. This is somewhat related to the random field approach (<xref rid="bib57" ref-type="bibr">Worsley et al., 1999</xref>), where isotropic smoothing is attained by smoothing along an induced manifold. In our application we use anisotropic diffusion as an empirical spatial prior in a Bayesian setting.</p>
    </sec>
    <sec>
      <title>The model</title>
      <p>In this section, we formulate a two-level GLM in terms of matrix-variate normal densities (<xref rid="bib21" ref-type="bibr">Gupta and Nagar, 2000</xref>). Our focus is the formulation of this as a multivariate normal model, with emphasis on covariance components and their hyperparameters. We start with a linear model, under Gaussian assumptions, of the form<disp-formula id="fd2"><label>(3)</label><mml:math id="M5" altimg="si8.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd/><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mtext>,</mml:mtext><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mtext>,</mml:mtext><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mo>⇒</mml:mo></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mtext>,</mml:mtext><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">MN</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mi>θ</mml:mi><mml:mtext>,</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi mathvariant="italic">K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>ε</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="normal">MN</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi mathvariant="italic">K</mml:mi><mml:mi mathvariant="italic">i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd/><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">MN</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi mathvariant="italic">K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the left-hand expressions specify a hierarchical linear model and the right-hand expressions define the implicit generative density in terms of a likelihood, <italic>p</italic>(<italic>Y</italic>|<italic>X</italic>, <italic>θ</italic>) and prior, <italic>p</italic>(<italic>θ</italic>). MN stands for matrix-normal, where the density on matrix <mml:math id="M6" altimg="si9.gif" overflow="scroll"><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mtext>,</mml:mtext><mml:mi>A</mml:mi><mml:mo>∼</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mtext>,</mml:mtext><mml:mi>S</mml:mi><mml:mo>⊗</mml:mo><mml:mi mathvariant="italic">K</mml:mi><mml:mo>)</mml:mo></mml:math>, has a mean, <italic>M</italic>, of size <italic>r</italic> × <italic>c</italic>, with covariances, <italic>K</italic> and <italic>S</italic>, of size <italic>c</italic> × <italic>c</italic> and <italic>r</italic> × <italic>r</italic>, that encode covariance between columns and rows respectively<xref rid="fn2" ref-type="fn">2</xref>. Here, <italic>Y</italic> is a <italic>T</italic> × <italic>N</italic> data matrix and <italic>X</italic> is a <italic>T</italic> × <italic>P</italic> design matrix with an associated unknown <italic>P</italic> × <italic>N</italic> parameter matrix <italic>θ</italic>.</p>
      <p>The errors at both levels have covariance <italic>S</italic><sub><italic>i</italic></sub> over rows (e.g., time, subjects or regressors) and <italic>K</italic><sub><italic>i</italic></sub> over columns (e.g., voxels). In this paper <italic>S</italic><sub><italic>i</italic></sub> are fixed. Eq. <xref rid="fd2" ref-type="disp-formula">(3)</xref> is a typical model used in the analysis of fMRI data comprising <italic>T</italic> scans, <italic>N</italic> voxels and <italic>P</italic> parameters. The addition of the second level places empirical shrinkage priors on the parameters. This model can now be simplified by vectorizing each component using the identity vec(<italic>ABC</italic>) = (<italic>C</italic><sup><italic>T</italic></sup><italic>⊗A</italic>)vec(<italic>B</italic>) (see <xref rid="app1" ref-type="sec">Appendix I</xref> and <xref rid="bib23" ref-type="bibr">Harville, 1997</xref>)<disp-formula id="fd3"><label>(4)</label><mml:math id="M7" altimg="si10.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>Z</mml:mi><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>∼</mml:mo></mml:mtd><mml:mtd><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext><mml:msub><mml:mi>Σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>y</italic> = vec(<italic>Y</italic>), <italic>Z</italic> = <italic>I</italic><sub><italic>N</italic></sub><italic>⊗X</italic>, <italic>w</italic> = vec(<italic>θ</italic>), <italic>e</italic><sub><italic>i</italic></sub> = vec(<italic>ε</italic><sub><italic>i</italic></sub>) and <italic>Σ</italic><sub><italic>i</italic></sub> = <italic>K</italic><sub><italic>i</italic></sub><italic>⊗S</italic><sub><italic>i</italic></sub>. <italic>⊗</italic> is the Kronecker product of two matrices and <italic>I</italic><sub><italic>N</italic></sub> is the identity matrix of size <italic>N</italic>. The unknown covariances <italic>Σ</italic>(<italic>λ</italic>)<sub>1</sub> and <italic>Σ</italic>(<italic>λ</italic>)<sub>2</sub> depend on hyperparameters, <italic>λ</italic>. The model parameters and hyperparameters are estimated using expectation maximization (<bold>EM</bold>) by maximizing the log-marginal likelihood<disp-formula><label>(5)</label><mml:math id="M8" altimg="si11.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mi>Σ</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Σ</mml:mi><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>with respect to the parameters in the <bold>E</bold>-Step and the covariance hyperparameters in the <bold>M</bold>-Step. Here, <italic>Σ</italic>(<italic>λ</italic>) represents the covariance of the data induced by both levels of the model. The model inversion with <bold>EM</bold> will be described later (see also <xref rid="app1 app2" ref-type="sec">Appendices I and II</xref>). First, we look at the hyperparameterization of the spatial covariances and the specific forms of <italic>K</italic>(<italic>λ</italic>) entailed by <italic>Σ</italic><sub><italic>i</italic></sub> = <italic>K</italic><sub><italic>i</italic></sub><italic>⊗S</italic><sub><italic>i</italic></sub>.</p>
    </sec>
    <sec>
      <title>The priors</title>
      <p>In the previous section, we reduced the problem of specifying a linear empirical Bayesian model to the specification of prior covariance components for noise and signal. In this section, we introduce diffusion-based spatial priors and consider adaptive priors that are functions of the parameters. In brief, we will assume the error or noise covariance is spatially unstructured; i.e., <italic>Σ</italic><sub>1</sub> = <italic>K</italic><sub>1</sub><italic>⊗S</italic><sub>1</sub>, where <italic>K</italic>(<italic>λ</italic>)<sub>1</sub> = <italic>υ</italic><sub>1</sub><italic>I</italic><sub><italic>N</italic></sub> and <italic>S</italic><sub>1</sub> = <italic>I</italic><sub><italic>T</italic></sub>. This means that <italic>υ</italic><sub>1</sub> is the error variance. For simplicity, we will assume that this is fixed over the same for all voxels; however, it is easy to specify a component for each voxel, as in conventional mass-univariate analyses.</p>
      <p>For the signal, we adopt an adaptive prior using a non-stationary diffusion kernel, which is based on a weighted graph Laplacian (see <xref rid="bib7" ref-type="bibr">Chung, 1991</xref> and next section), <italic>L</italic>(<italic>μ</italic>), which is a function of the conditional expectation of the parameters, <italic>μ</italic> = 〈<italic>w</italic>〉<disp-formula id="fd4"><label>(6)</label><mml:math id="M9" altimg="si12.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mo>)</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>υ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>This means the hyperparameters comprise, <italic>λ</italic> = {<italic>υ</italic><sub>1</sub>,<italic>υ</italic><sub>2</sub>,<italic>τ</italic>}, where the first hyperparameter controls a stationary independent and identical (i.i.d.) noise component, the second the amplitude of the parameter image and third its dispersion. The matrix <italic>L</italic> in Eq. <xref rid="fd4" ref-type="disp-formula">(6)</xref> is a weighted graph Laplacian, which is a discrete analogue of the Laplace–Beltrami operator used to model diffusion processes on a Riemannian manifold. The solution of the heat equation is<xref rid="fn3" ref-type="fn">3</xref><disp-formula><label>(7)</label><mml:math id="M10" altimg="si13.gif" overflow="scroll"><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mi>μ</mml:mi><mml:mo>⇒</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M11" altimg="si14.gif" overflow="scroll"><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M12" altimg="si15.gif" overflow="scroll"><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>The diffusion kernel, <italic>K</italic> = exp(− <italic>Lτ</italic>), is the local solution to the heat equation on a graph and corresponds to a symmetric diffusion kernel that encodes the dispersion of <italic>μ</italic>, over a period <italic>τ</italic>. The diffusion kernel also evolves according to the heat equation<disp-formula><label>(8)</label><mml:math id="M13" altimg="si16.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>We use this diffusion kernel as the covariance matrix of a GPM. Generally, the Laplacian is a function, <italic>L</italic>(<italic>μ</italic><sup>(<italic>m</italic>)</sup>), of the current image (of parameter expectations), where the superscript indicates the <italic>m</italic><sup>th</sup> iteration. In this situation, Green's function is a composition of local solutions.<disp-formula><label>(9)</label><mml:math id="M14" altimg="si17.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mo>)</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mo>υ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>…</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Updating <italic>K</italic><sub>D</sub><sup>(<italic>m</italic>)</sup> requires computation of the current Laplacian <italic>L</italic><sup>(<italic>m</italic>)</sup> and a matrix multiplication, both of which are costly operations on large matrices. However, if the Laplacian is approximately constant then <italic>K</italic><sub>D</sub><sup>(<italic>m</italic>)</sup> can be evaluated much more simply<disp-formula><label>(10)</label><mml:math id="M15" altimg="si18.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>…</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>≈</mml:mi></mml:mtd><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>This approximation retains the edge preserving character of the diffusive flow, without incurring the computational cost of re-evaluating the Laplacian. Our experience is that weighted graph Laplacians based on the ordinary least squares (OLS) estimate, <italic>μ</italic><sub>ols</sub>, gives very reasonable results. However, the need to update the Laplacian may arise when the OLS parameter estimate is very noisy. All anisotropic Laplacian priors in this paper are based on <italic>μ</italic><sub>ols</sub>; however, we have included update equations based on the Baker–Campbell–Hausdoff formula in <xref rid="app4" ref-type="sec">Appendix IV</xref>.</p>
      <p>In summary, the covariance components and their derivatives are:<disp-formula><label>(11)</label><mml:math id="M16" altimg="si19.gif" overflow="scroll"><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>υ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></disp-formula><disp-formula><mml:math id="M17" altimg="si20.gif" overflow="scroll"><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>υ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mtext>ols</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula><mml:math id="M18" altimg="si21.gif" overflow="scroll"><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mo>υ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math></disp-formula><disp-formula><mml:math id="M19" altimg="si22.gif" overflow="scroll"><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mo>υ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext></mml:msub></mml:math></disp-formula><disp-formula><mml:math id="M20" altimg="si23.gif" overflow="scroll"><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>υ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi>L</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mtext>D</mml:mtext></mml:msub></mml:math></disp-formula>where their hyperparameters <italic>λ</italic> = {<italic>υ</italic><sub>1</sub>, <italic>υ</italic><sub>2</sub>, <italic>τ</italic>} are optimized to ensure an optimal balance between signal and noise and that the parameter estimates have an optimal, non-stationary and non-isotropic smoothness encoded by the spatial covariance, <italic>K</italic><sub>2</sub>. In the next section, we review graph Laplacians and the diffusion model in more detail and then conclude with a summary of the <bold>EM</bold> scheme used for optimization.</p>
    </sec>
    <sec>
      <title>Diffusion on graphs</title>
      <p>In this section, we describe diffusion on graphs and illustrate how this furnishes useful spatial priors for parameters at each voxel. This formulation is useful as it is easily extended to vector and matrix-valued images, which will be necessary when modeling a general vector-field of parameter estimates, e.g., when the number of columns of the design matrix is greater than one. We start with some basic graph theory and then discuss diffusion in terms of graph Laplacians. The end point of this treatment is the form of the diffusion kernel, <italic>K</italic><sub>D</sub>, used in the spatial prior of the previous section. We will see that this is a function of the parameters that enables the prior smoothness to adapt locally to non-stationary features in the image of parameter estimates.</p>
      <sec>
        <title>GLM parameters as a function on a graph</title>
        <p>We consider the parameter estimates as a function on a graph, <italic>Γ</italic>, with vertices, edges and weights, <italic>Γ</italic> = (<italic>V</italic>, <italic>E</italic>, <italic>W</italic>). The vertices are indexed 1 to <italic>N</italic> and pairs are connected by edges, <italic>E</italic><sub><italic>kn</italic></sub>, where (<italic>k</italic>, <italic>n</italic>) ∈ <italic>V</italic>. If two vertices are connected, i.e., are neighbors, we write <italic>k</italic> ∼ <italic>n</italic>. Consider a regular 2D mesh with spatial coordinates <italic>u</italic><sup>1</sup> and <italic>u</italic><sup>2</sup>. <xref rid="fig1" ref-type="fig">Fig. 1</xref>a shows a surface plot of OLS parameter estimates of synthetic data described later (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>) to illustrate the construction of a weighted graph Laplacian. To simplify the discussion we concentrate on a small region over a 3 × 3 grid, or stencil (see inset). Pairs of numbers <italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup> indicate a vertex or pixel location, where each number corresponds to a spatial dimension. The function has a value at each pixel (voxel if in 3D anatomical space) given by its parameter estimate <italic>μ</italic>(<italic>u</italic>), so that three numbers locate a pixel at which a parameter has a specific value, <italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>, <italic>μ</italic>(<italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>). These are coordinates of the parameter estimate at a pixel in Euclidean space, <mml:math id="M21" altimg="si24.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:math>, which decomposes into ‘anatomical’ and ‘feature’ space coordinates (lower right of <xref rid="fig1" ref-type="fig">Fig. 1</xref>a). In this case these are 2 and 1 respectively. The 2D image is considered as a 2D sub-manifold of this 3D embedding space (<xref rid="bib48" ref-type="bibr">Sochen et al., 1998</xref>), which provides a general framework that is easily extended to 3D anatomical space and feature dimensions greater than one. We represent the <italic>k</italic><sup>th</sup> pixel by <italic>v</italic><sub><italic>k</italic></sub>. Distance between two pixels is taken as the shortest distance along the 2D sub-manifold of parameter estimates embedded in <mml:math id="M22" altimg="si25.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:math>. This is a geodesic distance between points on the sub-manifold, <italic>d</italic><sub><italic>s</italic></sub>(<italic>v</italic><sub><italic>k</italic></sub>, <italic>v</italic><sub><italic>n</italic></sub>). This is shown schematically in <xref rid="fig1" ref-type="fig">Fig. 1</xref>c between neighboring pixels. The shortest distance is easy to compute for direct neighbors (example shown in red), however, if the stencil were larger then fast marching algorithms (<xref rid="bib46" ref-type="bibr">Sethian, 1999</xref>) may be used to compute the shortest path between two points on the sub-manifold. Note that the displacement along the feature coordinates is scaled by a, such that if <italic>a</italic> = 0, then <italic>d</italic><sub><italic>s</italic></sub> is reduced to distance on the 2D domain and is no longer a function of image intensity (see subsection on special cases). The construction of a weighted graph Laplacian starts by specifying weights of edges between vertices, <italic>w</italic><sub><italic>kn</italic></sub>. These are a function of the geodesic distance, <italic>d</italic><sub><italic>s</italic></sub>(<italic>v</italic><sub><italic>k</italic></sub>, <italic>v</italic><sub><italic>n</italic></sub>), and are important for specifying non-stationary diffusion. This is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>b for the 3 × 3 stencil in <xref rid="fig1" ref-type="fig">Fig. 1</xref>a.</p>
      </sec>
      <sec>
        <title>Graph Laplacian</title>
        <p>As mentioned above, a graph is composed of vertices, edges and weights. Neighboring vertices are encoded by the adjacency matrix, <italic>A</italic>, with elements<disp-formula><label>(12)</label><mml:math id="M23" altimg="si27.gif" overflow="scroll"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="0.35em" height="0.3ex"/><mml:mtext>for</mml:mtext><mml:mspace width="0.35em" height="0.3ex"/><mml:mi>k</mml:mi><mml:mo>∼</mml:mo><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="0.35em" height="0.3ex"/><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>Weights make up a weight matrix, <italic>W</italic>, with elements<disp-formula id="fd5"><label>(13)</label><mml:math id="M24" altimg="si28.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.35em" height="0.3ex"/><mml:mtext>for</mml:mtext><mml:mspace width="0.35em" height="0.3ex"/><mml:mi>k</mml:mi><mml:mo>∼</mml:mo><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="8.23em" height="0.3ex"/><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>The un-normalized Laplacian of <italic>Γ</italic> is <italic>L</italic> = <italic>D</italic> − <italic>W</italic>, where <italic>D</italic> is a diagonal matrix with elements <italic>D</italic><sub><italic>kk</italic></sub> = <italic>Σ</italic><sub><italic>n</italic></sub><italic>w</italic><sub><italic>kn</italic></sub>, which is the degree of the <italic>k</italic><sup>th</sup> vertex. The graph Laplacian is sometimes called the admittance or Kirchhoff matrix. The weights <italic>w</italic><sub><italic>kn</italic></sub> ∈ [0, 1] encode the relationship between neighboring pixels and are symmetric; i.e., <italic>w</italic><sub><italic>kn</italic></sub> = <italic>w</italic><sub><italic>nk</italic></sub>. They play the role of conductivities, where a large value enables flow between pixels. <italic>κ</italic> is a constant that controls velocity of diffusion, which we set to one.</p>
        <p>The weights are a function of the distance, <italic>d</italic><sub><italic>s</italic></sub>(<italic>v</italic><sub><italic>k</italic></sub>, <italic>v</italic><sub><italic>n</italic></sub>), on the surface of the function, <italic>μ</italic>(<italic>u</italic>), between vertices <italic>v</italic><sub><italic>k</italic></sub> and <italic>v</italic><sub><italic>n</italic></sub>. It is this distance that defines the nature of diffusion generated by the weighted graph Laplacian. In brief, we will define this distance to make the diffusion isotropic on the surface of the parameter image. This means, when looked at from above, that the diffusion will appear less marked when the spatial gradients of parameters are high. In other words, diffusion will be attenuated at the edges of regions with high parameter values. More formally, we specify the distance by choosing a map, <italic>χ</italic>, from the surface of the function <italic>μ</italic>(<italic>u</italic>) to an embedding space, the Euclidean space of <mml:math id="M25" altimg="si29.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:math>. Each space has a manifold and metric, (<italic>M</italic>, <italic>g</italic>) and (<italic>N</italic>, <italic>h</italic>), respectively (see <xref rid="app3" ref-type="sec">Appendix III</xref> for more details and a heuristic explanation).<disp-formula><label>(14)</label><mml:math id="M26" altimg="si30.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo>χ</mml:mo><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi>N</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>χ</mml:mo><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>→</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mo>χ</mml:mo><mml:mn>1</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:msup><mml:mo>χ</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:msup><mml:mo>χ</mml:mo><mml:mn>3</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mtext>,</mml:mtext><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mtext>,</mml:mtext><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mtext>,</mml:mtext><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>Choosing a metric, <italic>H</italic>, of the embedding space (see below) and computing the Jacobian, <italic>J</italic>, we can calculate the induced metric, <italic>G</italic>, on <italic>μ</italic>(<italic>u</italic>) (<xref rid="bib48" ref-type="bibr">Sochen et al., 1998</xref>). In matrix form, these are<disp-formula><label>(15)</label><mml:math id="M27" altimg="si31.gif" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mo>χ</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>a</italic><sub><italic>i</italic></sub> are the relative scales among dimensions and derivatives are with respect to physical space; i.e., <italic>μ</italic><sub><italic>x</italic></sub> = ∂<italic>μ</italic>/∂<italic>x</italic>, which are computed using central differences. The induced metric is then<disp-formula id="fd6"><label>(16)</label><mml:math id="M28" altimg="si32.gif" overflow="scroll"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>H</mml:mi><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:msubsup><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:msubsup><mml:mi>μ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>which is used to calculate distance<disp-formula><label>(17)</label><mml:math id="M29" altimg="si33.gif" overflow="scroll"><mml:msubsup><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>G</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi>u</mml:mi></mml:math></disp-formula>where d<italic>u</italic> = (d<italic>u</italic><sup>1</sup>, d<italic>u</italic><sup>2</sup>)<sup><italic>T</italic></sup>. Due to the dependence of the graph Laplacian on the parameters we write the Laplacian as <italic>L</italic> = <italic>L</italic>(∇<italic>μ</italic><sub>ols</sub>), where the Laplacian is computed using the OLS estimate, <italic>μ</italic><sub>ols</sub>. As this depends on geodesic distances on the embedded sub-manifold of an image we call it a geodesic graph Laplacian (GGL). If <italic>a</italic><sub><italic>μ</italic></sub> = 0 then the Laplacian is based on Euclidean distance in anatomical space. We refer to this as a Euclidean graph Laplacian (EGL) (see also subsection below on special cases). Note that we have chosen the embedding coordinates and embedding space metric. This is one of the advantages of a geometric formulation as we could have chosen a non-Euclidean anatomical space, e.g., spherical coordinates to model diffusion on the surface of a sphere (see <xref rid="bib49 bib47" ref-type="bibr">Sochen et al., 2003; Sochen and Zeevi, 1998</xref>). The diffusion kernel can be computed efficiently using an eigenvalue decomposition of the Laplacian.<disp-formula><label>(18)</label><mml:math id="M30" altimg="si34.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>L</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>V</mml:mi><mml:mi>Λ</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>diag</mml:mtext><mml:mo>(</mml:mo><mml:mi>Λ</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:mspace width="0.35em" height="0.3ex"/><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>K</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>V</mml:mi><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>Λ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
        <p>This is a standard method for computing the matrix exponential (<xref rid="bib35" ref-type="bibr">Moler and Van Loan, 2003</xref>) with the added benefit that knowing the eigensystem simplifies many computations in the algorithm. See <xref rid="app4" ref-type="sec">Appendix IV</xref> for more details.</p>
      </sec>
    </sec>
    <sec>
      <title>Expectation maximization</title>
      <p>Inversion of the multivariate normal model in Eq. <xref rid="fd3" ref-type="disp-formula">(4)</xref> is straightforward and can be formulated in terms of expectation maximization (<bold>EM</bold>). <bold>EM</bold> entails the iterative application of an <bold>E</bold>-Step and <bold>M</bold>-Step (see <xref rid="app1" ref-type="sec">Appendix 1</xref> and <xref rid="bib15 bib16" ref-type="bibr">Friston et al., 2002, 2007</xref> for details). The <bold>E</bold>-Step evaluates the conditional density of the parameters in terms of their expectation and precision (i.e., inverse variance); <italic>p</italic>(<italic>w</italic>|<italic>y</italic>,<italic>λ</italic>) = <italic>N</italic>(<italic>μ</italic>, <italic>Π</italic><sup>− 1</sup>), where<disp-formula id="fd7"><label>(19)</label><mml:math id="M31" altimg="si35.gif" overflow="scroll"><mml:mi mathvariant="bold">E</mml:mi><mml:mtext>−Step</mml:mtext><mml:mspace width="0.35em" height="0.3ex"/><mml:mtable><mml:mtr><mml:mtd><mml:mi>μ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mover><mml:mo>∏</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∏</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The unknown covariances <italic>Σ</italic>(<italic>λ</italic>)<sub><italic>i</italic></sub> = <italic>K</italic><sub><italic>i</italic></sub><italic>⊗S</italic><sub><italic>i</italic></sub> are functions of covariance hyperparameters, <italic>λ</italic>. These are estimated by maximizing the log-marginal likelihood, ln <italic>p</italic>(<italic>y</italic>|<italic>λ</italic>), in an <bold>M</bold>-Step. This involves updating the hyperparameters by maximizing a bound on the log-marginal likelihood or log-evidence<disp-formula><label>(20)</label><mml:math id="M32" altimg="si36.gif" overflow="scroll"><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mi>Σ</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p>
      <p>We update hyperparameters (indexed by subscripts) using a Fisher-scoring scheme<xref rid="fn4" ref-type="fn">4</xref>, where Δ<italic>λ</italic> represents incremental change of <italic>λ</italic> <disp-formula><label>(21)</label><mml:math id="M33" altimg="si37.gif" overflow="scroll"><mml:mi mathvariant="bold">M</mml:mi><mml:mtext>−Step</mml:mtext><mml:mspace width="0.35em" height="0.3ex"/><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>λ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mo>∇</mml:mo><mml:mi>λ</mml:mi></mml:msub><mml:mi>F</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true">〈</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="true">〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><italic>I</italic>(<italic>λ</italic>) is the expected information matrix, see <xref rid="bib54" ref-type="bibr">Wand (2002)</xref>, with element <italic>I</italic><sub><italic>kl</italic></sub>, where the expectation, 〈〉, is over the marginal likelihood of the data, ∇<italic><sub>λ</sub>F</italic> is the score, i.e., a vector of gradients (<italic>k</italic><sup>th</sup> element given by ∂<italic>F</italic>/∂<italic>λ</italic><sub><italic>k</italic></sub>) with respect to the hyperparameters and <italic>Σ</italic> is the current maximum likelihood estimate of the data covariance (see <xref rid="app1" ref-type="sec">Appendix I</xref>). In the examples below, we fix <italic>S</italic><sub>1</sub> = <italic>I</italic><sub><italic>T</italic></sub> and <italic>S</italic><sub>2</sub> = 1; this means the only unknown covariances are <italic>K</italic>(<italic>λ</italic>)<sub><italic>i</italic></sub>. This scheme is formally identical to classical restricted maximum likelihood (ReML) (see <xref rid="bib16" ref-type="bibr">Friston et al., 2007</xref>).</p>
      <p>In summary, to invert our model we simply specify the covariances <italic>K</italic>(<italic>λ</italic>)<sub><italic>i</italic></sub> and their derivatives, ∂<italic>K</italic>/∂<italic>λ</italic><sub><italic>i</italic></sub>. These enter an <bold>M</bold>-Step to provide ReML or ML-II estimates of covariance hyperparameters. <italic>K</italic>(<italic>λ</italic>)<sub><italic>i</italic></sub> are then used in the <bold>E</bold>-Step to provide the conditional density of the parameters. <bold>E</bold>- and <bold>M</bold>-Steps are iterated until convergence, after which, the objective function for the <bold>M</bold>-Step can be used as an approximation to the models log-evidence. This quantity is useful in model comparison and selection, as we will see later when comparing models based on different spatial priors.</p>
      <p>We now have all the components of a generative model (shown schematically in <xref rid="fig2" ref-type="fig">Fig. 2</xref>) that, when inverted, furnishes parameter estimates that are adaptively smooth, with edge preserving characteristics. Furthermore, this smoothing is chosen automatically and optimizes the evidence of the model. Before applying this scheme to synthetic and real data we will consider some special cases that will be compared in the final section.</p>
    </sec>
    <sec>
      <title>Special cases</title>
      <sec>
        <title>Linear diffusion: <italic>a</italic><sub><italic>μ</italic></sub> = 0</title>
        <p>If we set the scale of the parameter dimension <italic>a</italic><sub><italic>μ</italic></sub> = 0 (see Eq. <xref rid="fd6" ref-type="disp-formula">(16)</xref> and <xref rid="fig1" ref-type="fig">Fig. 1</xref>c) we recover linear diffusion. The Laplacian (EGL) is now independent of <italic>μ</italic>. In this case edges are not preserved by the smoothness prior. Although these kernels are not the focus of this paper they are still useful and, as we demonstrate later, produce compelling results compared to non-spatial priors.</p>
      </sec>
      <sec>
        <title>Global shrinkage priors: <italic>K</italic><sub>D</sub> = <italic>I</italic></title>
        <p>If we removed diffusion by setting the Laplacian to zero then <italic>K</italic><sub>2</sub> = <italic>υ</italic><sub>2</sub><italic>I</italic><sub><italic>N</italic></sub>. This corresponds to global or spatially independent (shrinkage) priors (GSP) of the sort used in early posterior probability maps using empirical Bayes (<xref rid="bib14" ref-type="bibr">Friston and Penny, 2003</xref>). Here, we use the variability of parameter estimates over pixels to shrink their estimates appropriately and provide a posterior or conditional density.</p>
      </sec>
      <sec>
        <title>Ordinary least squares estimate: <italic>K</italic><sub>2</sub> = 0</title>
        <p>The OLS estimate obtains when we remove the empirical priors completely by setting <italic>K</italic><sub>2</sub> = 0.</p>
      </sec>
    </sec>
    <sec>
      <title>Synthetic data: de-noising an image</title>
      <p>In this section, we apply the algorithm to one image, i.e. <italic>T</italic> = 1, to demonstrate edge preservation and provide some intuition as to how this is achieved using a diffusion kernel based on a GGL and compare it to EGL. We use synthetic data shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. The model is<disp-formula><label>(22)</label><mml:math id="M34" altimg="si38.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>The central panel of <xref rid="fig3" ref-type="fig">Fig. 3</xref>a contains a binary image of a 2D closed curve (with values equal to one within and on the curve) on a circular background of zeros. Gaussian noise has been added, which we will try to remove using a GPM based on a diffusion kernel. The left panel shows the conditional expectation or mean using a diffusion kernel from a EGL, while GGL is shown on the right. Below each image is a color bar, which encodes the grey-scale of each pixel in the image. It is immediately obvious that smoothing with a EGL removes noise at the expense of blurring the image, while the GGL preserves edges. This is reflected in the values of log-marginal likelihood achieved for each prior (see <xref rid="tbl1" ref-type="table">Table 1</xref>). We will discuss the ratio of these values in the next section when we consider model comparison.</p>
      <p><xref rid="fig3" ref-type="fig">Fig. 3</xref>b shows contours of local diffusion kernels at three locations in the image, where the local diffusion kernel at the <italic>k</italic><sup>th</sup> pixel is a 2D image reconstructed from the appropriate row of <italic>K</italic><sub>2</sub>. These are superimposed on the smoothed image. Locations include (i) within the perimeter of the object, (ii) near the edge inside the perimeter of the object and (iii) outside the perimeter of the object. The EGL is on the left, where local kernels are the same through-out the image. This is not the case for the GGL on the right. The EGL smoothes the image isotropically, i.e., without a preferred direction. On the right, local kernels within and outside the perimeter of the object are different, depending on their relation to the edge of the central object. The contours of kernels within the perimeter spread into the interior of the object, but stop abruptly at the edge, encoding the topology of the surface, much like the contours on a geographic map. As a result the image is smoothed such that noise is removed, but not at the expense of over smoothing the edges of signal. This is shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>c for a cross-section at the level indicated by a dotted line in <xref rid="fig3" ref-type="fig">Fig. 3</xref>b. This shows the original binary image, noisy image, smoothing with EGL and GGL.</p>
      <p>Further intuition comes from considering the induced metric tensor, <italic>G</italic>; consider the square-root of the determinant, <mml:math id="M35" altimg="si39.gif" overflow="scroll"><mml:msqrt><mml:mrow><mml:mi>det</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:math>, which is the ratio of surface and domain areas. An area element on the surface of <italic>μ</italic>(<italic>u</italic>) is <mml:math id="M36" altimg="si40.gif" overflow="scroll"><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mtext>det</mml:mtext><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math>, while one on the domain is d<italic>A</italic><sub><italic>D</italic></sub> = d<italic>u</italic><sup>1</sup>d<italic>u</italic><sup>2</sup>. This gives a ratio <mml:math id="M37" altimg="si41.gif" overflow="scroll"><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mtext>D</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mtext>det</mml:mtext><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:math>, which can be calculated at each location of the image. This is referred to as the magnification factor (<xref rid="bib4" ref-type="bibr">Bishop, 1999</xref>). This provides a scalar value at each pixel that represents a salient difference between the sub-manifold of the function, compared to the flat surface of the domain. This is shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>d. Flat regions have ratio of about one, while edges are greater than unity. High values correspond to locations where the distance on <italic>μ</italic>(<italic>u</italic>) between adjacent pixels (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>) is large; i.e., at an edge, where gradients are large. This results in a small weight across the edge connecting these pixels and reduced flow. The effect is that regions with large gradients have less smoothing. As large gradients are a feature of edges, this means that they are preserved. To highlight the anisotropic nature of the ensuing diffusion, we have super-imposed ellipses representing the orientation and magnitude (eigenvectors and eigenvalues respectively) of <italic>G</italic> at a selection of different locations. Red and blue ellipses represent <mml:math id="M38" altimg="si42.gif" overflow="scroll"><mml:msqrt><mml:mrow><mml:mtext>det</mml:mtext><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:math> greater and lower than 2, respectively. It can be seen that the metric tensor is aligned with the edge of the central figure and isotropic elsewhere. This leads to preferential diffusion within the image and edge preservation.</p>
      <p>Lastly, we have included a representation of a global property of the graph Laplacian using the graph plot routine in Matlab (gplot.m) of the second and third eigenvectors (of the Laplacian) in <xref rid="fig3" ref-type="fig">Figs. 3</xref>e and f. This is a standard representation of similarity between vertices on a graph. The second eigenvector is known as the Fiedler vector (<xref rid="bib7" ref-type="bibr">Chung, 1991</xref>) and is used in graph-partitioning algorithms. The EGL is regular, whereas the GGL illustrates partitioning of the image into two distinct parts; the central region, which represents the central object in the image of <xref rid="fig3" ref-type="fig">Fig. 3</xref>a, while the background is represented by the periphery of the plot.</p>
    </sec>
    <sec>
      <title>Evaluations</title>
      <p>In this section, we compare the performance of three different Gaussian process priors used to model the same data. These were global shrinkage priors (GSP) and diffusion kernels from Euclidean (EGL) and geodesic graph Laplacians (GGL).</p>
      <sec>
        <title>Synthetic data: random effects analysis</title>
        <p>The next simulation is similar to the above except now we have twelve samples of the image. Their purpose is to demonstrate posterior probability maps (PPMs) and model selection. The samples, original image, OLS estimate and estimated posterior means are shown in <xref rid="fig4" ref-type="fig">Figs. 4</xref>a–c. <xref rid="fig4" ref-type="fig">Fig. 4</xref>d compares PPMs using the three different priors. The first observation is enhanced detection of the signal with EGL and GGL compared to GSP. The second is the edge preserving nature of GGL. The evidence for each model is shown in <xref rid="tbl2" ref-type="table">Table 2</xref>. As expected, given data with edges, the GGL attains the highest evidence. The log-marginal likelihood ratio (natural logarithm) for EGL and GGL was 146, which is very strong evidence in favor of the GGL. A direct comparison of the log-marginal likelihood is possible as the number of hyperparameters is equal for EGL and GGL. Additional penalty terms can be included for model comparison based on the number of hyperparameters used in a model and their uncertainty (<xref rid="bib3" ref-type="bibr">Bishop, 1995</xref>). Details of these additional terms are included in <xref rid="app2" ref-type="sec">Appendix II</xref>.</p>
      </sec>
      <sec>
        <title>Real data: random effects analysis</title>
        <p>fMRI data collected from twelve subjects during a study of the visual motion system (<xref rid="bib22" ref-type="bibr">Harrison et al., 2007</xref>) were used for our comparative analyses. The study had a 2 × 2 factorial design with motion type (coherent or incoherent) and motion speed as the two factors. Single subject analyses were performed, with no smoothing, using SPM2 (<ext-link xlink:href="http://www.fil.ion.ucl.ac.uk/spm" ext-link-type="uri">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) to generate contrast images of the main effect of coherence. Images (one slice) of the twelve contrast images are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>a. These constitute the data, <italic>Y,</italic> and the design matrix, <italic>X</italic> = 1, was a column of ones, implementing a single-sample <italic>t</italic>-test. The aim was to estimate <italic>μ</italic>(<italic>u</italic>); the conditional expectation of the main effect of coherent motion as a function of position in the brain. We calculated <italic>μ</italic>(<italic>u</italic>) under the different priors above.</p>
        <p>For demonstration purposes we selected a slice of the whole brain volume, which contained punctuate responses from bilateral posterior cingulate gyri (pCG). The conditional expectations under the Laplacian priors are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>b for EGL and GGL. Although regions of high or low parameter estimates can be distinguished in the EGL analysis (left panel), the borders are difficult to make out. The posterior mean of GGL is different with well-delineated borders between regions of high and low coefficients, e.g., increased response in pCG. Activated regions are no longer ‘blobs’. This feature is easily seen in <xref rid="fig5" ref-type="fig">Fig. 5</xref>c with contour plot of a local kernel superimposed.</p>
        <p>Posterior probability maps (<xref rid="bib14" ref-type="bibr">Friston and Penny, 2003</xref>) of full slices for EGL and GGL are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>d with thresholds <italic>p</italic>(<italic>w</italic> &gt; 0.5) &gt; 0.95 and close-ups of all three priors in <xref rid="fig5" ref-type="fig">Fig. 5</xref>e. The odd-one-out is the model with global shrinkage priors that had only one positive region within the whole slice. Surface plots are shown in <xref rid="fig5" ref-type="fig">Figs. 5</xref>f–h and graph embeddings in <xref rid="fig5" ref-type="fig">Figs. 5</xref>i and j. Note the vertical axes of surface plots showing largest magnitude for GGL and the degree of shrinkage with GSP.</p>
        <p>The log-marginal likelihood for each model is shown in <xref rid="tbl2" ref-type="table">Table 2</xref>. The highest log-evidence was for GGL. The difference in log-evidence for the geodesic and Euclidean Laplacian priors was 260. This represents very strong evidence that the data were likely to be generated from a field with spatial covariance given by GGL compared to EGL. This sort of model comparison suggests that the data support the use of adaptive diffusion kernels when modeling spatial covariance of activation effects.</p>
      </sec>
    </sec>
    <sec>
      <title>Discussion</title>
      <p>We have outlined a Bayesian scheme to estimate the optimal smoothing of conditional parameter estimates using a Gaussian process model. In this model, the prior covariance uses a diffusion kernel generated by a weighted graph Laplacian.</p>
      <p>There are many issues to consider. We have only demonstrated the model using scalar-valued parameter images. A typical GLM of single subject data has a vector of parameters of size <italic>P</italic> × 1, at each voxel, with a covariance matrix, size <italic>P</italic> × <italic>P</italic>, where the design matrix has <italic>P</italic> columns. This means that there is a vector-field of regression coefficients over a brain volume. The weights of a GGL can be a function of scalars, vectors or matrices, which make it very flexible. For example, a GGL based on distance between parameter vectors at different voxels is easily implemented using the scheme presented here. More complex spaces, such as a field of symmetric positive definite (SPD) matrices, used to regularize Diffusion Tensor Images (DTI) (<xref rid="bib5 bib53 bib59" ref-type="bibr">Chefd’hotel et al., 2002; Tschumperle and Deriche, 2003; Zhang and Hancock, 2006a</xref>), require methods from Lie group analysis, where a SPD matrix is represented as a sub-manifold of <mml:math id="M39" altimg="si43.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:msup></mml:math>. Matrices can be represented by vectors and probabilities over such a space can be represented by Gaussian densities (<xref rid="bib2" ref-type="bibr">Begelfor and Werman, 2005</xref>), which suggests the possibility of using a Gaussian process prior over a spatial distribution of SPD matrices, or a Lie–Gaussian process prior. We also have considered the simplest noise model in this paper; however, noise models that vary over space, i.e., a heteroscedastic noise process, are also easily formulated using Gaussian process priors (see Chapter 5, <xref rid="bib42" ref-type="bibr">Rasmussen and Williams, 2006</xref>) and <xref rid="bib18 bib25" ref-type="bibr">Goldberg et al., 1998; Kersting et al., 2007</xref>). A possible use in fMRI is a GPP over autoregressive model coefficients in single subject data-sets following <xref rid="bib39" ref-type="bibr">Penny et al. (2007)</xref>.</p>
      <p>A major computational issue is the time needed to compute the eigensystem of the Laplacian from which the matrix exponential, inverse, trace and determinant can be computed. The computational complexity scales with <italic>N</italic><sup>3</sup>, which is an issue for large data-sets. We have made no attempt to address this issue here, as our focus was on the combination of graph-theoretic approaches to image processing and spatial GPMs. The time taken to process the 3319 voxels in the random-effects analysis above was about 20 min on a standard personal computer. This has to be reduced, especially if a whole volume is to be processed. An advantage of a geometric formulation of the Laplacian is that 2D coordinates of the cortical surface can be used as the anatomical space and suggests using a cortical mesh, similar to that used in MEG source reconstruction. The cortical mesh is constructed from anatomical MRI and contains subject-specific anatomical information. A GPP based on such a diffusion kernel provides a way to formulate not only anatomically, but also functionally informed basis functions, thereby extending work by <xref rid="bib26" ref-type="bibr">Kiebel et al. (2000)</xref>.</p>
      <p>There is a growing literature on sparse GPPs for regression (<xref rid="bib31 bib41" ref-type="bibr">Lawrence, 2006; Quinonero-Candela and Rasmussen, 2005</xref>) used to formulate an approximate instead of a full GPP for use on large data-sets. Alternatively, multi-grid methods may be used (<xref rid="bib30" ref-type="bibr">Larin, 2004</xref>) or we can utilize the graphical structure of the model and apply graph-theoretic methods to optimally partition a graph into sub-graphs or nested graphs (<xref rid="bib52" ref-type="bibr">Tolliver et al., 2005</xref>). Recently, the computationally efficient properties of wavelets have been used to adaptively smooth fMRI parameter images (<xref rid="bib12" ref-type="bibr">Flandin and Penny, 2007</xref>). Diffusion wavelets are an established method for fast implementation of general diffusive processes (<xref rid="bib9 bib34" ref-type="bibr">Coifman and Maggioni, 2006; Maggioni and Mahadevan, 2006</xref>) and suggest an alternative implementation of the current method.</p>
      <p>The issue of inverting large matrices is avoided by using the mean-field approximation of <xref rid="bib38" ref-type="bibr">Penny et al. (2005)</xref>. The spatial precision matrix they used is equivalent to the Euclidean graph Laplacian used here. This encodes local information, given by the neighborhood of a vertex, which they use in a variational Bayes scheme to estimate scaling parameters for each regressor, given data. The spatial covariance matrix can be calculated from the matrix exponential of this, which requires consideration when dealing with large data-sets as outlined above. What we get in return is a full spatial covariance that encodes global properties of the graph and the possibility of multivariate statistics over anatomical space.</p>
      <p>As we focused on second level (between-subject) analyses the issue of temporal dynamics did not arise. However, for single-subject data this is not the case. A sensible approach would be to use a GLM to summarize temporal features in the signal and adaptively smooth over the vector-field of GLM regression coefficients, as mentioned above. Alternatively, a Kalman filter approach could be used; however, this may be more appropriate for EEG/MEG. The resulting algorithm would have a GPP for spatial signal with temporal covariance constrained by the Kalman filter.</p>
      <p>The application of the current method to random-effects analysis was for demonstration purposes only. The method may also be useful in modeling functional and structural data from lesion studies, retinotopic mapping in fMRI, high-resolution fMRI and diffusion tensor imaging (<xref rid="bib11 bib59" ref-type="bibr">Faugeras et al., 2004; Zhang and Hancock, 2006a</xref>).</p>
    </sec>
    <sec id="app1">
      <label>Appendix A</label>
      <title>Linear algebra for the <bold>EM</bold> scheme</title>
      <p>This appendix provides notes on the linear algebra used to compute the gradients and curvatures necessary for the <bold>EM</bold> scheme in the main text. They are not necessary to understand the results presented above but help optimize implementation.</p>
      <p>We require the bound on the log-marginal likelihood, ln <italic>p</italic>(<italic>y</italic>|<italic>λ</italic>)<disp-formula id="fd8"><label>(A.1)</label><mml:math id="M40" altimg="si44.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>F</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mi>Σ</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Σ</mml:mi><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and its derivatives. The first term of <xref rid="fd8" ref-type="disp-formula">A.1</xref> is<disp-formula id="fd9"><label>(A.2)</label><mml:math id="M41" altimg="si45.gif" overflow="scroll"><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mi>Σ</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mo>∏</mml:mo><mml:mo>|</mml:mo></mml:math></disp-formula>see Appendix of <xref rid="bib42" ref-type="bibr">Rasmussen and Williams (2006)</xref>. This can be reduced further using<disp-formula><label>(A.3)</label><mml:math id="M42" altimg="si46.gif" overflow="scroll"><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mi>rank</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>rank</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:math></disp-formula>The second term of <xref rid="fd8" ref-type="disp-formula">A.1</xref> is<disp-formula id="fd10"><label>(A.4)</label><mml:math id="M43" altimg="si47.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>A</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we have used vec(<italic>A</italic>)<sup><italic>T</italic></sup> vec(<italic>B</italic>) = <italic>tr</italic>(<italic>A</italic><sup><italic>T</italic></sup><italic>B</italic>) and <italic>εˆ</italic><sub>1</sub> = <italic>Y</italic> − <italic>Xθˆ</italic> is the matrix of prediction errors, where <italic>μ</italic> = vec(<italic>θˆ</italic>) (see Eq. <xref rid="fd7" ref-type="disp-formula">(19)</xref>).</p>
      <p>The conditional precision is<disp-formula><mml:math id="M44" altimg="si48.gif" overflow="scroll"><mml:mo>∏</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⊗</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></p>
      <p>Given the number of columns, <italic>P</italic>, of the design matrix used in this paper is one (i.e., a scalar field of parameter estimates) and <italic>K</italic><sub>2</sub> includes a scale term, <italic>υ</italic><sub>2</sub>, we can assume <italic>X</italic><sup><italic>T</italic></sup><italic>S</italic><sub>1</sub><sup>− 1</sup><italic>X</italic> = <italic>S</italic><sub>2</sub><sup>− 1</sup> (note that this is not the case in general). The conditional precision then factorizes and we avoid computing the Kronecker tensor products implicit in the <bold>EM</bold> scheme. The precision then simplifies to<disp-formula><label>(A.5)</label><mml:math id="M45" altimg="si49.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mo>∏</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>However, more generally, i.e., <italic>P</italic> &gt; 1<disp-formula><label>(A.6)</label><mml:math id="M46" altimg="si50.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mover><mml:mo>∏</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⊗</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we have used the matrix inversion lemma (see below), eigenvalue decomposition; <italic>Σ</italic><sub>2</sub> = <italic>V</italic><sub>2</sub><italic>D</italic><sub>2</sub><italic>V</italic><sub>2</sub><sup>−</sup> <sup>1</sup> and<disp-formula><label>(A.7)</label><mml:math id="M47" altimg="si51.gif" overflow="scroll"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>⊗</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>where <italic>V</italic> and <italic>D</italic> are eigenvectors and eigenvalues respectively of matrices A and B and in this case <italic>V</italic><sup>−</sup> <sup>1</sup> = <italic>V</italic><sup><italic>T</italic></sup>. Using <italic>S</italic><sub>1</sub> = <italic>I</italic><sub><italic>T</italic></sub> and <italic>S</italic><sub>2</sub> = 1 the conditional mean decomposes into the OLS estimate and a shrinkage term due to the priors.<disp-formula><label>(A.8)</label><mml:math id="M48" altimg="si52.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>μ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mtext>ols</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mtext>ols</mml:mtext></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>Σ</mml:mi><mml:mo stretchy="true">―</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mtext>ols</mml:mtext></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext>vec</mml:mtext><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>To compute the derivatives required for the <bold>M</bold>-Step, we use the matrix inversion lemma (MIL)<disp-formula><label>(A.9)</label><mml:math id="M49" altimg="si53.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mo>(</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mover><mml:mo>∏</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and standard results for Kronecker tensor products to show the score and expected information reduce to<disp-formula><label>(A.10)</label><mml:math id="M50" altimg="si54.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>The second line can be simplified further using <italic>tr</italic>(<italic>A⊗B</italic>) = <italic>tr</italic>(<italic>A</italic>)<italic>tr</italic>(<italic>B</italic>). Expressions for <italic>Ã</italic><sub><italic>k</italic></sub>, <italic>B</italic>˜<sub><italic>k</italic></sub>, <italic>A</italic><sub><italic>k</italic></sub>, <italic>B</italic><sub><italic>k</italic></sub>, <italic>C</italic><sub><italic>k</italic></sub>, and <italic>D</italic><sub><italic>k</italic></sub> are given in <xref rid="tbl3" ref-type="table">Table 3</xref>, where we have used <italic>X</italic><sup><italic>T</italic></sup><italic>S</italic><sub>1</sub><sup>− 1</sup><italic>X</italic> = <italic>S</italic><sub>2</sub><sup>− 1</sup> (special case where <italic>P</italic> = 1).</p>
    </sec>
    <sec id="app2">
      <label>Appendix B</label>
      <title>Computing the model evidence; accuracy and complexity</title>
      <p>Given <xref rid="fd9 fd10" ref-type="disp-formula">A.2 and A.4</xref> we can write the bound on the log-marginal likelihood as<disp-formula id="fd11"><label>(A.11)</label><mml:math id="M51" altimg="si55.gif" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mo>∏</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>After convergence of the <bold>EM</bold> scheme, this bound can be used as an approximation to the log-evidence, which can be expressed in terms of accuracy and complexity terms,<disp-formula><label>(A.12)</label><mml:math id="M52" altimg="si56.gif" overflow="scroll"><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:mo>∏</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>μ</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>ê</italic><sub>1</sub> = vec(<italic>εˆ</italic><sub>1</sub>). The first three terms of <xref rid="fd11" ref-type="disp-formula">A.12</xref> represent the accuracy and remaining terms complexity. To see the equivalence of <xref rid="fd11" ref-type="disp-formula">A.11</xref> and <xref rid="fd11" ref-type="disp-formula">A.12</xref> more clearly,<disp-formula><mml:math id="M53" altimg="si57.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>∏</mml:mo><mml:mi>μ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>∏</mml:mo><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>∏</mml:mo><mml:mi>μ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>μ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mi>μ</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>μ</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the MIL and Eq. <xref rid="fd7" ref-type="disp-formula">(19)</xref> are used in the first line and Eq. <xref rid="fd7" ref-type="disp-formula">(19)</xref> again in the second. To see the decomposition into accuracy and complexity terms consider the likelihood, prior and posterior of the parameters (<italic>p</italic>(<italic>y</italic>|<italic>w</italic>,<italic>λ</italic>), <italic>p</italic>(<italic>w</italic>) and <italic>q</italic>(<italic>w</italic>) respectively), note that<disp-formula><label>(A.13)</label><mml:math id="M54" altimg="si58.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>∫</mml:mo><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>w</mml:mi><mml:mtext>,</mml:mtext><mml:mi>λ</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>∫</mml:mo><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mi>ln</mml:mi><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>〈</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mi>〉</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">KL</mml:mi><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>〈</mml:mi><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mi>〉</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>e</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msup><mml:mo>∏</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">KL</mml:mi><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>(</mml:mo><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∏</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mo>∏</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where 〈<italic>L</italic>(<italic>w</italic>)〉<sub><italic>q</italic>(<italic>w</italic>)</sub> is the average log-likelihood under <italic>q</italic>(<italic>w</italic>) and the Kullback–Leibler (KL) divergence is a penalty term on the parameters. We have used standard results for Gaussian densities <italic>N</italic>(<italic>w</italic>; <italic>m</italic>, <italic>S</italic>)<disp-formula><label>(A.14)</label><mml:math id="M55" altimg="si59.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mtext>,</mml:mtext><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>W</mml:mi><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">KL</mml:mi><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>ln</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mo>)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the KL divergence is over two Gaussian densities <italic>q</italic>(<italic>w</italic>) ∼ <italic>N</italic>(<italic>μ</italic><sub>0</sub>, <italic>Σ</italic><sub>0</sub>) and <italic>p</italic>(<italic>w</italic>) ∼ <italic>N</italic>(<italic>μ</italic><sub>1</sub>, <italic>Σ</italic><sub>1</sub>), i.e., are the posterior and prior densities over the parameters.</p>
      <p>In practice, optimization of non-negative scale parameters in the <bold>M</bold>-Step uses the transform; <italic>γ</italic><sub><italic>i</italic></sub> = ln<italic>λ</italic><sub><italic>i</italic></sub>. The derivatives in <xref rid="tbl3" ref-type="table">Table 3</xref> are then ∂<italic>K</italic>/∂<italic>γ</italic><sub><italic>i</italic></sub> = <italic>λ</italic><sub><italic>i</italic></sub>∂<italic>K</italic>/∂<italic>λ</italic><sub><italic>i</italic></sub>. Under this change of variables, the hyperparameters have non-informative log-normal hyper-priors. Uncertainty about the hyperparameters can be included in the log-evidence for any model <italic>m</italic>. For example, the approximate log-evidence including uncertainty of one hyperparameter is<disp-formula><label>(A.15)</label><mml:math id="M56" altimg="si60.gif" overflow="scroll"><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">γ</mml:mi><mml:mtext>,</mml:mtext><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>ln</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi mathvariant="italic">γ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">γ</mml:mi><mml:mtext>,</mml:mtext><mml:mi>m</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>ln</mml:mi><mml:mi>I</mml:mi></mml:math></disp-formula>Where the second-order derivative is the expected information used in the <bold>M</bold>-Step. See <xref rid="bib16" ref-type="bibr">Friston et al. (2007)</xref> for details.</p>
    </sec>
    <sec id="app3">
      <label>Appendix C</label>
      <title>Metrics on manifolds</title>
      <p>The intuition behind the induced metric comes from considering Pythagoras' theorem. See <xref rid="fig1" ref-type="fig">Fig. 1</xref>c (inset).<disp-formula><label>(A.16)</label><mml:math id="M57" altimg="si61.gif" overflow="scroll"><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></disp-formula></p>
      <p>More formally, consider a one-dimensional curve embedded in two-dimensional Euclidean space. A map from one manifold, (<italic>M</italic>, <italic>g</italic>), to another, (<italic>N</italic>, <italic>h</italic>), where <italic>G</italic> and <italic>H</italic> are metrics associate with each respectively, is<disp-formula><label>(A.17)</label><mml:math id="M58" altimg="si62.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mo>χ</mml:mo><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi>N</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>χ</mml:mo><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>→</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mo>χ</mml:mo><mml:mn>1</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mtext>,</mml:mtext><mml:msup><mml:mo>χ</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mtext>,</mml:mtext><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Where <italic>u</italic> is a local coordinate on the curve and <italic>χ</italic><sup>1</sup> and <italic>χ</italic><sup>2</sup> are coordinates in the embedding space. A distance <italic>d</italic><sub><italic>s</italic></sub> on the curve in terms of d<italic>u</italic> is given by<disp-formula><label>(A.18)</label><mml:math id="M59" altimg="si63.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>H</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>J</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mo>χ</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>G</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>H</mml:mi><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.35em" height="0.3ex"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>G</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Where the relative scale between the domain and feature coordinates is <italic>a</italic> and <italic>G</italic> is the induced metric, i.e., metric on the curve.</p>
    </sec>
    <sec id="app4">
      <label>Appendix D</label>
      <title>Computing the graph Laplacian</title>
      <p>We assemble the graph Laplacian using a 3 × 3 stencil with 8 nearest neighbors. See <xref rid="fig1" ref-type="fig">Fig. 1</xref> showing how the distance between function values (parameter image) at different points in the embedded sub-manifold are represented by a graph with edge weights, <italic>w</italic><sub><italic>kn</italic></sub>. Computing the eigensystem of the graph Laplacian simplifies many computations; for example<disp-formula><label>(A.19)</label><mml:math id="M60" altimg="si64.gif" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>Λ</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mtext>diag</mml:mtext><mml:mo>(</mml:mo><mml:mi>Λ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>,</mml:mtext><mml:mo>…</mml:mo><mml:mtext>,</mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="italic">, where</mml:mi><mml:mspace width="0.35em" height="0.3ex"/><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mi>Λ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mi>Λ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>The third line affords a way to compute the matrix exponential (<xref rid="bib35" ref-type="bibr">Moler and Van Loan, 2003</xref>). The issue with updating the graph Laplacian with each iteration based on the posterior mean of the parameters is that it entails a composition of non-commuting matrices. An approximation is possible using a truncation of the Baker–Campbell–Hausdoff (BCH) formula (<xref rid="bib45" ref-type="bibr">Rossmann, 2002</xref>).<disp-formula><label>(A.20)</label><mml:math id="M61" altimg="si65.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>≈</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>[</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mtext>,</mml:mtext><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we have used the first three terms of the BCH formula;<disp-formula><label>(A.21)</label><mml:math id="M62" altimg="si66.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>C</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>log</mml:mi><mml:mo>(</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi></mml:mtd><mml:mtd><mml:mo>≈</mml:mo></mml:mtd><mml:mtd><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>[</mml:mo><mml:mi>A</mml:mi><mml:mtext>,</mml:mtext><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>[<italic>A</italic>, <italic>B</italic>] is the commutator of operators, <italic>A</italic> and <italic>B</italic> given by [<italic>A</italic>, <italic>B</italic>] = <italic>AB</italic> − <italic>BA</italic>.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgment</title>
      <p>The Wellcome Trust and British Council funded this work.</p>
    </ack>
    <fn-group>
      <fn id="fn1">
        <label>1</label>
        <p>Element-wise exponential as opposed to matrix exponential used in <xref rid="fd4" ref-type="disp-formula">Eq. (6)</xref>.</p>
      </fn>
      <fn id="fn2">
        <label>2</label>
        <p>This means that the vectorized random matrix has a multivariate normal density, given by vec(<italic>A<sup>T</sup></italic>) ∼ <italic>N</italic>(vec(<italic>M<sup>T</sup></italic>),<italic>S</italic>⊗<italic>K</italic>) and vec(<italic>A</italic>) ∼ <italic>N</italic>(vec(<italic>M</italic>),<italic>K</italic>⊗<italic>S</italic>).</p>
      </fn>
      <fn id="fn3">
        <label>3</label>
        <p>Minus sign used by convention.</p>
      </fn>
      <fn id="fn4">
        <label>4</label>
        <p>This is equivalent to a Newton step, but using the expected curvature as opposed to the local curvature of the objective function.</p>
      </fn>
    </fn-group>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Alvarez</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lions</surname>
              <given-names>P.L.</given-names>
            </name>
            <name>
              <surname>Morel</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>Image selective smoothing and edge-detection by nonlinear diffusion: 2</article-title>
          <source>SIAM J. Numer. Anal.</source>
          <year>1992</year>
          <volume>29</volume>
          <fpage>845</fpage>
          <lpage>866</lpage>
        </citation>
      </ref>
      <ref id="bib2">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Begelfor</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Werman</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>How to put probabilities on homographies</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
          <year>2005</year>
          <volume>27</volume>
          <fpage>1666</fpage>
          <lpage>1670</lpage>
          <pub-id pub-id-type="pmid">16238000</pub-id>
        </citation>
      </ref>
      <ref id="bib3">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bishop</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Neural Networks for Pattern Recognition</article-title>
          <year>1995</year>
          <publisher-name>Oxford University Press</publisher-name>
          <publisher-loc>Oxford</publisher-loc>
        </citation>
      </ref>
      <ref id="bib4">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bishop</surname>
              <given-names>C.M.</given-names>
            </name>
          </person-group>
          <article-title>Latent variable models</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Jordan</surname>
              <given-names>M.I.</given-names>
            </name>
          </person-group>
          <source>Learning in Graphical Models</source>
          <year>1999</year>
          <publisher-name>MIT Press</publisher-name>
          <publisher-loc>Massachusetts</publisher-loc>
          <fpage>371</fpage>
          <lpage>403</lpage>
        </citation>
      </ref>
      <ref id="bib5">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chefd'hotel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tschumperle</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Deriche</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Faugeras</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Constrained flows of matrix-valued functions: application to diffusion tensor regularization</article-title>
          <source>Comput. Vis. - Eccv</source>
          <year>2002</year>
          <volume>2350</volume>
          <issue>Pt 1</issue>
          <fpage>251</fpage>
          <lpage>265</lpage>
        </citation>
      </ref>
      <ref id="bib6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>ChefD'Hotel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tschumperle</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Deriche</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Faugeras</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Regularizing flows for constrained matrix-valued images</article-title>
          <source>J. Math. Imaging Vis.</source>
          <year>2004</year>
          <volume>20</volume>
          <fpage>147</fpage>
          <lpage>162</lpage>
        </citation>
      </ref>
      <ref id="bib7">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chung</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Spectral Graph Theory</article-title>
          <year>1991</year>
          <publisher-name>American Mathematics Society</publisher-name>
          <publisher-loc>Providence Rhode Island</publisher-loc>
        </citation>
      </ref>
      <ref id="bib8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chung</surname>
              <given-names>M.K.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Robbins</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Paus</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Taylor</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Giedd</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Rapoport</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>Deformation-based surface morphometry applied to gray matter deformation</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>18</volume>
          <fpage>198</fpage>
          <lpage>213</lpage>
          <pub-id pub-id-type="pmid">12595176</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Coifman</surname>
              <given-names>R.R.</given-names>
            </name>
            <name>
              <surname>Maggioni</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Diffusion wavelets</article-title>
          <source>Appl. Comput. Harmon. Anal.</source>
          <year>2006</year>
          <volume>21</volume>
          <fpage>53</fpage>
          <lpage>94</lpage>
        </citation>
      </ref>
      <ref id="bib10">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cosman</surname>
              <given-names>E.R.</given-names>
            </name>
            <name>
              <surname>Fisher</surname>
              <given-names>J.W.</given-names>
            </name>
            <name>
              <surname>Wells</surname>
              <given-names>W.M.</given-names>
            </name>
          </person-group>
          <article-title>Exact MAP activity detection in fMRI using a GLM with an Ising spatial prior</article-title>
          <source>Med. Image Comput. Comput. -Assist. Interv. - Miccai</source>
          <year>2004</year>
          <volume>3217</volume>
          <issue>Pt 2</issue>
          <fpage>703</fpage>
          <lpage>710</lpage>
          <comment>(Proceedings)</comment>
        </citation>
      </ref>
      <ref id="bib11">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Faugeras</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Adde</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Charpiat</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Chefd'Hotel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Clerc</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Deneux</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Deriche</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hermosillo</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Keriven</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kornprobst</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Variational, geometric, and statistical methods for modeling brain anatomy and function</article-title>
          <source>NeuroImage</source>
          <year>2004</year>
          <volume>23</volume>
          <fpage>S46</fpage>
          <lpage>S55</lpage>
          <pub-id pub-id-type="pmid">15501100</pub-id>
        </citation>
      </ref>
      <ref id="bib12">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Flandin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian fMRI data analysis with sparse spatial basis function priors</article-title>
          <source>NeuroImage</source>
          <year>2007</year>
          <volume>34</volume>
          <fpage>1108</fpage>
          <lpage>1125</lpage>
          <pub-id pub-id-type="pmid">17157034</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friman</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Borga</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lundberg</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Knutsson</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Adaptive analysis of fMRI data</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>19</volume>
          <fpage>837</fpage>
          <lpage>845</lpage>
          <pub-id pub-id-type="pmid">12880812</pub-id>
        </citation>
      </ref>
      <ref id="bib14">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Posterior probability maps and SPMs</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>19</volume>
          <fpage>1240</fpage>
          <lpage>1249</lpage>
          <pub-id pub-id-type="pmid">12880849</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Phillips</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Classical and Bayesian inference in neuroimaging: theory</article-title>
          <source>NeuroImage</source>
          <year>2002</year>
          <volume>16</volume>
          <fpage>465</fpage>
          <lpage>483</lpage>
          <pub-id pub-id-type="pmid">12030832</pub-id>
        </citation>
      </ref>
      <ref id="bib16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mattout</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Variational free energy and the Laplace approximation</article-title>
          <source>NeuroImage</source>
          <year>2007</year>
          <volume>34</volume>
          <fpage>220</fpage>
          <lpage>234</lpage>
          <pub-id pub-id-type="pmid">17055746</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gerig</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Kubler</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Kikinis</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Jolesz</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Nonlinear anisotropic filtering of MRI data</article-title>
          <source>IEEE Trans. Med. Imag.</source>
          <year>1992</year>
          <volume>11</volume>
          <fpage>221</fpage>
          <lpage>232</lpage>
        </citation>
      </ref>
      <ref id="bib18">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Goldberg</surname>
              <given-names>P.W.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>C.K.I.</given-names>
            </name>
            <name>
              <surname>Bishop</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Regression with input-dependent noise: a Gaussian process treatment</article-title>
          <source>NIPS 10</source>
          <year>1998</year>
          <publisher-name>MIT Press</publisher-name>
        </citation>
      </ref>
      <ref id="bib19">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gossl</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Auer</surname>
              <given-names>D.P.</given-names>
            </name>
            <name>
              <surname>Fahrmeir</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian spatiotemporal inference in functional magnetic resonance imaging</article-title>
          <source>Biometrics</source>
          <year>2001</year>
          <volume>57</volume>
          <fpage>554</fpage>
          <lpage>562</lpage>
          <pub-id pub-id-type="pmid">11414583</pub-id>
        </citation>
      </ref>
      <ref id="bib20">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Grady</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names>E.L.</given-names>
            </name>
          </person-group>
          <article-title>The Graph Analysis Toolbox: Image Processing on Arbitrary Graphs</article-title>
          <year>2003</year>
          <publisher-name>Boston University</publisher-name>
          <publisher-loc>Boston, MA</publisher-loc>
        </citation>
      </ref>
      <ref id="bib21">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>A.K.</given-names>
            </name>
            <name>
              <surname>Nagar</surname>
              <given-names>D.K.</given-names>
            </name>
          </person-group>
          <article-title>Matrix Variate Distributions</article-title>
          <year>2000</year>
          <publisher-name>Chapman and Hall/CRC</publisher-name>
          <publisher-loc>Boca Raton</publisher-loc>
        </citation>
      </ref>
      <ref id="bib22">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Harrison</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Stephan</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Extra-classical receptive field effects measured in striate cortex with fMRI</article-title>
          <source>NeuroImage</source>
          <year>2007</year>
          <volume>34</volume>
          <issue>3</issue>
          <fpage>1199</fpage>
          <lpage>1208</lpage>
          <comment>(Feb 1)</comment>
          <pub-id pub-id-type="pmid">17169579</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Harville</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Matrix Algebra from A Statistician's Perspective</article-title>
          <year>1997</year>
          <publisher-name>Springer Science+Business Media Inc</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
      <ref id="bib24">
        <citation citation-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Jordan</surname>
              <given-names>M.I.</given-names>
            </name>
          </person-group>
          <source>Learning in Graphical Models</source>
          <year>1999</year>
          <publisher-name>The MIT press</publisher-name>
        </citation>
      </ref>
      <ref id="bib25">
        <citation citation-type="book"><person-group person-group-type="author"><name><surname>Kersting</surname><given-names>K.</given-names></name><name><surname>Plagemann</surname><given-names>C.</given-names></name><name><surname>Pfaff</surname><given-names>P.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name></person-group><article-title>Most likely heteroscedastic Gaussian process regression</article-title>International Conference on Machine Learning<year>2007</year></citation>
      </ref>
      <ref id="bib26">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kiebel</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Anatomically informed basis functions</article-title>
          <source>NeuroImage</source>
          <year>2000</year>
          <volume>11</volume>
          <fpage>656</fpage>
          <lpage>667</lpage>
          <pub-id pub-id-type="pmid">10860794</pub-id>
        </citation>
      </ref>
      <ref id="bib27">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>Z.H.</given-names>
            </name>
          </person-group>
          <article-title>Robust anisotropic diffusion to produce clear statistical parametric map from noisy fMRI</article-title>
          <source>Proceedings of the 15th Brazilian Symposium on Computer Graphics and Image Processing</source>
          <year>2002</year>
          <fpage>11</fpage>
          <lpage>17</lpage>
        </citation>
      </ref>
      <ref id="bib28">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Javier</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>Z.H.</given-names>
            </name>
          </person-group>
          <article-title>Robust anisotropic diffusion to produce enhanced statistical parametric map from noisy fMRI</article-title>
          <source>Comput. Vis. Image Underst.</source>
          <year>2005</year>
          <volume>99</volume>
          <fpage>435</fpage>
          <lpage>452</lpage>
        </citation>
      </ref>
      <ref id="bib29">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Knutsson</surname>
              <given-names>H.E.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Granlund</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Anisotropic nonstationary image estimation and its applications: 1. Restoration of noisy images</article-title>
          <source>IEEE Trans. Commun.</source>
          <year>1983</year>
          <volume>31</volume>
          <fpage>388</fpage>
          <lpage>397</lpage>
        </citation>
      </ref>
      <ref id="bib30">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Larin</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>On a multigrid method for solving partial eigenproblems</article-title>
          <source>Sib. J. Numer. Math.</source>
          <year>2004</year>
          <volume>7</volume>
          <fpage>25</fpage>
          <lpage>42</lpage>
        </citation>
      </ref>
      <ref id="bib31">
        <citation citation-type="other">Lawrence, N., 2006. Large scale learning with the Gaussian process latent variable model (Technical Report No. CS-06-05. University of Sheffield).</citation>
      </ref>
      <ref id="bib32">
        <citation citation-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>MacKay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <source>Introduction to Gaussian Processes, Neural Networks and Machine Learning</source>
          <year>1998</year>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin</publisher-loc>
        </citation>
      </ref>
      <ref id="bib33">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>MacKay</surname>
              <given-names>D.J.C.</given-names>
            </name>
          </person-group>
          <article-title>Information Theory, Inference, and Learning Algorithms</article-title>
          <year>2003</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
        </citation>
      </ref>
      <ref id="bib34">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Maggioni</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mahadevan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A Multiscale Framework For Markov Decision Processes Using Diffusion Wavelets</article-title>
          <year>2006</year>
          <publisher-name>University of Massachusetts</publisher-name>
          <publisher-loc>Massachusetts</publisher-loc>
        </citation>
      </ref>
      <ref id="bib35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moler</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Van Loan</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later</article-title>
          <source>SIAM Rev.</source>
          <year>2003</year>
          <volume>45</volume>
          <fpage>3</fpage>
          <lpage>49</lpage>
        </citation>
      </ref>
      <ref id="bib36">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Random-effects analysis</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zeki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <source>Human Brain Function</source>
          <year>2003</year>
          <publisher-name>Elseiver Science (USA)</publisher-name>
          <publisher-loc>San Diego, California</publisher-loc>
        </citation>
      </ref>
      <ref id="bib37">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Kiebel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Variational Bayesian inference for fMRI time series</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>19</volume>
          <fpage>727</fpage>
          <lpage>741</lpage>
          <pub-id pub-id-type="pmid">12880802</pub-id>
        </citation>
      </ref>
      <ref id="bib38">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian fMRI time series analysis with spatial priors</article-title>
          <source>NeuroImage</source>
          <year>2005</year>
          <volume>24</volume>
          <fpage>350</fpage>
          <lpage>362</lpage>
          <pub-id pub-id-type="pmid">15627578</pub-id>
        </citation>
      </ref>
      <ref id="bib39">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penny</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Flandin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Trujillo-Barreto</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian comparison of spatially regularised general linear models</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2007</year>
          <volume>28</volume>
          <fpage>275</fpage>
          <lpage>293</lpage>
          <pub-id pub-id-type="pmid">17133400</pub-id>
        </citation>
      </ref>
      <ref id="bib40">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Perona</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Malik</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Scale-space and edge-detection using anisotropic diffusion</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
          <year>1990</year>
          <volume>12</volume>
          <fpage>629</fpage>
          <lpage>639</lpage>
        </citation>
      </ref>
      <ref id="bib41">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quinonero-Candela</surname>
              <given-names>J.Q.</given-names>
            </name>
            <name>
              <surname>Rasmussen</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <article-title>A unifying view of sparse approximate Gaussian process regression</article-title>
          <source>J. Mach. Learn. Res.</source>
          <year>2005</year>
          <volume>6</volume>
          <fpage>1939</fpage>
          <lpage>1959</lpage>
        </citation>
      </ref>
      <ref id="bib42">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rasmussen</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Gaussian Processes for Machine Learning</article-title>
          <year>2006</year>
          <publisher-name>The MIT Press</publisher-name>
          <publisher-loc>Massachusetts, Cambridge</publisher-loc>
        </citation>
      </ref>
      <ref id="bib43">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Romeny</surname>
              <given-names>B.M.T.</given-names>
            </name>
          </person-group>
          <article-title>Geometry-driven Diffusion in Computer Vision</article-title>
          <year>1994</year>
          <publisher-name>Kluwer Academic Publishers</publisher-name>
        </citation>
      </ref>
      <ref id="bib44">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Romeny</surname>
              <given-names>B.M.T.</given-names>
            </name>
          </person-group>
          <article-title>Front-end Vision and Multi-Scale Image Analysis</article-title>
          <year>2003</year>
          <publisher-name>Springer</publisher-name>
        </citation>
      </ref>
      <ref id="bib45">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rossmann</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Lie Groups: An Introduction through Linear Groups</article-title>
          <year>2002</year>
          <publisher-name>Oxford University Press</publisher-name>
          <publisher-loc>Oxford</publisher-loc>
        </citation>
      </ref>
      <ref id="bib46">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Sethian</surname>
              <given-names>J.A.</given-names>
            </name>
          </person-group>
          <article-title>Level Set Methods and Fast Marching Methods Evolving Interfaces in Computational Geometry, Fluid Mechanics, Computer Vision, and Materials Science</article-title>
          <year>1999</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge</publisher-loc>
        </citation>
      </ref>
      <ref id="bib47">
        <citation citation-type="book"><person-group person-group-type="author"><name><surname>Sochen</surname><given-names>N.</given-names></name><name><surname>Zeevi</surname><given-names>Y.Y.</given-names></name></person-group><article-title>Representation of colored images by manifolds embedded in higher dimensional non-euclidean space</article-title>International Conference on Image Processing<year>1998</year></citation>
      </ref>
      <ref id="bib48">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sochen</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Kimmel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Malladi</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>A general framework for low level vision</article-title>
          <source>IEEE Trans. Image Process.</source>
          <year>1998</year>
          <volume>7</volume>
          <fpage>310</fpage>
          <lpage>318</lpage>
          <pub-id pub-id-type="pmid">18276251</pub-id>
        </citation>
      </ref>
      <ref id="bib49">
        <citation citation-type="book"><person-group person-group-type="author"><name><surname>Sochen</surname><given-names>N.</given-names></name><name><surname>Deriche</surname><given-names>R.</given-names></name><name><surname>Lucero-Lopez</surname><given-names>P.</given-names></name></person-group><article-title>The Beltrami flow over implicit manifolds</article-title>Proceedings of the Ninth IEEE International Conference on Computer Vision (ICCV)<year>2003</year></citation>
      </ref>
      <ref id="bib50">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Spielman</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Teng</surname>
              <given-names>S.H.</given-names>
            </name>
          </person-group>
          <article-title>Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems</article-title>
          <source>Proceedings of the 36th Annual ACM Symposium on Theory of Computing</source>
          <year>2004</year>
          <fpage>81</fpage>
          <lpage>90</lpage>
        </citation>
      </ref>
      <ref id="bib51">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Teo</surname>
              <given-names>P.C.</given-names>
            </name>
            <name>
              <surname>Sapiro</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wandell</surname>
              <given-names>B.A.</given-names>
            </name>
          </person-group>
          <article-title>Creating connected representations of cortical gray matter for functional MRI visualization</article-title>
          <source>IEEE Trans. Med. Imag.</source>
          <year>1997</year>
          <volume>16</volume>
          <fpage>852</fpage>
          <lpage>863</lpage>
        </citation>
      </ref>
      <ref id="bib52">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tolliver</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Baker</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Collins</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Multilevel Spectral Partitioning for Efficient Image Segmentation and Tracking</article-title>
          <year>2005</year>
        </citation>
      </ref>
      <ref id="bib53">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tschumperle</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Deriche</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>DT-MRI images: estimation, regularization, and application</article-title>
          <source>Comput. Aided Syst. Theor. - Eurocast</source>
          <year>2003</year>
          <volume>2809</volume>
          <fpage>530</fpage>
          <lpage>541</lpage>
        </citation>
      </ref>
      <ref id="bib54">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wand</surname>
              <given-names>M.P.</given-names>
            </name>
          </person-group>
          <article-title>Vector differential calculus in statistics</article-title>
          <source>Am. Stat.</source>
          <year>2002</year>
          <volume>56</volume>
          <fpage>55</fpage>
          <lpage>62</lpage>
        </citation>
      </ref>
      <ref id="bib55">
        <citation citation-type="other">Weickert, J., 1996. Anisotropic diffusion in image processing.</citation>
      </ref>
      <ref id="bib56">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>Fully Bayesian spatio-temporal modeling of FMRI data</article-title>
          <source>IEEE Trans. Med. Imag.</source>
          <year>2004</year>
          <volume>23</volume>
          <fpage>213</fpage>
          <lpage>231</lpage>
        </citation>
      </ref>
      <ref id="bib57">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Andermann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Koulis</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>MacDonald</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>Detecting changes in nonisotropic images</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>8</volume>
          <fpage>98</fpage>
          <lpage>101</lpage>
          <pub-id pub-id-type="pmid">10524599</pub-id>
        </citation>
      </ref>
      <ref id="bib58">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hancock</surname>
              <given-names>E.R.</given-names>
            </name>
          </person-group>
          <article-title>Image scale-space from the heat kernel</article-title>
          <source>Progr. Pattern Recogn. Image Anal. Applic. Proc.</source>
          <year>2005</year>
          <volume>3773</volume>
          <fpage>181</fpage>
          <lpage>192</lpage>
        </citation>
      </ref>
      <ref id="bib59">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hancock</surname>
              <given-names>E.R.</given-names>
            </name>
          </person-group>
          <article-title>Riemannian graph diffusion for DT-MRI regularization</article-title>
          <source>Med. Image Comput. Comput. - Assist. Interv.—Miccai</source>
          <year>2006</year>
          <volume>4191</volume>
          <issue>Pt 2</issue>
          <fpage>234</fpage>
          <lpage>242</lpage>
        </citation>
      </ref>
      <ref id="bib60">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hancock</surname>
              <given-names>E.R.</given-names>
            </name>
          </person-group>
          <article-title>Smoothing tensor-valued images using anisotropic geodesic diffusion</article-title>
          <source>Struct. Syntact. Stat. Pattern Recogn. Proc.</source>
          <year>2006</year>
          <volume>4109</volume>
          <fpage>83</fpage>
          <lpage>91</lpage>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>GLM parameters as a function over physical space: (a) parameter estimates, considered as a function, <italic>μ</italic>(<italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>), over a 2D regular mesh, is thought of as a 2D sub-manifold embedded within a 3D space comprising ‘anatomical’ and ‘feature’ space coordinates. This is shown using parameter estimates from the synthetic data-set described in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. The inset shows parameter estimates over a 3 × 3 stencil in physical space. Coordinates of the function in <mml:math id="M63" altimg="si1.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:math> are (<italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>, <italic>μ</italic>(<italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>)). (b) A graph <italic>Γ</italic> is composed of vertices, edges and weights <italic>Γ</italic> = (<italic>V</italic>, <italic>E</italic>, <italic>W</italic>), shown for the 3 × 3 stencil in panel a, where the weight of an edge coupling the <italic>k</italic><sup>th</sup> and <italic>n</italic><sup>th</sup> vertices is <italic>w</italic><sub><italic>kn</italic></sub>. (c) Graph weights are a function of distance, <italic>d</italic><sub><italic>s</italic></sub> (example shown in red and see Eq. <xref rid="fd5" ref-type="disp-formula">(13)</xref>), on <italic>μ</italic>(<italic>u</italic><sup>1</sup>, <italic>u</italic><sup>2</sup>), such that <italic>w</italic><sub><italic>kn</italic></sub> ∈ [0, 1]. Note that the feature dimension is scaled by a (equivalent to <mml:math id="M64" altimg="si3.gif" overflow="scroll"><mml:msub><mml:msqrt><mml:mi>a</mml:mi></mml:msqrt><mml:mi>μ</mml:mi></mml:msub></mml:math> in main text), meaning that if <italic>a</italic> = 0 ⇒ <italic>d</italic><sub><italic>s</italic></sub> = d<italic>u</italic>, in which case the weights are independent of image intensity.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Generative model: single observations of an image are generated from the GPP, <italic>p</italic>(vec(<italic>θ</italic>)|<italic>λ</italic>) = <italic>N</italic>(0, <italic>Σ</italic><sub>2</sub>), parameterized by <italic>υ</italic><sub>2</sub>,<italic>τ</italic>. Multiple observations are collected in the matrix <italic>Y</italic>. <italic>X</italic> is the design matrix and the noise model is an i.i.d. Gaussian distribution, <italic>p</italic>(vec(<italic>ε</italic><sub>1</sub>)) = <italic>N</italic>(0,<italic>υ</italic><sub>1</sub><italic>I</italic><sub><italic>TN</italic></sub>).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>Synthetic data — de-noising an image: (a) a binary image with added noise is shown in the central panel (see grey-scale for pixel values). GPMs using diffusion kernels from EGL and GGL are shown on left and right respectively. On the left, noise is removed along with the edge of the original image. On the right, noise has been removed, while preserving the edge of the original object. (b) The left and right images of (a) with contour plots of local diffusion kernels from three locations overlaid. These are the same for kernels from EGL; however, is not the case for GGL. Local kernels adapt to the edge of the central image. (c) Cross-section at the level indicated in panel b. This shows the original binary image, noisy image, smoothing with EGL and GGL. For GGL, the noisy image is preferentially smoothed on flat regions. (d) Two representations of the induced metric tensor, <italic>G</italic>; square-root of determinant (area ratio of surface:domain) and orientation/magnitude (eigenvectors/values). Large ratio and aligned, smaller ellipses at the image edge is associated with reduced flow across the edge. (e and f) Global property of EGL and GGL shown using gplot.m (Matlab routine) of second and third eigenvectors of graph Laplacians. Symmetry is due to circular domain of image.</p>
      </caption>
      <graphic xlink:href="gr3ac"/>
      <graphic xlink:href="gr3df"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Synthetic data — random effects analysis: (a) twelve samples of the binary image shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>, (b) original image and OLS estimate, (c) estimated conditional means using GSP, EGL and GGL, (d) posterior probability maps at thresholds, <italic>p</italic>(<italic>w</italic> &gt; 0.33) &gt; 0.95.</p>
      </caption>
      <graphic xlink:href="gr4ab"/>
      <graphic xlink:href="gr4cd"/>
    </fig>
    <fig id="fig5">
      <label>Fig. 5</label>
      <caption>
        <p>Real data: twelve contrast images of a slice showing bilateral response in posterior cingulated gryi (pCG) during a study of coherent motion (<xref rid="bib22" ref-type="bibr">Harrison et al., 2007</xref>). (a) Twelve samples (b) estimated conditional means using EGL and GGL (left and right). (c) Inset of panel b with contour plot of a local diffusion kernel overlaid. Distinguishing borders between regions of high/low parameter estimates is difficult due to smoothing by the EGL. However, borders are easily seen on the right. (d) Posterior probability maps; where white regions indicate <italic>p</italic>(<italic>w</italic> &gt; 0.5) &gt; 0.95. (e) Inset of panel d for GSP, EGL and GGL. Active regions using EGL are characterized by rounded edges, i.e., blobs, while for GGL the shape of bilateral response are elongated in fitting with the anatomy of pCG, (f–h) surface plots of conditional means from inset. Note vertical scale, especially for GSP, which shows large shrinkage compared to EGL and GGL, (i and h) graph plot (gplot.m) of second and third eigenvectors of EGL and GGL. Heterogeneous graph weights of GGL are easily observed compared to EGL.</p>
      </caption>
      <graphic xlink:href="gr5ac"/>
      <graphic xlink:href="gr5df"/>
      <graphic xlink:href="gr5gh"/>
      <graphic xlink:href="gr5ij"/>
    </fig>
    <table-wrap position="float" id="tbl1">
      <label>Table 1</label>
      <caption>
        <p>Model comparison for synthetic data shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>: fixed parameters and log evidence (natural logarithm) for EGL and GGL (difference shown in parentheses)</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top">Covariance</th>
            <th valign="top">Fixed parameters</th>
            <th valign="top">Log evidence</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top">EGL</td>
            <td valign="top"><italic>a</italic><sub>1</sub> = <italic>a</italic><sub>2</sub> = 1</td>
            <td align="char" valign="top">159.97</td>
          </tr>
          <tr>
            <td valign="top">GGL</td>
            <td valign="top"><italic>a</italic><sub>1</sub> = <italic>a</italic><sub>2</sub> = 1<italic>a</italic><sub><italic>μ</italic></sub> = 2</td>
            <td align="char" valign="top">420.10 (260)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="tbl2">
      <label>Table 2</label>
      <caption>
        <p>Model comparison for synthetic (<xref rid="fig4" ref-type="fig">Fig. 4</xref>) and real data (<xref rid="fig5" ref-type="fig">Fig. 5</xref>)</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top">Covariance</th>
            <th valign="top">Fixed parameters</th>
            <th valign="top">Synthetic data</th>
            <th valign="top">Real data</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top">GSP</td>
            <td valign="top"><italic>a</italic><sub>1</sub> = <italic>a</italic><sub>2</sub> = 1</td>
            <td align="char" valign="top">− 3.3325 × 10<sup>3</sup></td>
            <td align="char" valign="top">− 1.0371 × 10<sup>5</sup></td>
          </tr>
          <tr>
            <td valign="top">EGL</td>
            <td valign="top"><italic>a</italic><sub>1</sub> = <italic>a</italic><sub>2</sub> = 1</td>
            <td align="char" valign="top">− 3.0518 × 10<sup>3</sup></td>
            <td align="char" valign="top">− 1.0246 × 10<sup>5</sup></td>
          </tr>
          <tr>
            <td valign="top">GGL</td>
            <td valign="top"><italic>a</italic><sub>1</sub> = <italic>a</italic><sub>2</sub> = 1<italic>a</italic><sub><italic>μ</italic></sub> = 2</td>
            <td align="char" valign="top">− 2.9054 × 10<sup>3</sup> (146)</td>
            <td align="char" valign="top">− 1.0220 × 10<sup>5</sup> (260)</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Fixed parameters and log evidence for GSP, EGL and GGL (difference between GGL and EGL shown in parentheses).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl3">
      <label>Table 3</label>
      <caption>
        <p>Expressions used to compute log-marginal likelihood and its derivatives</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th colspan="4" valign="top">A<hr/></th>
          </tr>
          <tr>
            <th valign="top">
              <italic>Ã</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </th>
            <th valign="top">
              <italic>B˜</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </th>
            <th valign="top"/>
            <th valign="top"/>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top">∂<italic>K</italic><sub>1</sub>/∂<italic>υ</italic><sub>1</sub></td>
            <td valign="top">
              <italic>S</italic>
              <sub>1</sub>
            </td>
            <td valign="top"/>
            <td valign="top"/>
          </tr>
          <tr>
            <td valign="top">∂<italic>K</italic><sub>2</sub>/∂<italic>υ</italic><sub>2</sub></td>
            <td valign="top">
              <italic>XS</italic>
              <sub>2</sub>
              <italic>X</italic>
              <sup>
                <italic>T</italic>
              </sup>
            </td>
            <td valign="top"/>
            <td valign="top"/>
          </tr>
          <tr>
            <td valign="top">∂<italic>K</italic><sub>2</sub>/∂<italic>τ</italic></td>
            <td valign="top">
              <italic>XS</italic>
              <sub>2</sub>
              <italic>X</italic>
              <sup>
                <italic>T</italic>
              </sup>
            </td>
            <td valign="top"/>
            <td valign="top"/>
          </tr>
          <tr>
            <td colspan="4" valign="top"/>
          </tr>
          <tr>
            <td colspan="4" valign="top">B</td>
          </tr>
          <tr>
            <td colspan="4">
              <hr/>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <italic>A</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
            <td valign="top">
              <italic>B</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
            <td valign="top">
              <italic>C</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
            <td valign="top">
              <italic>D</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
          </tr>
          <tr>
            <td colspan="4">
              <hr/>
            </td>
          </tr>
          <tr>
            <td valign="top">
              <italic>K</italic>
              <sub>1</sub>
              <sup>− 1</sup>
              <italic>Ã</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
            <td valign="top">
              <italic>S</italic>
              <sub>1</sub>
              <italic><sup>−1</sup>B˜<sub>k</sub></italic>
            </td>
            <td valign="top">
              <italic>K</italic>
              <sub>1</sub>
              <sup>−1</sup>
              <italic>K¯K</italic>
              <sub>1</sub>
              <sup>− 1</sup>
              <italic>Ã</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
            <td valign="top">
              <italic>S</italic>
              <sub>1</sub>
              <sup>− 1</sup>
              <italic>XS¯X</italic>
              <sup>
                <italic>T</italic>
              </sup>
              <italic>S</italic>
              <sub>1</sub>
              <sup>− <sup>1</sup></sup>
              <italic>B˜</italic>
              <sub>
                <italic>k</italic>
              </sub>
            </td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-wrap>
</article>