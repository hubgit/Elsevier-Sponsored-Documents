<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuropsychologia</journal-id>
      <journal-title>Neuropsychologia</journal-title>
      <issn pub-type="ppub">0028-3932</issn>
      <issn pub-type="epub">1873-3514</issn>
      <publisher>
        <publisher-name>Pergamon Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2833414</article-id>
      <article-id pub-id-type="pmid">20006628</article-id>
      <article-id pub-id-type="publisher-id">NSY3511</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.12.011</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Progressive associative phonagnosia: A neuropsychological analysis</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Hailstone</surname>
            <given-names>Julia C.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Crutch</surname>
            <given-names>Sebastian J.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Vestergaard</surname>
            <given-names>Martin D.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Patterson</surname>
            <given-names>Roy D.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Warren</surname>
            <given-names>Jason D.</given-names>
          </name>
          <email>jwarren@drc.ion.ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>a</sup>Dementia Research Centre, Institute of Neurology, University College London, Queen Square, London WC1N 3BG, United Kingdom</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>b</sup>Centre for the Neural Basis of Hearing, Physiology Department, University of Cambridge, Cambridge, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. Tel.: +44 0207 829 8773; fax: +44 0207 676 2066. <email>jwarren@drc.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <month>3</month>
        <year>2010</year>
      </pub-date>
      <volume>48</volume>
      <issue>4</issue>
      <fpage>1104</fpage>
      <lpage>1114</lpage>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>9</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>23</day>
          <month>11</month>
          <year>2009</year>
        </date>
        <date date-type="accepted">
          <day>7</day>
          <month>12</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â© 2010 Elsevier Ltd.</copyright-statement>
        <copyright-year>2009</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Abstract</title>
        <p>There are few detailed studies of impaired voice recognition, or phonagnosia. Here we describe two patients with progressive phonagnosia in the context of frontotemporal lobar degeneration. Patient QR presented with behavioural decline and increasing difficulty recognising familiar voices, while patient KL presented with progressive prosopagnosia. In a series of neuropsychological experiments we assessed the ability of QR and KL to recognise and judge the familiarity of voices, faces and proper names, to recognise vocal emotions, to perceive and discriminate voices, and to recognise environmental sounds and musical instruments. The patients were assessed in relation to a group of healthy age-matched control subjects. QR exhibited severe impairments of voice identification and familiarity judgments with relatively preserved recognition of difficulty-matched faces and environmental sounds; recognition of musical instruments was impaired, though better than recognition of voices. In contrast, patient KL exhibited severe impairments of both voice and face recognition, with relatively preserved recognition of musical instruments and environmental sounds. Both patients demonstrated preserved ability to analyse perceptual properties of voices and to recognise vocal emotions. The voice processing deficit in both patients could be characterised as associative phonagnosia: in the case of QR, this was relatively selective for voices, while in the case of KL, there was evidence for a multimodal impairment of person knowledge. The findings have implications for current cognitive models of voice recognition.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Voice</kwd>
        <kwd>Face</kwd>
        <kwd>Person knowledge</kwd>
        <kwd>Prosopagnosia</kwd>
        <kwd>Frontotemporal lobar degeneration</kwd>
        <kwd>Dementia</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <label>1</label>
      <title>Introduction</title>
      <p>Prosopagnosia, or impaired recognition of familiar faces, has been widely studied both in patients with acquired brain lesions and as a developmental disorder (<xref rid="bib1 bib14 bib16 bib21 bib39 bib69" ref-type="bibr">Barton, 2003; De Renzi, Faglioni, Grossi, &amp; Nichelli, 1991; Duchaine &amp; Nakayama, 2006; Gainotti, 2007b; Lucchelli &amp; Spinnler, 2008; Young, Newcombe, de Haan, Small, &amp; Hay, 1993</xref>). A striking clinical illustration of the acquired breakdown of face recognition ability is the syndrome of progressive prosopagnosia, a canonical manifestation of right temporal lobe atrophy in the frontotemporal lobar degeneration (FTLD) spectrum (<xref rid="bib10 bib19 bib33 bib34 bib35 bib54" ref-type="bibr">Chan et al., 2009; Evans, Heggs, Antoun, &amp; Hodges, 1995; Josephs et al., 2008; Joubert et al., 2003, 2004; Tyrrell, Warrington, Frackowiak, &amp; Rossor, 1990</xref>). Progressive prosopagnosia may represent a variant of semantic dementia dominated by deficits of nonverbal knowledge, including knowledge of familiar people (<xref rid="bib20 bib23 bib25 bib26 bib30 bib52 bib53" ref-type="bibr">Gainotti, 2007a; Gainotti, Ferraccioli, Quaranta, &amp; Marra, 2008; Gentileschi, Sperber, &amp; Spinnler, 1999; Gentileschi, Sperber, &amp; Spinnler, 2001; Hanley, Young, &amp; Pearson, 1989; Snowden, Thompson, &amp; Neary, 2004; Thompson et al., 2004</xref>). The syndrome is of considerable neuropsychological as well as clinical importance because it provides a window on the organisation of person knowledge in the brain (<xref rid="bib7 bib8 bib39 bib40 bib52 bib53 bib64" ref-type="bibr">Bruce &amp; Young, 1986; Burton &amp; Bruce, 1993; Lucchelli &amp; Spinnler, 2008; Lyons, Kay, Hanley, &amp; Haslam, 2006; Snowden et al., 2004; Thompson et al., 2004; Warrington, 1979</xref>). However, faces, while typically the most salient source of nonverbal information about other people, are only one of several components of person knowledge. Other channels of person knowledge, notably voices, commonly become affected with evolution of the progressive prosopagnosia syndrome (<xref rid="bib22 bib23 bib26" ref-type="bibr">Gainotti, Barbier, &amp; Marra, 2003; Gainotti et al., 2008; Gentileschi et al., 2001</xref>), however selective impairment of voice processing, or phonagnosia, is less commonly reported. Phonagnosia has been described as a developmental disorder (<xref rid="bib24" ref-type="bibr">Garrido et al., 2009</xref>) and, more commonly, in association with focal damage involving the right or left temporal lobe or the right parietal lobe (<xref rid="bib17 bib30 bib38 bib44 bib56 bib55 bib57 bib59" ref-type="bibr">Ellis, Young, &amp; Critchley, 1989; Hanley et al., 1989; Lang, Kneidl, Hielscher-Fastabend, &amp; Heckmann, 2009; Neuner &amp; Schweinberger, 2000; Van Lancker &amp; Canter, 1982; Van Lancker &amp; Kreiman, 1987; Van Lancker, Cummings, Kreiman, &amp; Dobkin, 1988; Van Lancker, Kreiman, &amp; Cummings, 1989</xref>), consistent with the distributed network of areas engaged by voice processing tasks in functional imaging studies of healthy subjects (<xref rid="bib3 bib31 bib42 bib61" ref-type="bibr">Belin, Zatorre, &amp; Ahad, 2002; Imaizumi et al., 1997; Nakamura et al., 2001; von Kriegstein, Kleinschmidt, &amp; Giraud, 2006</xref>). However, phonagnosia is much less well characterised than prosopagnosia; indeed, voice processing is often anecdotally assumed to be normal in early progressive cases (<xref rid="bib19 bib25 bib34" ref-type="bibr">Evans et al., 1995; Gentileschi et al., 1999; Joubert et al., 2003</xref>), and is generally assessed only following the development of face recognition deficits (<xref rid="bib22 bib25 bib26" ref-type="bibr">Gainotti et al., 2003; Gentileschi et al., 1999, 2001</xref>) and may not be identified as a clinical issue. Aside from the technical difficulty of assessing voice processing in clinical settings, this may be because phonagnosia is intrinsically less salient than face or name recognition deficits (<xref rid="bib44" ref-type="bibr">Neuner &amp; Schweinberger, 2000</xref>). Nevertheless, phonagnosia may be a significant and disabling clinical issue, especially in situations where compensatory cues are reduced or unavailable (e.g., over the telephone).</p>
      <p>While cognitive neuropsychological models of person identification have been developed chiefly for the case of faces, such models provide a framework for analysing the processing of voices and the effects of disease. Current models of person identification have been heavily influenced by data on face processing (<xref rid="bib7 bib18 bib39 bib44" ref-type="bibr">Bruce &amp; Young, 1986; Ellis, Jones, &amp; Mosdell, 1997; Lucchelli &amp; Spinnler, 2008; Neuner &amp; Schweinberger, 2000</xref>). These models agree broadly on the segregation of perceptual processing (via parallel processing of faces, voices and name stimuli), and a hierarchical processing of person information from early perceptual to higher semantic levels of processing. However, the detailed predictions of these models and their neuropsychological instantiation have yet to be fully worked out. Elaborations of the Bruce and Young model applied specifically to voice processing have been proposed (<xref rid="bib4 bib2 bib62 bib61" ref-type="bibr">Belin, Zatorre, Lafaille, Ahad, &amp; Pike, 2000; Belin, Fecteau, &amp; Bedard, 2004; von Kriegstein, Kleinschmidt, Sterzer, &amp; Giraud, 2005; von Kriegstein et al., 2006</xref>), but have rarely been systematically assessed in brain-damaged populations. Key unresolved issues include the degree of modality-specificity of face and voice processing deficits; the level at which any modality specificity arises; the extent to which perceptual and semantic levels of processing are interdependent; and the status of voices versus other categories of auditory objects, and other fine-grained semantic categories beyond the domain of person knowledge.</p>
      <p>We had the opportunity to address these issues in a case control study of two patients with deficits of person knowledge in the context of FTLD. The index case, patient QR, exhibited progressive loss of recognition of familiar voices as a leading clinical symptom, while the second patient, KL, presented with progressive prosopagnosia without a clinical complaint of altered voice recognition. We designed a series of neuropsychological experiments to characterise in detail different levels of voice processing in both patients. Perceptual and semantic processing of voices was assessed in relation to processing of faces, recognition of vocal emotions and identification of non-vocal sounds, in order to assess the modality- and material-specificity of any voice processing deficit.</p>
    </sec>
    <sec sec-type="methods">
      <label>2</label>
      <title>Methods and results</title>
      <sec>
        <label>2.1</label>
        <title>Subject details</title>
        <sec>
          <label>2.1.1</label>
          <title>Patient QR</title>
          <p>This 61-year-old right-handed female hairdresser presented with a 2-year history of insidiously progressive behavioural decline, with impassivity, obsessionality, clock-watching, loss of empathy and development of a sweet tooth. Impaired voice recognition was an early symptom. When first assessed she was no longer able to identify the voices of her children on the telephone, nor did she evince any sense that their voices were familiar. In contrast, recognition of faces had not been similarly affected: she consistently recognised family members, and despite the suggestion of some recent difficulty in identifying friends in social situations, she continued to exhibit a sense of familiarity toward them. On examination there was evidence of executive dysfunction, disinhibition, perseveration and impulsivity. Naming and verbal memory were impaired whereas early visual perceptual skills were preserved. The general neurological examination was unremarkable. Peripheral hearing assessed using pure tone audiometry was within normal limits for age. Brain MRI (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) showed bilateral fronto-temporal atrophy somewhat accentuated in the right anterior temporal lobe but extending posteriorly within the temporal lobe and including the superior temporal sulcus, with no significant cerebrovascular changes. The clinical diagnosis was behavioural variant frontotemporal dementia.</p>
        </sec>
        <sec>
          <label>2.1.2</label>
          <title>Patient KL</title>
          <p>This 72-year-old left-handed male academic presented with an 8-year history of insidious cognitive decline; initially reporting a difficulty in recognising neighbours and other close acquaintances, followed by progressive difficulties with word finding and topographical memory and mild behavioural changes. He had been born in the US but had lived in the UK periodically for over 50 years and consistently for the last 11 years. There was no history to suggest phonagnosia though he reported that he found understanding unfamiliar accents increasingly difficult. On examination there was evidence of mild disinhibition and impaired recognition of famous faces, with preservation of early visual perceptual skills. The general neurological examination was unremarkable. Peripheral hearing assessed using pure tone audiometry was within normal limits for age. Brain MRI (<xref rid="fig1" ref-type="fig">Fig. 1</xref>) showed bilateral predominantly anterior temporal lobe atrophy, more marked on the right side and in the inferior temporal cortices including the fusiform gyrus. The clinical diagnosis was temporal variant frontotemporal lobar degeneration with progressive right temporal lobe atrophy.</p>
        </sec>
        <sec>
          <label>2.1.3</label>
          <title>Healthy controls</title>
          <p>The experimental tasks were administered to a control group of healthy older individuals. All were native English speakers and British residents with no history of neurological or psychiatric illness and had normal screening audiometry. Perceptual and semantic tasks were administered to 24 control subjects (17 females; mean age = 64.5, SD = 4.3, range: 55–73; mean years of education 15.5, SD = 3.5, range: 11–25): between 20 and 24 controls completed each of the voice processing tests, and 15 controls also completed a test of environmental sound recognition. In addition, a test of vocal emotion recognition was administered to a separate group of 22 older controls (12 females; mean age = 67.2, SD = 8.8, range: 53–78).</p>
          <p>The background media exposure of the control subjects and both patients was assessed formally as a potentially relevant factor influencing voice recognition ability. The procedure and results are summarised in <xref rid="app1 app2" ref-type="sec">Appendices A and B</xref>.</p>
          <p>The study was approved by the local institutional research ethics committee and all subjects gave informed consent in accord with the principles of the Declaration of Helsinki.</p>
        </sec>
      </sec>
      <sec>
        <label>2.2</label>
        <title>Background neuropsychological assessment</title>
        <p>The performances of QR, KL and healthy controls on general neuropsychological tests and standard tests assessing identification of faces and topographical landmarks, examples of ‘unique entities’ in the visual modality, are summarised in <xref rid="tbl1" ref-type="table">Table 1</xref>. Both QR and KL had evidence of anomia on the Graded Naming Test (GNT) (<xref rid="bib65" ref-type="bibr">Warrington, 1997</xref>), and QR had evidence of additional impairments of single word comprehension (abstract synonyms: (<xref rid="bib67" ref-type="bibr">Warrington, McKenna, &amp; Orpwood, 1998</xref>)), surface dyslexia on the National Adult Reading Test (NART) (<xref rid="bib43" ref-type="bibr">Nelson, 1982</xref>) and executive function (<xref rid="bib15" ref-type="bibr">Delis, Kaplan, &amp; Kramer, 2001</xref>); neither patient showed a deficit of short term memory or early visual perceptual function. On the standard Famous Faces Test (<xref rid="bib66" ref-type="bibr">Warrington &amp; James, 1967</xref>) QR performed at the 5th percentile on face naming and normally on the face recognition component of the test, while KL showed impairments on both tasks. On a test assessing naming and recognition of 15 famous London landmarks from photographs (<xref rid="bib68" ref-type="bibr">Whiteley &amp; Warrington, 1978</xref>), QR and KL each performed below the 5th percentile for naming and at the 5th percentile for recognition.</p>
      </sec>
      <sec>
        <label>2.3</label>
        <title>Experimental investigations: structure and general procedure</title>
        <p>Voice recognition was assessed using tests of familiarity and identification of famous voices. Normal voice recognition ability was quantified in a pilot study (see <xref rid="app2" ref-type="sec">Appendix B</xref>) in healthy older adult controls, as it has been shown previously that normal individuals have more difficulty recognising public figures from voice than from faces or name (<xref rid="bib13 bib18 bib29" ref-type="bibr">Damjanovic &amp; Hanley, 2007; Ellis et al., 1997; Hanley, Smith, &amp; Hadfield, 1998</xref>). The specificity of any voice recognition deficit was assessed using recognition of public figures represented in other modalities (faces and names), and within the auditory modality using tests of vocal emotion recognition and environmental sound identification. In order to assess the effects of perceptual factors on any voice recognition deficit, discrimination of unfamiliar voices and perceptual analysis of vocal properties (speaker size and gender) were investigated in separate experiments. Single-case results were assessed in relation to the control sample using the method of <xref rid="bib11" ref-type="bibr">Crawford and Howell (1998)</xref>.</p>
        <p>Tests were administered in divided sessions. In order to minimise priming of voice recognition from other modalities, recognition tests were administered in a fixed order, motivated by evidence from the pilot control study (<xref rid="app2" ref-type="sec">Appendix B</xref>) that the public figures selected were all better recognised from face than from voice. Voice familiarity, naming and identification tasks were performed first, followed by face familiarity, naming and identification tasks, and finally the name familiarity task; the order of stimuli was randomised within each modality. Voice stimuli were delivered from digital wavefiles via a personal computer in free-field at a comfortable constant listening level. In cross-modal matching tasks, names were presented simultaneously spoken and written. Stimuli were presented in randomised order. For each test, the experimenter first ensured that subjects understood the task; however no feedback was given during the test proper.</p>
      </sec>
      <sec>
        <label>2.4</label>
        <title>Experiment 1: familiarity of voices, faces and personal names</title>
        <p>The aim of this experiment was to assess the familiarity of famous voices for QR and KL, and to compare this with familiarity judgments for faces and names of the same individuals. Voice samples were selected based on the initial pilot study in a separate group of healthy controls, and face photographs for the same individuals were used for the face recognition task. The final set of comprised 24 British and American public figures: (see <xref rid="app3" ref-type="sec">Appendix C</xref>) 10 politicians, five actors, seven media personalities from television and radio, and two members of royalty. Examples of stimuli are available from the authors.</p>
        <p>Voice samples and photographs were chosen so as to minimise other potential semantic cues to recognition. A selection of 24 unfamiliar voices and faces that were classified as unfamiliar by &gt;75% of controls were included in the final test; these were matched by gender to the familiar set and approximately matched for age and accent. Unfamiliar personal name foils were fabricated. Each stimulus was presented once, and subjects were asked to make a ‘yes/no’ judgement on familiarity.</p>
        <sec>
          <label>2.4.1</label>
          <title>Results</title>
          <p><xref rid="tbl2" ref-type="table">Table 2</xref> shows the results of familiarity judgments on voices, faces and names, in QR, KL and controls. For controls, the voice familiarity task was most difficult (mean score equivalent to 85% correct), compared to near-ceiling performance on face and name familiarity (mean score equivalent to 97% correct in each modality). QR performed close to chance (and significantly worse than controls: <italic>t</italic> = −3.8, <italic>p</italic> &lt; 0.01, df = 22) for voice familiarity judgments; for face familiarity judgments, QR's performance was above chance but also significantly worse than controls (<italic>t</italic> = −10.8, <italic>p</italic> &lt; 0.001, df = 22), while for name familiarity judgments her performance was significantly worse than the control mean (<italic>t</italic> = −2.2, <italic>p</italic> = 0.04, df = 22) but within the control range. Further analysis of errors made by QR revealed that she correctly classified only 15/24 familiar voices as familiar, and misclassified 14/24 unfamiliar voices as familiar. On name familiarity she correctly classified 19/24 familiar names as familiar and misclassified 0/24, while she correctly classified 19/24 familiar faces as familiar, but misclassified 14/24 unfamiliar faces as familiar (i.e., she showed an inflated false alarm rate, especially for face familiarity: 14/19 errors). KL's performance was significantly worse than controls for all three modalities (voices: <italic>t</italic> = −3.1, <italic>p</italic> &lt; 0.01, df = 22, faces: <italic>t</italic> = −9.6, <italic>p</italic> &lt; 0.001, df = 22, names: <italic>t</italic> = −8.3, <italic>p</italic> &lt; 0.001, df = 22). Analysis of KL's errors revealed a hit rate of only 6/24 familiar voices, 11/24 familiar faces and 14/24 familiar names. He made few false alarms: only 2/24 unfamiliar voices, 4/24 unfamiliar faces and 5/24 unfamiliar names were classed as familiar.</p>
        </sec>
        <sec>
          <label>2.4.2</label>
          <title>Comment</title>
          <p>QR performed close to chance for voice familiarity judgments; in addition, her ability to judge the familiarity of faces was also clearly impaired, whereas her ability to judge the familiarity of names was somewhat less impaired. This pattern suggests some modality specificity to QR's person familiarity deficit. KL performed similarly whether judging the familiarity of public figures from voice, face or name, supporting a multimodal person familiarity deficit.</p>
        </sec>
      </sec>
      <sec>
        <label>2.5</label>
        <title>Experiment 2: identification of voices and faces</title>
        <p>The ability to name or otherwise demonstrate recognition of voices and faces was assessed using the 24 public figures selected for the familiarity task. Subjects were asked to identify the voice or face as precisely as they could; the criterion for correct recognition was name (surname) or other identifying feature (e.g., an event closely associated with the person, occupational information), in line with the criteria used by <xref rid="bib52" ref-type="bibr">Snowden et al. (2004)</xref>. Controls were required to supply more specific contextual information. For voice stimuli, national or regional origin was not accepted, as this could be based on accent cues alone. As both patients had evidence for generalised word retrieval impairment on a standard naming task (<xref rid="tbl1" ref-type="table">Table 1</xref>) a cross-modal matching task was designed to maximise the opportunity to demonstrate recognition of voices and faces using an alternative procedure that did not rely on proper name retrieval. For both face and voice targets, three foil arrays were selected from the complete 24 item set. One array contained the 6 females from the complete set, a second array contained the 9 male politicians, and the third contained the 9 male media figures. Arrays were based on the individual's career, as this is likely to be an important organisational principle in the domain of person knowledge (<xref rid="bib12" ref-type="bibr">Crutch &amp; Warrington, 2004</xref>). Famous faces (with name foils only) were presented first, followed by famous voices (with faces and name foils presented simultaneously).</p>
        <sec>
          <label>2.5.1</label>
          <title>Results</title>
          <p><xref rid="tbl2" ref-type="table">Table 2</xref> shows the results of identification tasks for voices and faces in QR, KL and controls. Controls performed significantly better on tests assessing identification of faces than voices (naming: <italic>t</italic> = 5.9, <italic>p</italic> &lt; 0.001, df = 21; recognition: <italic>t</italic> = 6.1, <italic>p</italic> &lt; 0.001, df = 21); face recognition test performance was near-ceiling. Both QR and KL performed at floor and significantly worse than controls for both naming (<italic>t</italic> = −3.7, <italic>p</italic> &lt; 0.01, df = 21) and recognition (<italic>t</italic> = −4.7, <italic>p</italic> &lt; 0.001, df = 21) of famous voices. Both patients performed significantly worse than controls for face naming (QR: <italic>t</italic> = −5.6, <italic>p</italic> &lt; 0.001, df = 22, KL: <italic>t</italic> = −6.7, <italic>p</italic> &lt; 0.001, df = 22) and face recognition (QR: <italic>t</italic> = −8.1, <italic>p</italic> &lt; 0.001, df = 22, KL: <italic>t</italic> = −24.0, <italic>p</italic> &lt; 0.001, df = 22), however QR's performance improved substantially for recognition of faces compared with voices, and her performance was significantly superior to KL's (<italic>χ</italic><sup>2</sup> = 14.31, <italic>p</italic> &lt; 0.001, df = 1).</p>
          <p>On cross-modal matching tasks, control performance was near-ceiling for both voices and faces. For cross-modal matching of faces to names, both QR and KL performed significantly worse than controls (<italic>t</italic> &lt; −400, <italic>p</italic> &lt; 0.001, df = 19) but performed clearly above chance; QR's performance was significantly better than KL's (<italic>χ</italic><sup>2</sup> = 5.89, <italic>p</italic> &lt; 0.05, df = 1). For cross-modal matching of voices to faces and names, both patients performed at chance and significantly worse than controls (<italic>t</italic> = −22.2, <italic>p</italic> &lt; 0.001, df = 19).</p>
          <p>The experimental control group here had a high average NART IQ (120.9, SD = 6.3) and a greater mean number of years of education than QR, raising the possibility that a generic factor such as IQ contributed to her voice recognition deficit. We do not have a premorbid estimate of QR's IQ, and any estimation based, for example, on demographic factors such as occupation would need to be made with caution in the individual case. Moreover, regression analysis in a larger control sample of older adults (<italic>n</italic> = 48) (<xref rid="app2" ref-type="sec">Appendix B</xref>), showed no evidence of association between number of years of education or NART IQ and voice recognition performance. In order to further explore any IQ-related contribution to QR's voice recognition difficulty, we compared her performance on the voice recognition tasks with five healthy control subjects (3 females, 2 males) who had an average IQ typical for the greater London population (mean IQ 107.6, SD 6.7, range: 96–112). This control group included three controls from the experimental control group with lower IQs (mean IQ 110.3, SD 2.1, range: 108–112) and two additional older adult controls (IQs 96 and 111) not included in the main study as they did not complete the perceptual voice tests. QR's performance was significantly inferior to this lower-IQ control subgroup (<italic>p</italic> &lt; 0.001) on the voice familiarity (<italic>t</italic> = −6.7, <italic>p</italic> &lt; 0.001, df = 4), naming (<italic>t</italic> = −5.0, <italic>p</italic> &lt; 0.001, df = 4) and recognition (<italic>t</italic> = −6.0, <italic>p</italic> &lt; 0.001, df = 4) tasks.</p>
        </sec>
        <sec>
          <label>2.5.2</label>
          <title>Comment</title>
          <p>These findings corroborate the results of Experiment 1. QR had a severe impairment of voice identification, evident across the recognition and cross-modal matching procedures used here. Her ability to retrieve proper names from voice or face was clearly impaired, as anticipated on the basis of her general word retrieval impairment (<xref rid="tbl1" ref-type="table">Table 1</xref>). However, her ability to identify the same public figures from face information in the recognition and cross-modal matching conditions (which do not rely on naming), though deficient to healthy controls, was clearly superior to her ability to identify voices, and superior to KL's performance in either modality. QR's score on the voice recognition task was also highly significantly worse than the lower-IQ control group: it therefore seems unlikely that her voice recognition deficit was due to IQ factors. In line with previous work, control voice recognition scores were significantly lower than face recognition scores (<xref rid="bib29" ref-type="bibr">Hanley et al., 1998</xref>). In the pilot control regression analysis (<xref rid="app2" ref-type="sec">Appendix B</xref>), increased news exposure was positively associated with voice recognition score. It is unlikely this factor explains QR's voice recognition deficit, as QR rated in the highest category for the number of times per week she read or watched the news (<xref rid="app1" ref-type="sec">Appendix A</xref>). QR's relatively good performance on face recognition appears initially somewhat paradoxical in relation to her poor performance on the face familiarity judgment: however, this pattern is likely to reflect an inflated false alarm rate (14/19 errors) on the face familiarity task.</p>
        </sec>
      </sec>
      <sec>
        <label>2.6</label>
        <title>Experiment 3: comparison with recognition of lower frequency faces</title>
        <p>Quantifying any face recognition deficit in Experiment 2 was confounded by near-ceiling control performance on both face recognition and cross-modal matching tasks; furthermore, all the public figures selected were recognised better from face than from voice by controls, and face recognition performance may have been primed by previous presentation of the corresponding voices. We therefore selected from the pilot study stimuli an alternative set of 24 faces that were matched in accuracy of recognition and naming by controls to the voices used in the main study (recognition achieved by 77% of controls; mean = 76.7, SD = 8.7, range: 58–85%) (see <xref rid="app3" ref-type="sec">Appendix C</xref>). Recognition by pilot study group controls was not significantly different between this set of faces and the 24 voices (Wilcoxon rank-sum Test: <italic>z</italic> = −1.1, <italic>p</italic> &gt; 0.26). This alternative set of faces was administered to QR and KL.</p>
        <sec>
          <label>2.6.1</label>
          <title>Results</title>
          <p>On recognition of difficulty-matched faces (<xref rid="tbl2" ref-type="table">Table 2</xref>), QR's performance did not differ significantly from healthy controls for either face naming (<italic>t</italic> = −1.2, <italic>p</italic> = 0.26, df = 24) or recognition (<italic>t</italic> = −1.1, <italic>p</italic> = 0.30, df = 24). KL's performance remained significantly inferior to controls (naming: <italic>t</italic> = −2.8, <italic>p</italic> &lt; 0.05, df = 24; recognition: <italic>t</italic> = −3.2, <italic>p</italic> &lt; 0.001, df = 24).</p>
        </sec>
        <sec>
          <label>2.6.2</label>
          <title>Comment</title>
          <p>These findings support the concept of a relatively modality-specific deficit of voice recognition in QR, in contrast to the multimodal deficit of person recognition exhibited by KL.</p>
        </sec>
      </sec>
      <sec>
        <label>2.7</label>
        <title>Experiment 4: perceptual analysis of voices: vocal size and gender</title>
        <p>QR's voice recognition deficit could in principle reflect impaired perceptual processing of voices. We designed a series of experiments to investigate this possibility. Perceptual analysis of voices was assessed in both QR and KL using tasks requiring encoding of the basic voice properties of vocal tract length and gender.</p>
        <p>Perceptual judgment of speaker size is a fundamental task of auditory cognition in humans and other species, and vocal tract length (VTL) is an important cue for perception of speaker size by normal subjects (<xref rid="bib32" ref-type="bibr">Ives, Smith, &amp; Patterson, 2005</xref>). Here we assessed categorical (‘big’ versus ‘small’) judgements of vocal size based on a VTL cue. Stimuli in this test were based on sixteen consonant-vowel syllables recorded by a single male speaker and digitally resynthesised using a previously described algorithm (<xref rid="bib36" ref-type="bibr">Kawahara &amp; Irino, 2004</xref>) that allows apparent vocal tract length to be varied independently of glottal pulse rate (voice pitch). Each syllable was presented at two extreme VTL values, one corresponding to a speaker height of 2 m (equivalent to a very tall man, ‘big’) and the other to a height of 0.5 m (equivalent to a child, ‘small’), forming the basis for 32 trials (16 big and 16 small). Each syllable was randomly assigned (independently of VTL) one of four similar pitch values within the normal human male vocal range (116, 120, 138, 158 Hz). Examples of stimuli are available from the authors. On each trial, subjects heard a sequence of three repetitions of the identical syllable, and were asked to decide if the sounds were made by a big person or a small person.</p>
        <p>Voice gender can be determined using various low-level perceptual features including pitch and VTL. In this test we sought to determine whether such low-level cues could be used to assign a gender label to the stimulus. 24 stimuli (12 males, 12 females) were selected from the test set for the voice familiarity task. On each trial the subject was asked to decide if the voice was male or female.</p>
        <sec>
          <label>2.7.1</label>
          <title>Results</title>
          <p><xref rid="tbl3" ref-type="table">Table 3</xref> shows the results of perceptual analysis tasks for voices and faces in QR, KL and controls. Both patients were able to judge gender and speaker size, and their performance was not significantly different to healthy controls (QR: <italic>t</italic> = 0.0, <italic>p</italic> = 0.97, df = 20; KL: <italic>t</italic> = −0.8, <italic>p</italic> = 0.44, df = 20).</p>
        </sec>
        <sec>
          <label>2.7.2</label>
          <title>Comment</title>
          <p>QR's satisfactory performance on these tests makes it unlikely that her impaired ability to identify voices was grounded in an early vocal perceptual deficit.</p>
        </sec>
      </sec>
      <sec>
        <label>2.8</label>
        <title>Experiment 5: discrimination of unfamiliar voices</title>
        <p>Beyond early perceptual encoding but prior to the attribution of meaning it is likely that voice processing entails an interposed stage of representation of the voice as a complex auditory object (<xref rid="bib27 bib63" ref-type="bibr">Griffiths &amp; Warren, 2004; Warren, Scott, Price, &amp; Griffiths, 2006</xref>). This apperceptive stage of vocal processing can be assessed by tasks requiring discrimination of unfamiliar speakers. We created a novel speaker discrimination task in which subjects were required to detect a change in speaker within spoken phrases (highly over-learned sequences comprising days of the week ‘Monday, Tuesday, Wednesday, Thursday’ or months of the year ‘January, February, March, April’). In order to control for gender, age and accent, all speakers were female, aged 21–31 years, with a standard Southern English accent. Inter-speaker variations in vocal pitch were controlled by fixing f0 of recorded stimuli at 220 Hz using Goldwave<sup>®</sup> software. Recorded single words were concatenated with fixed inter-word gaps (0.1 s) to equate overall speech rate. Examples of stimuli are available from the authors. 24 trials were presented using spoken sequences of days of the week, followed by 24 trials using sequences of months. On each trial, subjects were asked to decide whether the spoken phrase contained a change in speaker (on change trials the change always occurred at the midpoint of the phrase, to maximise available vocal information for each speaker). Patient performance on these vocal tasks was compared with performance on a standard test of perceptual processing of face identity, the Benton Facial Recognition Test (<xref rid="bib5" ref-type="bibr">Benton, Hamsher, Varney, &amp; Spreen, 1989</xref>): this test depends on successful perceptual encoding of the configuration of a face, and requires the subject to match a photograph of target face to one (or three) of six other photographs of the target with distractor faces under different viewing conditions.</p>
        <sec>
          <label>2.8.1</label>
          <title>Results</title>
          <p>On the speaker discrimination task, QR's performance did not differ significantly from controls (<italic>t</italic> = 1.3, <italic>p</italic> = 0.22, df = 20) (<xref rid="tbl3" ref-type="table">Table 3</xref>). KL's performance was also not significantly different from controls (sample: <italic>t</italic> = −0.6, <italic>p</italic> = 0.54, df = 20). Both QR and KL performed normally on the Benton test of perceptual matching of faces.</p>
        </sec>
        <sec>
          <label>2.8.2</label>
          <title>Comment</title>
          <p>This experiment provides further evidence that pre-semantic vocal processing mechanisms were intact in QR and KL. Both patients were able to achieve an intact representation of individual voices as auditory objects sufficient to discriminate between different speakers, yet were unable to gain a sense of familiarity to a voice or to associate these representations with other stored information about familiar speakers.</p>
        </sec>
      </sec>
      <sec>
        <label>2.9</label>
        <title>Experiment 6: recognition of vocal emotions</title>
        <p>Vocal emotion and identity information are likely to be at least partly dissociable cognitively and anatomically (<xref rid="bib2" ref-type="bibr">Belin et al., 2004</xref>). Patients with FTLD (in particular, the so-called behavioural variant) often show altered responses to emotions in various input modalities, including voice (<xref rid="bib37 bib51" ref-type="bibr">Keane, Calder, Hodges, &amp; Young, 2002; Snowden et al., 2008</xref>). Processing of vocal emotion by QR and KL was therefore assessed in a separate experiment. 40 nonverbal vocalisations, 10 representing each of the emotions happiness, sadness, anger and fear, were selected from a previously developed set (<xref rid="bib49 bib48" ref-type="bibr">Sauter &amp; Scott, 2007; Sauter, Calder, Eisner, &amp; Scott, in press</xref>). Items most reliably recognised by young normal subjects based on these previous normative data were selected. The subject's task on each trial was to select the emotion label describing the target emotion in a four-alternative forced choice format.</p>
        <sec>
          <label>2.9.1</label>
          <title>Results</title>
          <p><xref rid="tbl3" ref-type="table">Table 3</xref> shows the results of the vocal emotion recognition test for QR, KL and controls. Both QR and KL performed comparably to healthy controls (QR: <italic>t</italic> = −1.0, <italic>p</italic> = 0.34, df = 21, KL: <italic>t</italic> = −1.6, <italic>p</italic> = 0.12, df = 21).</p>
        </sec>
        <sec>
          <label>2.9.2</label>
          <title>Comment</title>
          <p>Considered together with the results of Experiments 1–3, these findings provide support for a dissociation between vocal identity and vocal emotion processing in these patients.</p>
        </sec>
      </sec>
      <sec>
        <label>2.10</label>
        <title>Experiment 7: identification of environmental sounds</title>
        <p>It is not established to what extent the processing of voices is separable from other complex nonverbal sounds. We addressed this issue in a further experiment probing recognition of environmental sounds. 40 common environmental sounds representing a variety of sound sources, including elemental sounds (e.g., thunder), man-made objects (e.g., kettle whistling), and animal calls (e.g., cow mooing), were selected from on-line databases. Environmental sounds were identified either by sound source (e.g., cow or a tap), or a description of the sound (e.g., mooing or dripping water); relatively lenient criteria for recognition were used, in line with the criteria used for person identification (Experiments 2 and 3). In a cross-modal version of the test, the subject was presented with arrays of four names and pictures, and required to match each sound with the correct name-picture combination.</p>
        <sec>
          <label>2.10.1</label>
          <title>Results</title>
          <p><xref rid="tbl4" ref-type="table">Table 4</xref> shows the results of environmental sounds identification tests for QR, KL and controls. On the sound recognition test, both QR and KL performed comparably to healthy controls (QR: <italic>t</italic> = −1.0, <italic>p</italic> = 0.35, df = 14; KL: <italic>t</italic> = −1.4, <italic>p</italic> = 0.18, df = 14). On the cross-modal matching task, KL performed at ceiling and QR near-ceiling; 9/10 control subjects performed at ceiling on this task.</p>
        </sec>
        <sec>
          <label>2.10.2</label>
          <title>Comment</title>
          <p>Both QR and KL performed essentially normally on tests of environmental sound recognition. These findings suggest that the deficit of voice recognition exhibited by each patient is at least relatively specific for human voices.</p>
        </sec>
      </sec>
      <sec>
        <label>2.11</label>
        <title>Experiment 8: identification of musical instruments</title>
        <p>Voice identification requires fine-grained perceptual and semantic processing within a single highly differentiated category of complex sounds. It is unclear therefore whether selective deficits of voice processing versus other kinds of complex sounds reflect the privileged ecological status of human voices or rather the greater demands of processing unique auditory exemplars. Similar arguments have previously been advanced to challenge claims that human faces constitute a privileged category of visual objects (<xref rid="bib23" ref-type="bibr">Gainotti et al., 2008</xref>). Here we addressed this issue using an alternative finely differentiated category of complex sounds: musical instruments. Subjects were asked firstly to name 20 different sequentially presented instruments from their sounds (audio clips between 4 and 10 s in duration), and then to identify the same instruments in a cross-modal matching condition, in which instrument sounds were presented together with arrays of four written instrument names and pictures. Cross-modal arrays contained the target instrument, a within-instrument family distractor (e.g., woodwind, brass, strings, percussion, and keyboard), and two instruments from a different instrument family. As QR had no musical training and KL had only 2 years of childhood piano lessons, patient performance was compared to 12 controls with up to 2 years musical training (defined as “inexperienced listeners”: (<xref rid="bib28" ref-type="bibr">Halpern, Bartlett, &amp; Dowling, 1995</xref>)).</p>
        <sec>
          <label>2.11.1</label>
          <title>Results</title>
          <p><xref rid="tbl4" ref-type="table">Table 4</xref> shows the results of musical instrument identification tests for QR, KL and controls. Inexperienced listeners recognised on average 68.5% (SD = 14.4%) of the instruments, an accuracy level inferior to recognition of famous voices by the same controls. Both patients performed significantly worse than controls on tests of instrument sound naming (QR: <italic>t</italic> = −2.8, <italic>p</italic> &lt; 0.05, df = 11; KL: <italic>t</italic> = −2.4, <italic>p</italic> &lt; 0.05, df = 11) and recognition (QR: −2.6, <italic>p</italic> &lt; 0.05, df = 11; KL: <italic>t</italic> = −2.2, <italic>p</italic> &lt; 0.05, df = 11). On the cross-modal matching task QR performed above chance, however her score was significantly different to controls (<italic>t</italic> = −9.4, <italic>p</italic> &lt; 0.001, df = 11); in contrast KL's performance was not significantly different to controls (<italic>t</italic> = −1.7, <italic>p</italic> = 0.12, df = 11). Both controls’ and patients’ performance improved on the visual version of the task. Both patients’ scores were significantly different to controls on tests of instrument picture naming (QR: <italic>t</italic> = −7.4, <italic>p</italic> &lt; 0.001, df = 11; KL: <italic>t</italic> = −3.4, <italic>p</italic> &lt; 0.01, df = 11) and recognition (QR: <italic>t</italic> = −4.7, <italic>p</italic> &lt; 0.01, df = 11; KL: <italic>t</italic> = −2.8, <italic>p</italic> &lt; 0.05, df = 11).</p>
        </sec>
        <sec>
          <label>2.11.2</label>
          <title>Comment</title>
          <p>These findings suggest that QR's ability to recognise another category of finely differentiated sounds (musical instruments) was impaired, though superior to her ability to recognise voices. In contrast, KL exhibited normal auditory recognition of instruments on the cross-modal matching task. This pattern of results might signify that QR has an auditory agnosia that affects recognition of voices and certain other categories of auditory objects, whereas KL has a primary deficit of person knowledge. However, this interpretation requires some qualification, since both patients also exhibited impaired visual recognition of instruments relative to the healthy control group, while QR scored lower on both the auditory and pictorial versions of the task relative to KL. It is difficult to equate musical exposure between non-musicians (KL's musical experience is likely to have been wider than QR's) and this may also be affected by other factors, such as general educational attainment (QR had fewer years of formal education than KL). These factors are likely a priori to be relatively more important for music than person knowledge. Moreover, no other category of complex nonverbal sounds is truly comparable in diversity and familiarity to human voices (for practical purposes, a musically untrained subject is likely to be acquainted with perhaps twenty or thirty musical instruments, but potentially hundreds of individual human voices).</p>
        </sec>
      </sec>
    </sec>
    <sec>
      <label>3</label>
      <title>Discussion</title>
      <p>Here we have presented neuropsychological evidence for distinctive deficits of voice recognition in two patients with focal neurodegenerative disorders. The first patient, QR, exhibited severe impairments of voice identification and familiarity judgments with preserved recognition of difficulty-matched faces and environmental sounds; recognition of another highly differentiated category of complex sounds (musical instruments), though impaired, was substantially better than recognition of voices. In contrast, patient KL exhibited severe impairments of both voice and face recognition, partly preserved recognition of musical instruments and essentially normal recognition of environmental sounds. Both patients demonstrated preserved ability to analyse perceptual properties of voices to the level of individual speaker discrimination and to recognise emotions in voices. The profiles of deficits exhibited by both QR and KL are summarised in <xref rid="tbl5" ref-type="table">Table 5</xref>. QR's deficit of voice processing could be characterised as a failure to associate familiar voices with other specific semantic information about the individual: associative phonagnosia. Further, this deficit is relatively selective for voices. KL's more uniform deficit of recognition across modalities (voices, faces and names) suggests a multimodal failure of person knowledge with associative phonagnosia as one component.</p>
      <p>Detailed studies of phonagnosia are comparatively few (<xref rid="bib24 bib44 bib56 bib55 bib57 bib59" ref-type="bibr">Garrido et al., 2009; Neuner &amp; Schweinberger, 2000; Van Lancker &amp; Canter, 1982; Van Lancker &amp; Kreiman, 1987; Van Lancker et al., 1988, 1989</xref>) and neuropsychological investigations of voice recognition have generally been undertaken in patients presenting with acquired or developmental prosopagnosia (<xref rid="bib23 bib25 bib26 bib61" ref-type="bibr">Gainotti et al., 2008; Gentileschi et al., 1999, 2001; von Kriegstein et al., 2006</xref>). Selective phonagnosia has recently been described on a developmental basis (<xref rid="bib24" ref-type="bibr">Garrido et al., 2009</xref>): this individual had deficits of voice recognition and familiarity despite normal face recognition. Deficits in person knowledge are well described as a presentation of right temporal lobe degeneration: selective impairment of face recognition and multimodal impairment extending to recognition of voices and names have been described (<xref rid="bib19 bib20 bib22 bib23 bib25 bib26 bib34 bib52 bib53" ref-type="bibr">Evans et al., 1995; Gainotti, 2007a; Gainotti et al., 2003, 2008; Gentileschi et al., 1999, 2001; Joubert et al., 2003; Snowden et al., 2004; Thompson et al., 2004</xref>). It has previously been shown (<xref rid="bib52 bib53" ref-type="bibr">Snowden et al., 2004; Thompson et al., 2004</xref>) that patients with predominant right temporal lobe atrophy are in general more impaired for recognition of faces than names, whereas patients with predominant left temporal lobe atrophy exhibit the reverse pattern of deficits. Considered together with the neuroimaging findings in the present cases (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), this evidence suggests that the anterior temporal lobes instantiate mechanisms for processing multiple aspects of person knowledge and the right temporal lobe may be implicated particularly for aspects of nonverbal person knowledge. Associated broader deficits of verbal or visual semantics have been documented in a number of cases, leading to the proposal that person knowledge is mediated by a distributed bi-temporal network with dedicated brain regions representing modality-specific information. A substrate of this kind would allow for modality-specific biases within a more widespread defect of person knowledge (<xref rid="bib44" ref-type="bibr">Neuner &amp; Schweinberger, 2000</xref>): cross- or multimodal knowledge could be mediated by close connections within a common network or by additional dedicated (anterior temporal) mechanisms (<xref rid="bib52 bib53" ref-type="bibr">Snowden et al., 2004; Thompson et al., 2004</xref>). However, phonagnosia has not previously been emphasised as the leading feature of person knowledge breakdown in degenerative disease, and detailed anatomical correlates of this deficit remain to be established.</p>
      <p>The present findings speak to current cognitive models of the organisation of person knowledge and particularly the processing of human voices. Models of voice processing (<xref rid="bib2 bib18 bib44" ref-type="bibr">Belin et al., 2004; Ellis et al., 1997; Neuner &amp; Schweinberger, 2000</xref>) have been heavily influenced by hierarchical models of face recognition such as that developed by <xref rid="bib7" ref-type="bibr">Bruce and Young (1986)</xref>. The Bruce and Young model posits separable pathways for processing faces, voices and names that operate partly in parallel and generate structural representations of person information at modality-specific recognition units. Recognition units are linked to cross-modal Person Identity Nodes (PINs) that are linked independently in turn to amodal stored semantic information about the person (e.g., profession, nationality, autobiographical details). A sense of familiarity for the person could arise prior to the stage of explicit identification (e.g., at the PIN: (<xref rid="bib9" ref-type="bibr">Burton, Bruce, &amp; Johnston, 1990</xref>; Burton et al., 1993)). The model also incorporates a further functional separation of pathways processing identity information and affective signals. <xref rid="bib2" ref-type="bibr">Belin and colleagues (2004)</xref> have proposed a hierarchical model of voice recognition based on the Bruce and Young model. According to this model, voices first undergo acoustic analysis in common with other classes of complex sounds in the ascending auditory pathways to primary auditory cortex, followed by a stage of vocal structural encoding proposed to engage cortical areas in bilateral upper superior temporal sulcus, and subsequent stages of speaker identification and association with cross-modal and amodal knowledge about the speaker are mediated in the anterior temporal lobe and beyond. Within this formulation, the superior temporal sulcus might plausibly instantiate voice-specific recognition units, and anterior temporal cortex the PIN.</p>
      <p>This model has received experimental support from neuropsychological and functional imaging studies (<xref rid="bib4 bib3 bib60 bib62 bib63" ref-type="bibr">Belin et al., 2000, 2002; von Kriegstein, Eger, Kleinschmidt, &amp; Giraud, 2003; von Kriegstein et al., 2005; Warren et al., 2006</xref>). However, modality-specific deficits of person knowledge (in judgements of familiarity and also retrieval of semantic information) present a potentially critical test of the model, and indeed models of the semantic system more broadly (<xref rid="bib20 bib39 bib41 bib52 bib53" ref-type="bibr">Gainotti, 2007a; Lucchelli &amp; Spinnler, 2008; Mahon, Anzellotti, Schwarzbach, Zampini, &amp; Caramazza, 2009; Snowden et al., 2004; Thompson et al., 2004</xref>). The multimodal impairments displayed by KL here (and by most previously studied patients with progressive prosopagnosia) are consistent with a core defect affecting a multimodal store of knowledge about familiar people (<xref rid="bib20 bib23 bib26 bib39" ref-type="bibr">Gainotti, 2007a; Gainotti et al., 2008; Gentileschi et al., 2001; Lucchelli &amp; Spinnler, 2008</xref>), reflecting either damage to the stores proper or a disconnection from the PIN. However, QR exhibits a relatively selective associative deficit of voice recognition. Such a deficit could in principle arise at pre-semantic stages in the voice processing pathway: the demonstration of intact early vocal perceptual analysis and speaker discrimination in QR would be consistent with a dissociation of perceptual descriptions or voice recognition units from the PIN. A lesion at this processing stage might also account for loss of the sense of familiarity of voices. However, while voices are often analogised as ‘auditory faces’, the demands of perceptual analysis differ substantially between the auditory and visual modalities, and mechanisms for the perceptual analysis of voices remain poorly understood. Deriving a faithful neural representation of a voice is likely to depend on intact mechanisms for processing timbre, the spectrotemporal signature that defines a voice as an auditory object (<xref rid="bib27 bib63" ref-type="bibr">Griffiths &amp; Warren, 2004; Warren et al., 2006</xref>). Selectivity of voice recognition deficits could arise from an abnormal interaction between combinations of complex vocal properties such as timbre, articulation and prosody which distinguish an individual's voice (<xref rid="bib46 bib47 bib50 bib58" ref-type="bibr">Perrachione &amp; Wong, 2007; Remez, Fellowes, &amp; Rubin, 1997; Schweinberger, 2001; Van Lancker, Kreiman, &amp; Cummings, 1985</xref>), and subsequent stages of voice identity processing (it is of interest that KL reported some difficulty understanding unfamiliar accents). Interaction between perceptual and semantic mechanisms of voice processing would be in line with recent re-evaluations of models of person identification (<xref rid="bib39" ref-type="bibr">Lucchelli &amp; Spinnler, 2008</xref>), and may be particularly critical under non-standard listening conditions (e.g., identification of voices over the phone or when singing: (<xref rid="bib6 bib24" ref-type="bibr">Benzagmout, Chaoui, &amp; Duffau, 2008; Garrido et al., 2009</xref>).</p>
      <p>A related issue is the specificity of agnosia for voices versus other kinds of complex sounds and versus unique entities (i.e., items associated with proper nouns: (<xref rid="bib23" ref-type="bibr">Gainotti et al., 2008</xref>)) in sound or other modalities. This speaks to the more fundamental issue of the degree of specialisation of brain mechanisms for processing voices versus other kinds of ecologically relevant complex sounds (<xref rid="bib2" ref-type="bibr">Belin et al., 2004</xref>). Both QR and KL were able to recognise environmental sounds successfully, arguing against a generalised auditory agnosia: this dissociation corroborates previous findings (<xref rid="bib24 bib44 bib45" ref-type="bibr">Garrido et al., 2009; Neuner &amp; Schweinberger, 2000; Peretz et al., 1994</xref>). QR and KL demonstrated comparably weak performance for recognition of London landmarks, but in both cases this was clearly superior to recognition of voices (and in the case of KL, also superior to recognition of faces). Furthermore, QR demonstrated a clear superiority for recognition of faces versus voices. Taken together, these observations argue that phonagnosia in these cases is unlikely simply to reflect a generic defect of fine-grained semantic attributions. Within the auditory modality, both QR and KL showed superior recognition of musical instruments compared with voices, however QR's performance was clearly inferior both to healthy controls and KL. Musical instruments are themselves likely to constitute a specialised category of complex sounds, but (unlike voices) cannot strictly be considered ‘unique entities’: nevertheless, the pattern of QR's results raises the possibility that her phonagnosia is part of a broader defect of differentiation amongst closely related auditory entities, which could in turn arise at the level of associative (semantic) processing or as a result of an abnormal interaction between perceptual and semantic mechanisms. This formulation would be consistent with evidence in the visual domain, in both the present and previous studies e.g., (<xref rid="bib20 bib23" ref-type="bibr">Gainotti, 2007a; Gainotti et al., 2008</xref>): patients with right temporal lobe lesions in general exhibit a more severe deficit for recognition of faces than landmarks and other unique visual entities, however this recognition deficit is seldom restricted purely to faces.</p>
      <p>The present study shares the limitations of single neuropsychological case studies, including limited scope for anatomical correlation: this applies particularly to neurodegenerative pathologies, in which any regional selectivity of brain damage is relative rather than absolute. That caveat aside, these cases together illustrate a syndrome of progressive associative phonagnosia and demonstrate that this may be relatively selective with respect to other stages of voice analysis, other aspects of person knowledge and other categories of auditory objects. Important directions for future work will include the longitudinal study of the evolution of phonagnosia in relation to other defects of person knowledge in patients with degenerative pathologies, a more detailed examination of the processing of other unique or finely differentiated auditory entities in phonagnosic individuals, and structural and functional anatomical substrates for the syndrome.</p>
    </sec>
    <sec id="app1">
      <label>Appendix A</label>
      <p>Subjects’ exposure to the media.</p>
      <p>All subjects completed a questionnaire detailing their background media exposure; subjects were asked to estimate their average media exposure for the preceding three months according to one of three categories: number of hours per week spent watching television, number of hours per week spent listening to the radio, and number of times per week they watched or read the news. QR's media exposure was greater than the average experience of the controls in each of these categories: on average each week she spent over 20 h watching television (median control category = 5–10 h per week, range: 0–20+), over 20 h listening to the radio (median control category = 5–10 h per week, range: 0–20+), and read or watched the news more than 10 times (median control category = 8–10 times per week, range: 0–10+). In contrast KL reported lower than average exposure in all categories but fell within the control range. He currently listened to the radio 0–1 h per week (a change from 1–5 h per week prior to symptom onset), he read the news once a week, and did not regularly watch television.</p>
    </sec>
    <sec id="app2">
      <label>Appendix B</label>
      <p>Quantification of voice recognition ability: control pilot study.</p>
      <p>A pilot control sample of 26 older controls, 18 females (mean age = 65.5, SD = 7.3, range: 51–82) were tested using voices and face stimuli, obtained from publicly available sources. Familiarity, naming and recognition of 60 famous and 60 unfamiliar voices and faces were assessed, presenting the same public figures in both modalities. Order of presentation of faces and voices was balanced between subjects. One control subject from the pilot study was excluded, based on performance greater than 2 standard deviations below the control mean on all tests of voice and face identification; this may reflect a developmental or acquired difficulty with person recognition.</p>
      <p>To select items for the final study, the same public figures were used in three modalities to directly compare knowledge of representations from voice, face and name. As the voice is the modality of interest, the 24 public figures best recognised from voice by pilot study controls were selected; these items were all recognised by 60–100% of controls and for consistency across items, all voices selected were recognised more easily from face than from voice. A selection of unfamiliar voices and faces that were classified as unfamiliar by &gt;75% of controls were also selected for the final test.</p>
      <p>The pilot control sample scores for naming and recognition of the 24 voices and faces used in the final study were combined with scores from the main study control sample. Statistics for the total control sample of 48 controls, 33 females (mean age = 64.8, SD = 5.9, range: 51–82) were calculated. Voice identification scores (naming: mean = 16.2, SD = 4.9, range: 5–23; recognition: mean = 18.4, SD = 4.6, range: 5–24) were lower than scores for identification of the faces (naming: mean = 20.1, SD = 3.8, range: 8–24; recognition: mean = 22.8, SD = 2.0, range: 15–24). Paired <italic>t</italic>-tests were used to assess the difference in control scores between face identification and voice identification scores. Face naming scores were significantly greater than voice naming scores (<italic>t</italic> = 7.1, <italic>p</italic> &lt; 0.001, df = 47) and face recognition scores were significantly greater than voice recognition scores (<italic>t</italic> = 7.9, <italic>p</italic> &lt; 0.001, df = 47).</p>
      <p>In order to assess the relationship between background control variables and voice and face identification test scores, we tested univariate associations between the effects of age, sex, number of years of education, IQ (measured on the NART), naming ability (assessed on the GNT), and media exposure categorical data (see <xref rid="app1" ref-type="sec">Appendix A</xref>): for television watching (hours per week), radio listening (hours per week) and news exposure (see <xref rid="app1" ref-type="sec">Appendix A</xref>) on famous voice naming, voice recognition, face naming and face recognition test scores, respectively. To allow for violations of the normality assumption, bias-corrected bootstrapped confidence intervals (CI) were used.</p>
      <p>There was statistically significant evidence that an increase in exposure to the news was associated with an increase in voice recognition score (coefficient = 1.42 (CI 0.21, 2.86)). An increase in one news exposure category (categorised by the number of times a subject read or watched the news per week) resulted in 1.42 more public figures recognised by voice. There was no evidence of an association between voice recognition and any other background variables (age, sex, number of years of education, NART IQ, GNT score, television watching or radio listening). An increase in naming score on the GNT was significantly associated with an increase in voice naming score (coefficient = 0.58 (CI 0.11, 1.11)), an increase in face naming score (coefficient = 0.74 (CI 0.42, 1.10)) and an increase in face recognition score (coefficient = 0.22 (CI 0.04, 0.44)), respectively. An increase in age was statistically significantly associated with a decrease in: voice naming score (coefficient = −0.30 (CI −0.55, −0.08)), face naming score (coefficient = −0.30 (CI −0.50, −0.13)) and face recognition score (coefficient = −0.08 (CI −0.19, −0.01)). There was no evidence of association between any other background control variable (sex, number of years of education, NART IQ, TV watching or radio listening) and voice naming, face recognition or face naming scores.</p>
    </sec>
    <sec id="app3">
      <label>Appendix C</label>
      <p>Lists of the public figures selected for Experiments 1 and 2 and faces difficulty matched to voices in Experiment 3.<table-wrap position="anchor" id="N0x1d53150N0x2de91c8"><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Experiment 1: main study figures</th><th align="left">Experiment 3: difficulty matched faces</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Alan Bennett</td><td align="left">Alan Titchmarsh</td></tr><tr><td align="left">2</td><td align="left">Ann Widdecombe</td><td align="left">Anne Robinson</td></tr><tr><td align="left">3</td><td align="left">Bill Clinton</td><td align="left">Anthony Hopkins</td></tr><tr><td align="left">4</td><td align="left">Billy Connolly</td><td align="left">Barbara Windsor</td></tr><tr><td align="left">5</td><td align="left">Bob Geldof</td><td align="left">Bruce Forsyth</td></tr><tr><td align="left">6</td><td align="left">David Attenborough</td><td align="left">Charles Kennedy</td></tr><tr><td align="left">7</td><td align="left">Edward Heath</td><td align="left">Cilla Black</td></tr><tr><td align="left">8</td><td align="left">George Bush</td><td align="left">David Cameron</td></tr><tr><td align="left">9</td><td align="left">Gordon Brown</td><td align="left">David Frost</td></tr><tr><td align="left">10</td><td align="left">Ian Paisley</td><td align="left">David Jason</td></tr><tr><td align="left">11</td><td align="left">Janet Street-Porter</td><td align="left">Dolly Parton</td></tr><tr><td align="left">12</td><td align="left">Joanna Lumley</td><td align="left">Hugh Grant</td></tr><tr><td align="left">13</td><td align="left">John Humphreys</td><td align="left">Jack Nicholson</td></tr><tr><td align="left">14</td><td align="left">John Major</td><td align="left">Jeremy Clarkson</td></tr><tr><td align="left">15</td><td align="left">Jonathan Ross</td><td align="left">Jimmy Carter</td></tr><tr><td align="left">16</td><td align="left">Judi Dench</td><td align="left">John Cleese</td></tr><tr><td align="left">17</td><td align="left">Kenneth Williams</td><td align="left">John Snow</td></tr><tr><td align="left">18</td><td align="left">Margaret Thatcher</td><td align="left">Michael Caine</td></tr><tr><td align="left">19</td><td align="left">Neil Kinnock</td><td align="left">Michael Portillo</td></tr><tr><td align="left">20</td><td align="left">Prince Charles (written name: Charles Windsor)</td><td align="left">Moira Stewart</td></tr><tr><td align="left">21</td><td align="left">Princess Diana (written name: Diana Spencer)</td><td align="left">Robbie Coltrane</td></tr><tr><td align="left">22</td><td align="left">Ronnie Corbett</td><td align="left">Sean Connery</td></tr><tr><td align="left">23</td><td align="left">Terry Wogan</td><td align="left">Stephen Fry</td></tr><tr><td align="left">24</td><td align="left">Tony Blair</td><td align="left">Woody Allen</td></tr></tbody></table></table-wrap></p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>We are grateful to all the subjects for their participation. We thank Dr. Doris-Eva Bamiou for assistance with audiometric assessments, Jonathan Bartlett MSc for statistical advice and Prof. Sophie Scott and Dr. Disa Sauter for making available the vocal emotion stimuli. We thank Prof. EK Warrington for helpful discussion. This work was undertaken at UCLH/UCL who received a proportion of funding from the Department of Health's NIHR Biomedical Research Centres funding scheme. The Dementia Research Centre is an Alzheimer's Research Trust Co-ordinating Centre. This work was funded by the Wellcome Trust and by the UK Medical Research Council. SJC is supported by an Alzheimer's Research Trust Fellowship. JDW is supported by a Wellcome Trust Intermediate Clinical Fellowship.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Barton</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>Disorders of face perception and recognition</article-title>
          <source>Neurologic Clinics</source>
          <year>2003</year>
          <volume>21</volume>
          <fpage>521</fpage>
          <lpage>548</lpage>
          <pub-id pub-id-type="pmid">12916490</pub-id>
        </citation>
      </ref>
      <ref id="bib2">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Fecteau</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bedard</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Thinking the voice: Neural correlates of voice perception</article-title>
          <source>Trends in Cognitive Sciences</source>
          <year>2004</year>
          <volume>8</volume>
          <fpage>129</fpage>
          <lpage>135</lpage>
          <pub-id pub-id-type="pmid">15301753</pub-id>
        </citation>
      </ref>
      <ref id="bib3">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Ahad</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Human temporal-lobe response to vocal sounds</article-title>
          <source>Cognitive Brain Research</source>
          <year>2002</year>
          <volume>13</volume>
          <fpage>17</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="pmid">11867247</pub-id>
        </citation>
      </ref>
      <ref id="bib4">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Lafaille</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ahad</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pike</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Voice-selective areas in human auditory cortex</article-title>
          <source>Nature</source>
          <year>2000</year>
          <volume>403</volume>
          <fpage>309</fpage>
          <lpage>312</lpage>
          <pub-id pub-id-type="pmid">10659849</pub-id>
        </citation>
      </ref>
      <ref id="bib5">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Benton</surname>
              <given-names>A.L.</given-names>
            </name>
            <name>
              <surname>Hamsher</surname>
              <given-names>K.S.</given-names>
            </name>
            <name>
              <surname>Varney</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Spreen</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Contributions to neuropsychological assessment: A clinical manual</article-title>
          <year>1989</year>
          <publisher-name>Oxford University Press</publisher-name>
          <publisher-loc>Oxford</publisher-loc>
        </citation>
      </ref>
      <ref id="bib6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Benzagmout</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chaoui</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Duffau</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Reversible deficit affecting the perception of tone of a human voice after tumour resection from the right auditory cortex</article-title>
          <source>Acta Neurochirurgica</source>
          <year>2008</year>
          <volume>150</volume>
          <fpage>589</fpage>
          <lpage>593</lpage>
          <pub-id pub-id-type="pmid">18431530</pub-id>
        </citation>
      </ref>
      <ref id="bib7">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruce</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Young</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Understanding face recognition</article-title>
          <source>British Journal of Psychology</source>
          <year>1986</year>
          <volume>77</volume>
          <fpage>305</fpage>
          <lpage>327</lpage>
          <pub-id pub-id-type="pmid">3756376</pub-id>
        </citation>
      </ref>
      <ref id="bib8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Burton</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Bruce</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Naming faces and naming names: Exploring an interactive activation model of person recognition</article-title>
          <source>Memory</source>
          <year>1993</year>
          <volume>1</volume>
          <fpage>457</fpage>
          <lpage>480</lpage>
          <pub-id pub-id-type="pmid">7584282</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Burton</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Bruce</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Johnston</surname>
              <given-names>R.A.</given-names>
            </name>
          </person-group>
          <article-title>Understanding face recognition with an interactive activation model</article-title>
          <source>British Journal of Psychology</source>
          <year>1990</year>
          <volume>81</volume>
          <fpage>361</fpage>
          <lpage>380</lpage>
          <pub-id pub-id-type="pmid">2224396</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Pijnenburg</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Whitwell</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Barnes</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Scahill</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>The clinical profile of right temporal lobe atrophy</article-title>
          <source>Brain</source>
          <year>2009</year>
          <volume>132</volume>
          <fpage>1287</fpage>
          <lpage>1298</lpage>
          <pub-id pub-id-type="pmid">19297506</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crawford</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Howell</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Comparing an individual's test score against norms derived from small samples</article-title>
          <source>The Clinical Neuropsychologist</source>
          <year>1998</year>
          <volume>12</volume>
          <fpage>482</fpage>
          <lpage>486</lpage>
        </citation>
      </ref>
      <ref id="bib12">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crutch</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
          </person-group>
          <article-title>The semantic organisation of proper nouns: The case of people and brand names</article-title>
          <source>Neuropsychologia</source>
          <year>2004</year>
          <volume>42</volume>
          <fpage>584</fpage>
          <lpage>596</lpage>
          <pub-id pub-id-type="pmid">14725797</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Damjanovic</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Hanley</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Recalling episodic and semantic information about famous faces and voices</article-title>
          <source>Memory and Cognition</source>
          <year>2007</year>
          <volume>35</volume>
          <fpage>1205</fpage>
          <lpage>1210</lpage>
        </citation>
      </ref>
      <ref id="bib14">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>De Renzi</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Faglioni</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Grossi</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Nichelli</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Apperceptive and associative forms of prosopagnosia</article-title>
          <source>Cortex</source>
          <year>1991</year>
          <volume>27</volume>
          <fpage>213</fpage>
          <lpage>221</lpage>
          <pub-id pub-id-type="pmid">1879150</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Delis</surname>
              <given-names>D.C.</given-names>
            </name>
            <name>
              <surname>Kaplan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kramer</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Delis-Kaplan executive function system</article-title>
          <year>2001</year>
          <publisher-name>The Psychological Corporation</publisher-name>
          <publisher-loc>San Antonio</publisher-loc>
        </citation>
      </ref>
      <ref id="bib16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Duchaine</surname>
              <given-names>B.C.</given-names>
            </name>
            <name>
              <surname>Nakayama</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Developmental prosopagnosia: A window to content-specific face processing</article-title>
          <source>Current Opinion in Neurobiology</source>
          <year>2006</year>
          <volume>16</volume>
          <fpage>166</fpage>
          <lpage>173</lpage>
          <pub-id pub-id-type="pmid">16563738</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ellis</surname>
              <given-names>A.W.</given-names>
            </name>
            <name>
              <surname>Young</surname>
              <given-names>A.W.</given-names>
            </name>
            <name>
              <surname>Critchley</surname>
              <given-names>E.M.</given-names>
            </name>
          </person-group>
          <article-title>Loss of memory for people following temporal lobe damage</article-title>
          <source>Brain</source>
          <year>1989</year>
          <volume>112</volume>
          <fpage>1469</fpage>
          <lpage>1483</lpage>
          <pub-id pub-id-type="pmid">2597991</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ellis</surname>
              <given-names>H.D.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>D.M.</given-names>
            </name>
            <name>
              <surname>Mosdell</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Intra- and inter-modal repetition priming of familiar faces and voices</article-title>
          <source>British Journal of Psychology</source>
          <year>1997</year>
          <volume>88</volume>
          <fpage>143</fpage>
          <lpage>156</lpage>
          <pub-id pub-id-type="pmid">9061895</pub-id>
        </citation>
      </ref>
      <ref id="bib19">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Evans</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Heggs</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Antoun</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Hodges</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Progressive prosopagnosia associated with selective right temporal lobe atrophy. A new syndrome?</article-title>
          <source>Brain</source>
          <year>1995</year>
          <volume>118</volume>
          <fpage>1</fpage>
          <lpage>13</lpage>
          <pub-id pub-id-type="pmid">7894996</pub-id>
        </citation>
      </ref>
      <ref id="bib20">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gainotti</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Different patterns of famous people recognition disorders in patients with right and left anterior temporal lesions: A systematic review</article-title>
          <source>Neuropsychologia</source>
          <year>2007</year>
          <volume>45</volume>
          <fpage>1591</fpage>
          <lpage>1607</lpage>
          <pub-id pub-id-type="pmid">17275042</pub-id>
        </citation>
      </ref>
      <ref id="bib21">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gainotti</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Face familiarity feelings, the right temporal lobe and the possible underlying neural mechanisms</article-title>
          <source>Brain Research Reviews</source>
          <year>2007</year>
          <volume>56</volume>
          <fpage>214</fpage>
          <lpage>235</lpage>
          <pub-id pub-id-type="pmid">17822771</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gainotti</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Barbier</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Marra</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Slowly progressive defect in recognition of familiar people in a patient with right anterior temporal atrophy</article-title>
          <source>Brain</source>
          <year>2003</year>
          <volume>126</volume>
          <fpage>792</fpage>
          <lpage>803</lpage>
          <pub-id pub-id-type="pmid">12615639</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gainotti</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Ferraccioli</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Quaranta</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Marra</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Cross-modal recognition disorders for persons and other unique entities in a patient with right fronto-temporal degeneration</article-title>
          <source>Cortex</source>
          <year>2008</year>
          <volume>44</volume>
          <fpage>238</fpage>
          <lpage>248</lpage>
          <pub-id pub-id-type="pmid">18387554</pub-id>
        </citation>
      </ref>
      <ref id="bib24">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Garrido</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Eisner</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>McGettigan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Stewart</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Sauter</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Hanley</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Developmental phonagnosia: A selective deficit of vocal identity recognition</article-title>
          <source>Neuropsychologia</source>
          <year>2009</year>
          <volume>47</volume>
          <fpage>123</fpage>
          <lpage>131</lpage>
          <pub-id pub-id-type="pmid">18765243</pub-id>
        </citation>
      </ref>
      <ref id="bib25">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gentileschi</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sperber</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Spinnler</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Progressive defective recognition of familiar people</article-title>
          <source>Neurocase</source>
          <year>1999</year>
          <volume>5</volume>
          <fpage>407</fpage>
          <lpage>424</lpage>
        </citation>
      </ref>
      <ref id="bib26">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gentileschi</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sperber</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Spinnler</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Crossmodal agnosia for familiar people as a consequence of right infero-polar temporal atrophy</article-title>
          <source>Cognitive Neuropsychology</source>
          <year>2001</year>
          <volume>18</volume>
          <fpage>439</fpage>
          <lpage>463</lpage>
        </citation>
      </ref>
      <ref id="bib27">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Griffiths</surname>
              <given-names>T.D.</given-names>
            </name>
            <name>
              <surname>Warren</surname>
              <given-names>J.D.</given-names>
            </name>
          </person-group>
          <article-title>What is an auditory object?</article-title>
          <source>Nature Reviews Neuroscience</source>
          <year>2004</year>
          <volume>5</volume>
          <fpage>887</fpage>
          <lpage>892</lpage>
        </citation>
      </ref>
      <ref id="bib28">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Halpern</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Bartlett</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Dowling</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>Aging and experience in the recognition of musical transpositions</article-title>
          <source>Psychology and Aging</source>
          <year>1995</year>
          <volume>10</volume>
          <fpage>325</fpage>
          <lpage>342</lpage>
          <pub-id pub-id-type="pmid">8527054</pub-id>
        </citation>
      </ref>
      <ref id="bib29">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hanley</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.T.</given-names>
            </name>
            <name>
              <surname>Hadfield</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>I recognise you but I can’t place you: An investigation of familiar-only experiences during tests of voice and face recognition</article-title>
          <source>Quarterly Journal of Experimental Psychology Section A: Human Experimental Psychology</source>
          <year>1998</year>
          <volume>51</volume>
          <fpage>179</fpage>
          <lpage>195</lpage>
        </citation>
      </ref>
      <ref id="bib30">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hanley</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Young</surname>
              <given-names>A.W.</given-names>
            </name>
            <name>
              <surname>Pearson</surname>
              <given-names>N.A.</given-names>
            </name>
          </person-group>
          <article-title>Defective recognition of familiar people</article-title>
          <source>Cognitive Neuropsychology</source>
          <year>1989</year>
          <volume>6</volume>
          <fpage>179</fpage>
          <lpage>210</lpage>
        </citation>
      </ref>
      <ref id="bib31">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Imaizumi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mori</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kiritani</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kawashima</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sugiura</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fukuda</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Vocal identification of speaker and emotion activates different brain regions</article-title>
          <source>Neuroreport</source>
          <year>1997</year>
          <volume>8</volume>
          <fpage>2809</fpage>
          <lpage>2812</lpage>
          <pub-id pub-id-type="pmid">9295122</pub-id>
        </citation>
      </ref>
      <ref id="bib32">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ives</surname>
              <given-names>D.T.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>D.R.R.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>R.D.</given-names>
            </name>
          </person-group>
          <article-title>Discrimination of speaker size from syllable phrases</article-title>
          <source>Journal of the Acoustical Society of America</source>
          <year>2005</year>
          <volume>118</volume>
          <fpage>3816</fpage>
          <lpage>3822</lpage>
          <pub-id pub-id-type="pmid">16419826</pub-id>
        </citation>
      </ref>
      <ref id="bib33">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Josephs</surname>
              <given-names>K.A.</given-names>
            </name>
            <name>
              <surname>Whitwell</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Vemuri</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Senjem</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Boeve</surname>
              <given-names>B.F.</given-names>
            </name>
            <name>
              <surname>Knopman</surname>
              <given-names>D.S.</given-names>
            </name>
          </person-group>
          <article-title>The anatomic correlate of prosopagnosia in semantic dementia</article-title>
          <source>Neurology</source>
          <year>2008</year>
          <volume>71</volume>
          <fpage>1628</fpage>
          <lpage>1633</lpage>
          <pub-id pub-id-type="pmid">19001253</pub-id>
        </citation>
      </ref>
      <ref id="bib34">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Joubert</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Felician</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Barbeau</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sontheimer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Barton</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Ceccaldi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Impaired configurational processing in a case of progressive prosopagnosia associated with predominant right temporal lobe atrophy</article-title>
          <source>Brain</source>
          <year>2003</year>
          <volume>126</volume>
          <fpage>2537</fpage>
          <lpage>2550</lpage>
          <pub-id pub-id-type="pmid">14506066</pub-id>
        </citation>
      </ref>
      <ref id="bib35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Joubert</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Felician</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Barbeau</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sontheimer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Guedj</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ceccaldi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Progressive prosopagnosia: Clinical and neuroimaging results</article-title>
          <source>Neurology</source>
          <year>2004</year>
          <volume>63</volume>
          <fpage>1962</fpage>
          <lpage>1965</lpage>
          <pub-id pub-id-type="pmid">15557526</pub-id>
        </citation>
      </ref>
      <ref id="bib36">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kawahara</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Irino</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Underlying principles of a high-quality speech manipulation system STRAIGHT and its application to speech segregation</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Divenyi</surname>
              <given-names>P.L.</given-names>
            </name>
          </person-group>
          <source>Speech separation by humans and machines</source>
          <year>2004</year>
          <publisher-name>Kluwer Academic</publisher-name>
          <publisher-loc>Massachusetts</publisher-loc>
          <fpage>167</fpage>
          <lpage>180</lpage>
        </citation>
      </ref>
      <ref id="bib37">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Keane</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Calder</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Hodges</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Young</surname>
              <given-names>A.W.</given-names>
            </name>
          </person-group>
          <article-title>Face and emotion processing in frontal variant frontotemporal dementia</article-title>
          <source>Neuropsychologia</source>
          <year>2002</year>
          <volume>40</volume>
          <fpage>655</fpage>
          <lpage>665</lpage>
          <pub-id pub-id-type="pmid">11792405</pub-id>
        </citation>
      </ref>
      <ref id="bib38">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lang</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Kneidl</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Hielscher-Fastabend</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Heckmann</surname>
              <given-names>J.G.</given-names>
            </name>
          </person-group>
          <article-title>Voice recognition in aphasic and non-aphasic stroke patients</article-title>
          <source>Journal of Neurology</source>
          <year>2009</year>
          <volume>256</volume>
          <fpage>1303</fpage>
          <lpage>1306</lpage>
          <pub-id pub-id-type="pmid">19353219</pub-id>
        </citation>
      </ref>
      <ref id="bib39">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lucchelli</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Spinnler</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>A reappraisal of person recognition and identification</article-title>
          <source>Cortex</source>
          <year>2008</year>
          <volume>44</volume>
          <fpage>230</fpage>
          <lpage>237</lpage>
          <pub-id pub-id-type="pmid">18387553</pub-id>
        </citation>
      </ref>
      <ref id="bib40">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lyons</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Kay</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hanley</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Haslam</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Selective preservation of memory for people in the context of semantic memory disorder: Patterns of association and dissociation</article-title>
          <source>Neuropsychologia</source>
          <year>2006</year>
          <volume>44</volume>
          <fpage>2887</fpage>
          <lpage>2898</lpage>
          <pub-id pub-id-type="pmid">16876829</pub-id>
        </citation>
      </ref>
      <ref id="bib41">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mahon</surname>
              <given-names>B.Z.</given-names>
            </name>
            <name>
              <surname>Anzellotti</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Schwarzbach</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zampini</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Caramazza</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Category-specific organization in the human brain does not require visual experience</article-title>
          <source>Neuron</source>
          <year>2009</year>
          <volume>63</volume>
          <fpage>397</fpage>
          <lpage>405</lpage>
          <pub-id pub-id-type="pmid">19679078</pub-id>
        </citation>
      </ref>
      <ref id="bib42">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nakamura</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kawashima</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sugiura</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kato</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Nakamura</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hatano</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Neural substrates for recognition of familiar voices: A PET study</article-title>
          <source>Neuropsychologia</source>
          <year>2001</year>
          <volume>39</volume>
          <fpage>1047</fpage>
          <lpage>1054</lpage>
          <pub-id pub-id-type="pmid">11440757</pub-id>
        </citation>
      </ref>
      <ref id="bib43">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Nelson</surname>
              <given-names>H.E.</given-names>
            </name>
          </person-group>
          <article-title>National adult reading test (NART): Test manual</article-title>
          <year>1982</year>
          <publisher-name>NFER Nelson</publisher-name>
          <publisher-loc>Windsor</publisher-loc>
        </citation>
      </ref>
      <ref id="bib44">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Neuner</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Schweinberger</surname>
              <given-names>S.R.</given-names>
            </name>
          </person-group>
          <article-title>Neuropsychological impairments in the recognition of faces, voices, and personal names</article-title>
          <source>Brain and Cognition</source>
          <year>2000</year>
          <volume>44</volume>
          <fpage>342</fpage>
          <lpage>366</lpage>
          <pub-id pub-id-type="pmid">11104530</pub-id>
        </citation>
      </ref>
      <ref id="bib45">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peretz</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kolinsky</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tramo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Labrecque</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hublet</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Demeurisse</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Functional dissociations following bilateral lesions of auditory cortex</article-title>
          <source>Brain</source>
          <year>1994</year>
          <volume>117</volume>
          <fpage>1283</fpage>
          <lpage>1301</lpage>
          <pub-id pub-id-type="pmid">7820566</pub-id>
        </citation>
      </ref>
      <ref id="bib46">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Perrachione</surname>
              <given-names>T.K.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>P.C.</given-names>
            </name>
          </person-group>
          <article-title>Learning to recognize speakers of a non-native language: Implications for the functional organization of human auditory cortex</article-title>
          <source>Neuropsychologia</source>
          <year>2007</year>
          <volume>45</volume>
          <fpage>1899</fpage>
          <lpage>1910</lpage>
          <pub-id pub-id-type="pmid">17258240</pub-id>
        </citation>
      </ref>
      <ref id="bib47">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Remez</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Fellowes</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Rubin</surname>
              <given-names>P.E.</given-names>
            </name>
          </person-group>
          <article-title>Talker identification based on phonetic information</article-title>
          <source>Journal of Experimental Psychology: Human Perception and Performance</source>
          <year>1997</year>
          <volume>23</volume>
          <fpage>651</fpage>
          <lpage>666</lpage>
          <pub-id pub-id-type="pmid">9180039</pub-id>
        </citation>
      </ref>
      <ref id="bib48">
        <citation citation-type="other">Sauter D. A., Calder A. J., Eisner F., &amp; Scott S. K. (in press). Perceptual cues in non-verbal vocal expressions of emotion. <italic>Quarterly Journal of Experimental Psychology</italic>.</citation>
      </ref>
      <ref id="bib49">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sauter</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
          </person-group>
          <article-title>More than one kind of happiness: Can we recognize vocal expression of different positive states?</article-title>
          <source>Motivation and Emotion</source>
          <year>2007</year>
          <volume>31</volume>
          <fpage>192</fpage>
          <lpage>199</lpage>
        </citation>
      </ref>
      <ref id="bib50">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schweinberger</surname>
              <given-names>S.R.</given-names>
            </name>
          </person-group>
          <article-title>Human brain potential correlates of voice priming and voice recognition</article-title>
          <source>Neuropsychologia</source>
          <year>2001</year>
          <volume>39</volume>
          <fpage>921</fpage>
          <lpage>936</lpage>
          <pub-id pub-id-type="pmid">11516445</pub-id>
        </citation>
      </ref>
      <ref id="bib51">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Snowden</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Austin</surname>
              <given-names>N.A.</given-names>
            </name>
            <name>
              <surname>Sembi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Craufurd</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Neary</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Emotion recognition in Huntington's disease and frontotemporal dementia</article-title>
          <source>Neuropsychologia</source>
          <year>2008</year>
          <volume>46</volume>
          <fpage>2638</fpage>
          <lpage>2649</lpage>
          <pub-id pub-id-type="pmid">18533200</pub-id>
        </citation>
      </ref>
      <ref id="bib52">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Snowden</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Neary</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Knowledge of famous faces and names in semantic dementia</article-title>
          <source>Brain</source>
          <year>2004</year>
          <volume>127</volume>
          <fpage>860</fpage>
          <lpage>872</lpage>
          <pub-id pub-id-type="pmid">14985259</pub-id>
        </citation>
      </ref>
      <ref id="bib53">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Thompson</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Graham</surname>
              <given-names>K.S.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Patterson</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kapur</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Hodges</surname>
              <given-names>J.R.</given-names>
            </name>
          </person-group>
          <article-title>Dissociating person-specific from general semantic knowledge: Roles of the left and right temporal lobes</article-title>
          <source>Neuropsychologia</source>
          <year>2004</year>
          <volume>42</volume>
          <fpage>359</fpage>
          <lpage>370</lpage>
          <pub-id pub-id-type="pmid">14670574</pub-id>
        </citation>
      </ref>
      <ref id="bib54">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tyrrell</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Rossor</surname>
              <given-names>M.N.</given-names>
            </name>
          </person-group>
          <article-title>Progressive degeneration of the right temporal lobe studied with positron emission tomography</article-title>
          <source>Journal of Neurology Neurosurgery, and Psychiatry</source>
          <year>1990</year>
          <volume>53</volume>
          <fpage>1046</fpage>
          <lpage>1050</lpage>
        </citation>
      </ref>
      <ref id="bib55">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Voice discrimination and recognition are separate abilities</article-title>
          <source>Neuropsychologia</source>
          <year>1987</year>
          <volume>25</volume>
          <fpage>829</fpage>
          <lpage>834</lpage>
          <pub-id pub-id-type="pmid">3431677</pub-id>
        </citation>
      </ref>
      <ref id="bib56">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Canter</surname>
              <given-names>G.J.</given-names>
            </name>
          </person-group>
          <article-title>Impairment of voice and face recognition in patients with hemispheric damage</article-title>
          <source>Brain and Cognition</source>
          <year>1982</year>
          <volume>1</volume>
          <fpage>185</fpage>
          <lpage>195</lpage>
          <pub-id pub-id-type="pmid">6927560</pub-id>
        </citation>
      </ref>
      <ref id="bib57">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Cummings</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dobkin</surname>
              <given-names>B.H.</given-names>
            </name>
          </person-group>
          <article-title>Phonagnosia: A dissociation between familiar and unfamiliar voices</article-title>
          <source>Cortex</source>
          <year>1988</year>
          <volume>24</volume>
          <fpage>195</fpage>
          <lpage>209</lpage>
          <pub-id pub-id-type="pmid">3416603</pub-id>
        </citation>
      </ref>
      <ref id="bib58">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cummings</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Familiar voice recognition: Patterns and parameters</article-title>
          <source>Journal of Phonetics</source>
          <year>1985</year>
          <volume>13</volume>
          <fpage>19</fpage>
          <lpage>38</lpage>
        </citation>
      </ref>
      <ref id="bib59">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Van Lancker</surname>
              <given-names>D.R.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cummings</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Voice perception deficits: Neuroanatomical correlates of phonagnosia</article-title>
          <source>Journal of Clinical and Experimental Neuropsychology</source>
          <year>1989</year>
          <volume>11</volume>
          <fpage>665</fpage>
          <lpage>674</lpage>
          <pub-id pub-id-type="pmid">2808656</pub-id>
        </citation>
      </ref>
      <ref id="bib60">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Eger</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>A.L.</given-names>
            </name>
          </person-group>
          <article-title>Modulation of neural responses to speech by directing attention to voices or verbal content</article-title>
          <source>Cognitive Brain Research</source>
          <year>2003</year>
          <volume>17</volume>
          <fpage>48</fpage>
          <lpage>55</lpage>
          <pub-id pub-id-type="pmid">12763191</pub-id>
        </citation>
      </ref>
      <ref id="bib61">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>A.L.</given-names>
            </name>
          </person-group>
          <article-title>Voice recognition and cross-modal responses to familiar speakers’ voices in prosopagnosia</article-title>
          <source>Cerebral Cortex</source>
          <year>2006</year>
          <volume>16</volume>
          <fpage>1314</fpage>
          <lpage>1322</lpage>
          <pub-id pub-id-type="pmid">16280461</pub-id>
        </citation>
      </ref>
      <ref id="bib62">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sterzer</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>A.L.</given-names>
            </name>
          </person-group>
          <article-title>Interaction of face and voice areas during speaker recognition</article-title>
          <source>Journal of Cognitive Neuroscience</source>
          <year>2005</year>
          <volume>17</volume>
          <fpage>367</fpage>
          <lpage>376</lpage>
          <pub-id pub-id-type="pmid">15813998</pub-id>
        </citation>
      </ref>
      <ref id="bib63">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Warren</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Griffiths</surname>
              <given-names>T.D.</given-names>
            </name>
          </person-group>
          <article-title>Human brain mechanisms for the early analysis of voices</article-title>
          <source>Neuroimage</source>
          <year>2006</year>
          <volume>31</volume>
          <fpage>1389</fpage>
          <lpage>1397</lpage>
          <pub-id pub-id-type="pmid">16540351</pub-id>
        </citation>
      </ref>
      <ref id="bib64">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
          </person-group>
          <article-title>Neuropsychological evidence for multiple memory systems</article-title>
          <source>Ciba Foundation Symposium</source>
          <year>1979</year>
          <fpage>153</fpage>
          <lpage>166</lpage>
          <pub-id pub-id-type="pmid">261644</pub-id>
        </citation>
      </ref>
      <ref id="bib65">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
          </person-group>
          <article-title>The graded naming test: A restandardisation</article-title>
          <source>Neuropsychological Rehabilitation</source>
          <year>1997</year>
          <volume>7</volume>
          <fpage>143</fpage>
          <lpage>146</lpage>
        </citation>
      </ref>
      <ref id="bib66">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
            <name>
              <surname>James</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>An experimental study of facial recognition in patients with unilateral cerebral lesions</article-title>
          <source>Cortex</source>
          <year>1967</year>
          <volume>3</volume>
          <fpage>317</fpage>
          <lpage>326</lpage>
        </citation>
      </ref>
      <ref id="bib67">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
            <name>
              <surname>McKenna</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Orpwood</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Single word comprehension: A concrete and abstract word synonym test</article-title>
          <source>Neuropsychological Rehabilitation</source>
          <year>1998</year>
          <volume>8</volume>
          <fpage>143</fpage>
          <lpage>154</lpage>
        </citation>
      </ref>
      <ref id="bib68">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Whiteley</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Warrington</surname>
              <given-names>E.K.</given-names>
            </name>
          </person-group>
          <article-title>Selective impairment of topographical memory: A single case study</article-title>
          <source>Journal of Neurology Neurosurgery, and Psychiatry</source>
          <year>1978</year>
          <volume>41</volume>
          <fpage>575</fpage>
          <lpage>578</lpage>
        </citation>
      </ref>
      <ref id="bib69">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Young</surname>
              <given-names>A.W.</given-names>
            </name>
            <name>
              <surname>Newcombe</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>de Haan</surname>
              <given-names>E.H.</given-names>
            </name>
            <name>
              <surname>Small</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hay</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Face perception after brain injury. Selective impairments affecting identity and expression</article-title>
          <source>Brain</source>
          <year>1993</year>
          <volume>116</volume>
          <fpage>941</fpage>
          <lpage>959</lpage>
          <pub-id pub-id-type="pmid">8353717</pub-id>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Representative T1-weighted coronal brain MRI sections from each patient (the right hemisphere is shown on the left side of each image). Sections have been selected to show the following regions of potential relevance to voice processing deficits: a, frontal lobes; b, temporal poles; c, anterior temporal lobes; d, mid-temporal lobes including Heschl's gyri; e, temporo-parietal junction. Focal cerebral atrophy is shown in both patients: in QR, bilateral fronto-temporal atrophy accentuated in the right anterior temporal lobe and extending posteriorly and including the superior temporal sulcus; and in KL, bilateral predominantly anterior temporal lobe atrophy, more marked on the right side and in the inferior temporal cortices including the fusiform gyrus.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <table-wrap position="float" id="tbl1">
      <label>Table 1</label>
      <caption>
        <p>Summary of patient and control performance on background neuropsychological assessment.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">QR score (percentile)</th>
            <th align="left">KL score (percentile)</th>
            <th align="left">Controls mean (SD) <italic>n</italic> = 23</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="4" align="left">General neuropsychological tests</td>
          </tr>
          <tr>
            <td align="left"> NART full scale IQ</td>
            <td align="char">86</td>
            <td align="char">113</td>
            <td align="left">120.9 (6.3)</td>
          </tr>
          <tr>
            <td align="left"> MMSE (/30)</td>
            <td align="char">28</td>
            <td align="char">25</td>
            <td align="left">n/d</td>
          </tr>
          <tr>
            <td align="left"> WAIS III Digit Span (forwards, backwards)</td>
            <td align="char">12, 5 (60–80th)</td>
            <td align="char">14, 5 (80–95th)</td>
            <td align="left">n/d</td>
          </tr>
          <tr>
            <td align="left"> Graded Naming Test (/30)</td>
            <td align="char">4 (&lt;5th)</td>
            <td align="char">6 (&lt;5th)</td>
            <td align="left">26.0 (2.0)</td>
          </tr>
          <tr>
            <td align="left"> Synonyms—concrete (/25)</td>
            <td align="char">17 (10th)<sup>b</sup></td>
            <td align="char">21 (50th)<sup>b</sup></td>
            <td align="left">24.5 (0.7)</td>
          </tr>
          <tr>
            <td align="left"> Synonyms—abstract (/25)</td>
            <td align="char">12 (1–5th)<sup>b</sup></td>
            <td align="char">24 (75–90th)<sup>b</sup></td>
            <td align="left">24.3 (0.8)</td>
          </tr>
          <tr>
            <td align="left"> Object Decision task (/20)</td>
            <td align="char">19 (75–90th)</td>
            <td align="char">18 (50–75th)</td>
            <td align="left">17.8 (1.9)</td>
          </tr>
          <tr>
            <td align="left"> D-KEFS Design Fluency task: switching</td>
            <td align="char">1 (&lt;5th)</td>
            <td align="char">6 (50–75th)</td>
            <td align="left">n/d</td>
          </tr>
          <tr>
            <td colspan="4" align="left">  </td>
          </tr>
          <tr>
            <td align="left">Identification of unique visual entities</td>
            <td/>
            <td/>
            <td align="left"><italic>n</italic> = 17</td>
          </tr>
          <tr>
            <td align="left"> Famous faces test: naming (/12)</td>
            <td align="char">4 (5th)</td>
            <td align="char">1 (&lt;5th)</td>
            <td align="left">9.9 (1.7)</td>
          </tr>
          <tr>
            <td align="left"> Famous faces test: recognition (/12)</td>
            <td align="char">8 (10–25th)</td>
            <td align="char">1 (&lt;5th)</td>
            <td align="left">10.8 (1.2)</td>
          </tr>
          <tr>
            <td align="left"> Landmark naming (/15)</td>
            <td align="char">7 (&lt;5th)<sup>a</sup></td>
            <td align="char">6 (&lt;5th)<sup>a</sup></td>
            <td align="left">13.6 (1.7)</td>
          </tr>
          <tr>
            <td align="left"> Landmark recognition (/15)</td>
            <td align="char">8 (5th)<sup>a</sup></td>
            <td align="char">8 (5th)<sup>a</sup></td>
            <td align="left">13.7 (1.4)</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Percentiles calculated from standardised tests, except where marked: a, calculated from previous healthy control sample (<italic>n</italic> = 143); b, test administered with both visual and auditory presentation of words whereas the standardised percentiles are calculated for auditory presentation only. n/d = test not performed.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl2">
      <label>Table 2</label>
      <caption>
        <p>Results of experimental tests of familiarity and identification of public figures from voice, face and name in patients and controls.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">QR</th>
            <th align="left">KL</th>
            <th align="left">Controls <italic>n</italic> = 20–24 mean (SD)</th>
            <th align="left">Control range min–max</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="5" align="left">Voice</td>
          </tr>
          <tr>
            <td align="left"> Voice familiarity (/48) (% correct)</td>
            <td align="char">25 (52%)</td>
            <td align="char">28 (58%)</td>
            <td align="char">40.6 (4.0)</td>
            <td align="char">29–46</td>
          </tr>
          <tr>
            <td align="left"> Voice naming (/24)</td>
            <td align="char">0</td>
            <td align="char">0</td>
            <td align="char">16.7 (4.4)</td>
            <td align="char">7–23</td>
          </tr>
          <tr>
            <td align="left"> Voice recognition (/24)</td>
            <td align="char">0</td>
            <td align="char">0</td>
            <td align="char">18.8 (3.9)</td>
            <td align="char">10–23</td>
          </tr>
          <tr>
            <td align="left"> Cross-modal matching to face/name (/24)</td>
            <td align="char">3</td>
            <td align="char">3</td>
            <td align="char">23.5 (0.9)</td>
            <td align="char">21–24</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Face</td>
          </tr>
          <tr>
            <td align="left"> Face familiarity (/48) (% correct)</td>
            <td align="char">29 (60%)</td>
            <td align="char">31 (64%)</td>
            <td align="char">46.7 (1.6)</td>
            <td align="char">43–48</td>
          </tr>
          <tr>
            <td align="left"> Face naming (/24)</td>
            <td align="char">6</td>
            <td align="char">3</td>
            <td align="char">21.4 (2.7)</td>
            <td align="char">16–24</td>
          </tr>
          <tr>
            <td align="left"> Face recognition (/24)</td>
            <td align="char">17</td>
            <td align="char">4</td>
            <td align="char">23.6 (0.8)</td>
            <td align="char">21–24</td>
          </tr>
          <tr>
            <td align="left"> Cross-modal matching to name (/24)</td>
            <td align="char">19</td>
            <td align="char">11</td>
            <td align="char">24.0 (0.0)</td>
            <td align="char">24–24</td>
          </tr>
          <tr>
            <td align="left"> Difficulty-matched faces: naming (/24)</td>
            <td align="char">6</td>
            <td align="char">1</td>
            <td align="char">14 (6.8)<xref rid="tbl3fn1" ref-type="table-fn">a</xref></td>
            <td align="char">2–24</td>
          </tr>
          <tr>
            <td align="left"> Difficulty-matched faces: recognition (/24)</td>
            <td align="char">13</td>
            <td align="char">1</td>
            <td align="char">19 (5.6)<xref rid="tbl3fn1" ref-type="table-fn">a</xref></td>
            <td align="char">3–24</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Name</td>
          </tr>
          <tr>
            <td align="left"> Name familiarity (/48) (% correct)</td>
            <td align="char">43 (90%)</td>
            <td align="char">33 (69%)</td>
            <td align="char">46.6 (1.6)</td>
            <td align="char">42–48</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tbl3fn1">
          <label>a</label>
          <p>Pilot control sample (<italic>n</italic> = 26) scores for identification of 24 faces (see <xref rid="app2" ref-type="sec">Appendix B</xref>) were used to assess performance on this test.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl3">
      <label>Table 3</label>
      <caption>
        <p>Results of experimental tests of perceptual processing of voices and faces, and recognition of vocal emotion in patients and controls.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">QR</th>
            <th align="left">KL</th>
            <th align="left">Controls <italic>n</italic> = 21 mean (SD)</th>
            <th align="left">Control range min–max</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="5" align="left">Voice perception</td>
          </tr>
          <tr>
            <td align="left"> Gender discrimination (/24)</td>
            <td align="char">24</td>
            <td align="char">24</td>
            <td align="left">n/a</td>
            <td align="left">n/a</td>
          </tr>
          <tr>
            <td align="left"> Size discrimination (/32)</td>
            <td align="char">29</td>
            <td align="char">25</td>
            <td align="left">28.8 (4.7)</td>
            <td align="left">17–32</td>
          </tr>
          <tr>
            <td align="left"> Speaker discrimination (/48)</td>
            <td align="char">39</td>
            <td align="char">33</td>
            <td align="left">35.0 (3.1)</td>
            <td align="left">29–41</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Face perception</td>
          </tr>
          <tr>
            <td align="left"> Benton facial recognition test (/56)</td>
            <td align="char">48</td>
            <td align="char">41</td>
            <td align="left">n/a</td>
            <td align="left">n/a</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Vocal emotion recognition</td>
          </tr>
          <tr>
            <td align="left"> Cross-modal matching to emotion (/40)</td>
            <td align="char">32</td>
            <td align="char">30</td>
            <td align="left">35.1 (3.1)<xref rid="tbl4fn1" ref-type="table-fn">a</xref></td>
            <td align="left">26–39</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>n/a = test not performed.</p>
        </fn>
      </table-wrap-foot>
      <table-wrap-foot>
        <fn id="tbl4fn1">
          <label>a</label>
          <p>Separate control group results (<italic>n</italic> = 22), were used to assess performance on this test.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl4">
      <label>Table 4</label>
      <caption>
        <p>Results of experimental tests of environmental sound and musical instrument identification in patients and controls.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">QR</th>
            <th align="left">KL</th>
            <th align="left">Controls mean (SD) <italic>n</italic> = 12</th>
            <th align="left">Control range min–max</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="5" align="left">Environmental sounds</td>
          </tr>
          <tr>
            <td align="left"> Environmental sound recognition (/40)</td>
            <td align="char">35</td>
            <td align="char">34</td>
            <td align="char">37.1 (2.1)<xref rid="tbl5fn1" ref-type="table-fn">a</xref></td>
            <td align="char">33–39</td>
          </tr>
          <tr>
            <td align="left"> Cross-modal matching to picture/name (/40)</td>
            <td align="char">39</td>
            <td align="char">40</td>
            <td align="char">39.9 (0.3)<xref rid="tbl5fn2" ref-type="table-fn">b</xref></td>
            <td align="char">39–40</td>
          </tr>
          <tr>
            <td colspan="5" align="left">  </td>
          </tr>
          <tr>
            <td colspan="5" align="left">Musical instruments</td>
          </tr>
          <tr>
            <td align="left"> Instrument sound name (/20)</td>
            <td align="char">5</td>
            <td align="char">6</td>
            <td align="char">13.1 (2.8)</td>
            <td align="char">8–18</td>
          </tr>
          <tr>
            <td align="left"> Instrument sound recognition (/20)</td>
            <td align="char">6</td>
            <td align="char">7</td>
            <td align="char">13.7 (2.9)</td>
            <td align="char">9–18</td>
          </tr>
          <tr>
            <td align="left"> Instrument picture name (/20)</td>
            <td align="char">4</td>
            <td align="char">11</td>
            <td align="char">17.1 (1.7)</td>
            <td align="char">14–19</td>
          </tr>
          <tr>
            <td align="left"> Instrument picture recognition (/20)</td>
            <td align="char">10</td>
            <td align="char">13</td>
            <td align="char">17.3 (1.5)</td>
            <td align="char">15–19</td>
          </tr>
          <tr>
            <td align="left"> Cross-modal matching sound to picture/name (/20)</td>
            <td align="char">12</td>
            <td align="char">18</td>
            <td align="char">19.3 (0.8)</td>
            <td align="char">18–20</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tbl5fn1">
          <label>a</label>
          <p><italic>n</italic> = 14 controls.</p>
        </fn>
      </table-wrap-foot>
      <table-wrap-foot>
        <fn id="tbl5fn2">
          <label>b</label>
          <p><italic>n</italic> = 10 controls.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl5">
      <label>Table 5</label>
      <caption>
        <p>Summary of experimental neuropsychological profiles in QR and KL.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th align="left">Domain</th>
            <th align="left">Case QR</th>
            <th align="left">Case KL</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">Voices</td>
            <td align="left">Identification</td>
            <td align="left">↓</td>
            <td align="left">↓</td>
          </tr>
          <tr>
            <td/>
            <td align="left">Familiarity</td>
            <td align="left">↓</td>
            <td align="left">↓</td>
          </tr>
          <tr>
            <td/>
            <td align="left">Emotion recognition</td>
            <td align="left">N</td>
            <td align="left">N</td>
          </tr>
          <tr>
            <td/>
            <td align="left">Perception</td>
            <td align="left">N</td>
            <td align="left">N</td>
          </tr>
          <tr>
            <td colspan="4" align="left">  </td>
          </tr>
          <tr>
            <td align="left">Other sounds</td>
            <td align="left">Musical instrument matching</td>
            <td align="left">↓</td>
            <td align="left">N</td>
          </tr>
          <tr>
            <td/>
            <td align="left">Environmental sound recognition</td>
            <td align="left">N</td>
            <td align="left">N</td>
          </tr>
          <tr>
            <td colspan="4" align="left">  </td>
          </tr>
          <tr>
            <td align="left">Faces</td>
            <td align="left">Recognition</td>
            <td align="left">N<xref rid="tbl6fn1" ref-type="table-fn">a</xref></td>
            <td align="left">↓↓</td>
          </tr>
          <tr>
            <td/>
            <td align="left">Perception</td>
            <td align="left">N</td>
            <td align="left">N</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>N: normal performance, <bold>↓</bold>: impaired performance relative to controls, <bold>↓↓</bold>: impaired performance relative to both controls and other case.</p>
        </fn>
      </table-wrap-foot>
      <table-wrap-foot>
        <fn id="tbl6fn1">
          <label>a</label>
          <p>When matched to voices for difficulty.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </floats-wrap>
</article>