<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title>Neuroimage</journal-title>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2775905</article-id>
      <article-id pub-id-type="pmid">19632341</article-id>
      <article-id pub-id-type="publisher-id">YNIMG6436</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2009.07.032</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>On-line plasticity in spoken sentence comprehension: Adapting to time-compressed speech</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Adank</surname>
            <given-names>Patti</given-names>
          </name>
          <email>Patti.Adank@manchester.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Devlin</surname>
            <given-names>Joseph T.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>a</sup>School of Psychological Sciences, University of Manchester, Manchester, UK</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>b</sup>Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, Nijmegen, The Netherlands</addr-line>
      </aff>
      <aff id="aff3">
        <addr-line><sup>c</sup>Cognitive, Perceptual and Brain Sciences and Institute of Cognitive Neuroscience, University College London, London, UK</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. School of Psychological Sciences, Zochonis Building, University of Manchester, Brunswick Street, Manchester, M13 9PL, UK. <email>Patti.Adank@manchester.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <day>01</day>
        <month>1</month>
        <year>2010</year>
      </pub-date>
      <volume>49</volume>
      <issue>1</issue>
      <fpage>1124</fpage>
      <lpage>1132</lpage>
      <history>
        <date date-type="received">
          <day>18</day>
          <month>2</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>2</day>
          <month>7</month>
          <year>2009</year>
        </date>
        <date date-type="accepted">
          <day>15</day>
          <month>7</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2010 Elsevier Inc.</copyright-statement>
        <copyright-year>2009</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Abstract</title>
        <p>Listeners show remarkable flexibility in processing variation in speech signal. One striking example is the ease with which they adapt to novel speech distortions such as listening to someone with a foreign accent. Behavioural studies suggest that significant improvements in comprehension occur rapidly — often within 10–20 sentences. In the present experiment, we investigate the neural changes underlying on-line adaptation to distorted speech using time-compressed speech. Listeners performed a sentence verification task on normal-speed and time-compressed sentences while their neural responses were recorded using fMRI. The results showed that rapid learning of the time-compressed speech occurred during presentation of the first block of 16 sentences and was associated with increased activation in left and right auditory association cortices and in left ventral premotor cortex. These findings suggest that the ability to adapt to a distorted speech signal may, in part, rely on mapping novel acoustic patterns onto existing articulatory motor plans, consistent with the idea that speech perception involves integrating multi-modal information including auditory and motoric cues.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Learning</kwd>
        <kwd>Auditory systems</kwd>
        <kwd>Functional MRI</kwd>
        <kwd>Prefrontal cortex</kwd>
        <kwd>Temporal cortex</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <title>Introduction</title>
      <p>When meeting someone with a heavy foreign or regional accent, listeners may find themselves struggling to understand them at first, but comprehension becomes easier within a few minutes. After interacting even longer, one may not even be aware of the speaker's accent anymore. This situation illustrates a remarkable faculty of the speech comprehension system: the ability to quickly adapt to the acoustic consequences of a wide variation in sources when perceiving speech. Listeners have been found to adapt to foreign-accented speech (<xref rid="bib14" ref-type="bibr">Clarke and Garrett, 2004</xref>), noise-vocoded speech (<xref rid="bib70" ref-type="bibr">Shannon et al., 1995</xref>), spectrally shifted speech (<xref rid="bib62" ref-type="bibr">Rosen et al., 1999</xref>), synthetic speech (<xref rid="bib32" ref-type="bibr">Greenspan et al., 1988</xref>), and time-compressed speech (<xref rid="bib21" ref-type="bibr">Dupoux and Green, 1997</xref>) to name a few. This ability to adapt to distortions of the speech signal in general, has been studied extensively using time-compressed speech, which is a method for artificially shortening the duration of an audio signal without affecting the fundamental frequency of that signal (<xref rid="bib31 bib53 bib68 bib80" ref-type="bibr">Golomb et al., 2007; Pallier et al., 1998; Sebastián-Gallés et al., 2000; Wingfield et al., 2003</xref>). Listeners are quickly able to adapt to sentences compressed up to 35% of their original duration, within 10–20 sentences (<xref rid="bib21" ref-type="bibr">Dupoux and Green, 1997</xref>). Even though the distortion of the acoustic signal associated with time-compressing speech differs from, for instance, variations caused by speaking with a foreign accent, time-compressed speech has been used to study adaptation processes, as it is easy to create speech samples at a wide variety of compression rates. Furthermore, it allows for using the same speaker in time-compressed and uncompressed conditions, which is often not possible using foreign-accented speech.</p>
      <p>Behaviourally, perceptual adaptation to distorted speech has often been described as an attention-weighing process in which listeners shift their attention from task-irrelevant to task-relevant cues (<xref rid="bib30 bib31 bib51" ref-type="bibr">Goldstone, 1998; Golomb et al., 2007; Nosofsky, 1986</xref>). More specifically, it has been argued that learning of time-compressed speech is characterised by the recalibration of the boundaries between speech sounds to accommodate the faster speech rate (<xref rid="bib31" ref-type="bibr">Golomb et al., 2007</xref>). In other words, the adaptation is believed to occur primarily at an auditory level using increased attentional resources.</p>
      <p>Although previous studies have investigated the neural bases associated with comprehending time-compressed speech, none have investigated the adaptation process that occurs when listeners are first confronted with this unusual manipulation of the speech stream. For instance, <xref rid="bib55" ref-type="bibr">Peelle et al. (2004)</xref> reported that processing time-compressed speech strongly recruits bilateral auditory cortices, among other regions. Similarly, <xref rid="bib56" ref-type="bibr">Poldrack et al. (2001)</xref> found that activation in left superior temporal sulcus (STS) increased as sentence compression rate increased up to 30% of the original duration. Further compression, however, rendered the speech unintelligible and reduced activation in left STS. In contrast, right STS activation increased linearly, even at the highest levels of compression where speech was no longer intelligible. These results suggest two different processing mechanisms: a left hemisphere linguistic component responding to the content of the sentences and a right hemisphere acoustic component responding primarily to the complexity of the acoustic signal. Neither study, however, investigated the adaptation process — in fact, Peelle et al. familiarised listeners with the sound of time-compressed speech prior to taking part in the fMRI experiment specifically to avoid this confound. As a result, it is unclear which neural systems are responsible for this rapid perceptual adaptation.</p>
      <p>In the present study, we aimed to address this question by monitoring the on-line adaptation process while participants performed a speeded sentence verification task on time-compressed sentences. The goal of the present study, therefore, was to better understand the neural mechanisms underlying the adaptation process itself, instead of the specific neural activation pattern for processing time-compressed speech, as was the case in previous studies.</p>
      <p>In the field of speech comprehension research, there has been a longstanding debate about which mechanisms are required for speech processing. One theory holds that only auditory processes are required for effective speech perception (<xref rid="bib20 bib72" ref-type="bibr">Diehl and Kluender, 1989; Stevens and Klatt, 1974</xref>). A competing theory claims there is an additional role for the motor (i.e. speech production) system and is derived from Liberman's Motor Theory of Speech Perception (<xref rid="bib44 bib42" ref-type="bibr">Liberman et al., 1967; Liberman and Mattingly, 1985</xref>). In its original form, MTSP claimed: first, that speech tokens such as words, phonemes or phonetic features can only be recognized by mapping acoustic patterns onto articulatory (motor) plans and second, that speech processing involves a tight coupling between auditory and motor processes. The former claim is clearly incorrect (<xref rid="bib20 bib27 bib75" ref-type="bibr">Diehl and Kluender, 1989; Galantucci et al., 2006; Toni et al., 2008</xref>). Recent studies, however, have found support for a tight coupling between perception and production systems (<xref rid="bib43" ref-type="bibr">Liberman and Whalen, 2000</xref>) by showing the involvement of the speech motor system in speech perception tasks (<xref rid="bib16 bib18 bib24 bib52 bib58 bib77" ref-type="bibr">D'Ausilio et al., 2009; Davis and Johnsrude, 2003; Fadiga et al., 2002; Okada and Hickok, 2006; Pulvermüller et al., 2006; Watkins et al., 2003</xref>). <xref rid="bib47" ref-type="bibr">Meister et al. (2007)</xref> provided perhaps the clearest evidence by using transcranial magnetic stimulation to show that stimulation of left ventral premotor cortex (PMv) disrupted speech perception when syllables were embedded in noise without affecting a similar control task of detecting tones in noise. In short, there is renewed interest in the involvement of the motor (i.e. articulatory) system in speech perception, although not in the form of Liberman's original Motor Theory.</p>
      <p>Crucially, the two accounts make different predictions about the neural mechanisms involved in the adaptation process. The former predicts that it is done purely acoustically by recognizing distorted signals as instances of abstract auditory prototypes such as phonemes or phonological word forms. Consequently, adaptation-related activation changes would be expected solely in auditory regions associated with speech perception (<xref rid="bib33" ref-type="bibr">Guenther et al., 2004</xref>). The latter, however, predicts that the distorted acoustic signal is recognized at least in part by mapping it onto articulatory motor plans — a form of sensorimotor integration that implicitly simulates the motor patterns used to produce a comparable spoken sentence. In this case, adaptation-related activation would be expected in both auditory regions as well as in ventral premotor regions associated with speech production (<xref rid="bib10 bib77 bib78" ref-type="bibr">Blank et al., 2002; Watkins et al., 2003, 2002</xref>).</p>
    </sec>
    <sec sec-type="materials|methods">
      <title>Materials and methods</title>
      <sec>
        <title>Participants</title>
        <p>Twenty-two participants (13M, 9F) took part in the study although four (2M, 2F) were subsequently excluded due to: i) excessive head motion (&gt; 10 mm), ii) an unexpected brain abnormality, iii) chance level performance in the scanner, and iv) an error acquiring the scanning data. The 18 remaining participants were right-handed, native speakers of British English (mean 26.7 years, median 22.5 years, range 18–60 years) without any history of neurological or psychiatric disease. The behavioural and neuroimaging data from the older participant (the one 60 year-old) did not differ qualitatively from the younger participants and therefore was included in all analyses. None had any form of oral or written language impairment or any previous experience with time-compressed speech. None of the participants reported any hearing difficulties, but were not audiometrically screened. In-scanner preliminary testing revealed that all participants could hear the stimuli clearly enough to perform the task (see <xref rid="sec1" ref-type="sec">fMRI data acquisition</xref>, below). All gave written informed consent and were paid for their participation. The study was approved by the NHS Berkshire Research Ethics Committee.</p>
      </sec>
      <sec>
        <title>Task</title>
        <p>The task was a computerized version of the Speech and Capacity of Language Processing Test, or SCOLP (<xref rid="bib3 bib45" ref-type="bibr">Baddeley et al., 1992; May et al., 2001</xref>). Participants listened to a simple sentence and decided whether it was true or false, indicating their response with a button press. In all cases, the validity of the sentence was obvious (e.g., “Bedroom slippers are made in factories” vs. “Nuns are made in factories”) with invalid sentences generated by changing participants and predicates from true sentences (see <xref rid="bib1" ref-type="bibr">Adank et al. (2009</xref>) for additional task details). Accuracy and response times were recorded per trial and adaptation to time-compressed speech was operationalized as the increase in the speed of sentence verification times.</p>
      </sec>
      <sec>
        <title>Stimuli</title>
        <p>The auditory stimuli were recordings of 200 SCOLP sentences, 100 true and 100 false, by a male Southern Standard British English speaker. The recordings were made in an anechoic room directly onto digital auditory tape (DAT), while the digital output from the DAT recorder was fed to the digital input of the sound card in the PC. Next, all sentences were saved into separate files with the beginning and ends trimmed at zero crossings as closely as possible to the onset/offset of the initial/final speech sounds and re-sampled at 22050 Hz. The time-compressed sentences were obtained using PSOLA (<xref rid="bib49" ref-type="bibr">Moulines and Charpentier, 1990</xref>), as implemented in the Praat software package (<xref rid="bib11" ref-type="bibr">Boersma and Weenink, 2003</xref>). Two versions of each recorded sentence were created: sentences resynthesized at 100% of their original duration (normal-speed sentences) and resynthesized sentences shortened to 45% of their original duration (time-compressed sentences). The normal sentences were resynthesized to ensure that any differences between the two types of sentences were due solely to the time compression and not the resynthesis process. The sentences consisted of 6.5 syllables on average (range 3–12 syllables, range 477–1221 ms) and the average speech rate of the normal-speed sentences was 4.1 syllables per second, and the average speech rate of the time-compressed sentences was 9.2 syllables per second. Finally, each sentence was peak-normalized at 99% of its maximum amplitude and scaled to 70 dB SPL using Praat. Stimulus presentation and response time measurement were performed using Presentation (Neurobehavioral Systems, Albany, CA).</p>
      </sec>
      <sec>
        <title>Design and procedure</title>
        <p>The main experiment used an atypical block design in which all the normal-speed sentences (<italic>n</italic> = 64) occurred in the first half of the experiment and all the time-compressed sentences (<italic>n</italic> = 64) in the second half. This was necessary because pilot testing in the scanner demonstrated that alternating blocks of normal-speed and time-compressed sentences during scanning prevented behavioural adaptation — in fact, participants found both types of speech much more difficult. Consequently, the design shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref> was used to allow listeners to get used to the task and to the scanner noise during the presentation of the 64 normal-speed sentences and to allow them to efficiently tune into the time-compressed sentences in the second block. Such a design is similar to pharmacological fMRI studies where the time course of the pharmacological agent often makes it impossible to alternate between drug and non-drug conditions. Like those studies, we specifically looked for interactions between our experimental conditions and time to exclude non-specific effects of time such as scanner drift and physiological noise aliasing (<xref rid="bib81" ref-type="bibr">Wise and Tracey, 2006</xref>).</p>
        <p>A single trial began with a tone signal of 100 ms, followed by a pause of 100 ms, and then the auditory sentence (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). The inter-trial interval varied randomly between 4000–6000 ms providing a jittered sampling of the evoked haemodynamic response function (<xref rid="bib17 bib76" ref-type="bibr">Dale, 1999; Veltman et al., 2002</xref>). Although the stimuli were presented and analysed in an event-related design, trials occurred in short mini-blocks of four sentences followed by a silent baseline trial (duration randomly varied from 4000–6000 ms) to maximize statistical power. The entire duration of the run was 17 min.</p>
        <p>Afterwards, a second (behavioural) test was run outside the scanner to determine whether adaptation was stable after the scanning session or whether it continued in the quieter environment. Participants were tested individually in a quiet room using headphones (Philips SBC HN110) immediately following the fMRI experiment. 64 new time-compressed sentences were presented. Presentation of all three sets of 64 sentences (normal-speed, time-compressed, and the time-compressed sentences in the post-task) was counter-balanced across subjects. Each set consisted of 32 true and 32 false sentences. The sentences were presented in a semi-randomised order per participant and true and false sentences were counter-balanced across experimental blocks.</p>
      </sec>
      <sec id="sec1">
        <title>fMRI data acquisition</title>
        <p>Scanning was performed at the Birkbeck-UCL Neuroimaging (BUCNI) Centre on a 1.5 T MR scanner (Siemens Avanto, Siemens Medical Systems, Erlangen, Germany). The experiment began by acquiring a high-resolution structural scan (3D Turbo-FLASH, TR = 12 s, TE = 5.6 ms, 1 × 1 × 1 mm resolution) used for anatomical localisation. Next, participants were familiarised with the task during a brief practice run using six normal-speed sentences not included in the rest of the experiment. They were instructed to respond through a button press with their right index finger when the sentence was true and with their right middle finger when the sentence was false. The sentences were presented over electro-static headphones (MRConFon, Magdeburg, Germany) during continuous scanner acquisition (GE-EPI, TR = 3 s, TE = 50 ms, 192 × 192 FOV, 64 × 64 matrix, 35 axial slices, yielding a notional 3 × 3 × 3 mm resolution) — in other words, over the noise of the scanner. The main experiment lasted just under 17 min and on average, 332 volumes (range: 330–336) were collected per participant. The presentation of the four blocks of normal sentences lasted on average 172 volumes (43 per block) for the normal-speed sentences, and 151 volumes (38 per block) for the time-compressed sentences. In other words, slightly less data were collected for the time-compressed sentence conditions because, by definition, the duration of the sentences was shorter. In theory, this may slightly reduce BOLD signal sensitivity for time-compressed relative to normal-speed sentences, but this was unavoidable given the nature of the experiment.</p>
        <p>The choice of continuous, rather than sparse, sampling was based on a trade-off between the ability to reliably detect adaptation-related changes in blood oxygen level dependent (BOLD) signal and the length of the experiment. Continuous sampling results in both acoustic masking of the auditory sentences (<xref rid="bib69" ref-type="bibr">Shah et al., 1999</xref>) and contamination of the BOLD signal response in auditory regions (<xref rid="bib4 bib34 bib73" ref-type="bibr">Bandettini et al., 1998; Hall et al., 1999; Talavage et al., 1999</xref>). The former, however, was not a problem as a relatively quiet acquisition sequence (∼ 80 dB SPL) coupled with sound attenuating headphones (∼ 30 dB attenuation) ensured that the sentences were easily heard. Indeed, all participants confirmed their ability to hear and understand the sentences during this practice session. Contamination of the BOLD signal was potentially more problematic because scanner noise elevates BOLD responses in auditory areas (<xref rid="bib26 bib34" ref-type="bibr">Gaab et al., 2006; Hall et al., 1999</xref>), and these effects need not be identical across regions (<xref rid="bib74 bib87" ref-type="bibr">Tamer et al., in press; Zaehle et al., 2007</xref>). In the current experiment, however, we were specifically interested in reductions in BOLD signal that index adaptation-related changes. As a result, elevated BOLD responses <italic>per se</italic> were not problematic; only responses driven to saturation levels by the scanner noise would reduce sensitivity and previous studies have clearly shown that typical EPI sequences reduce, but do not eliminate, the dynamic range of the BOLD response (<xref rid="bib26 bib87" ref-type="bibr">Gaab et al., 2006; Zaehle et al., 2007</xref>). Moreover, although some “silent” imaging protocols exist (<xref rid="bib35 bib65" ref-type="bibr">Hall et al., 2009; Schwarzbauer et al., 2006</xref>) they are not yet widely available. fMRI systems without these protocols (such as our own) require silent periods between volume acquisitions lasting between 16 and 32 s to avoid scanner-noise contamination and ensure an adequate sampling of the evoked haemodynamic response function (HRF) (<xref rid="bib22 bib23 bib34 bib36 bib74" ref-type="bibr">Eden et al., 1999; Edmister et al., 1999; Hall et al., 1999; Hickok et al., 1997; Tamer et al., in press</xref>). A sparse design would therefore result in our experiment lasting between 54 and 90 min, which was deemed likely to seriously reduce participant's performance due to fatigue. As a result, we chose to use a continuous sampling paradigm instead. One consequence of this choice was that the adaptation task was performed over the background noise of the scanner, and it became an empirical question whether embedding its noise would alter the typical behavioural profile of rapid adaptation.</p>
      </sec>
      <sec>
        <title>Analyses</title>
        <p>Response times (RT) were measured from the end of each audio file, following <xref rid="bib45" ref-type="bibr">May et al. (2001)</xref>, and RTs beyond 3000 ms were trimmed without replacement (0.4%). Each set of sentences was divided into four blocks of 16 so that the time course of adaptation could be examined. A 16 sentence block-size was used because pilot testing revealed that learning was typically stable after 14–18 sentences and smaller windows reduce the accuracy of estimating induced BOLD signal responses (<xref rid="bib50" ref-type="bibr">Murphy and Garavan, 2005</xref>). The mean RT of correct responses was used in the group analyses. Both accuracy and RTs were evaluated with a repeated-measures 2 × 4 ANOVA with Speech Type (normal-speed, time-compressed) and Block (1–4) as independent factors. Obviously, adaptation to the time-compressed sentences was only possible in the second half of the experiment. Consequently, our <italic>a priori</italic> hypothesis was that we would observe adaptation effects only for time-compressed and not normal-speed sentences.</p>
        <p>The functional imaging data were analysed using FSL (<ext-link xlink:href="http://www.fmrib.ox.ac.uk/fsl" ext-link-type="uri">www.fmrib.ox.ac.uk/fsl</ext-link>). After deleting the first three volumes of each run to allow for T1 equilibrium, the functional images were realigned to correct for small head movements (<xref rid="bib39" ref-type="bibr">Jenkinson et al., 2002</xref>). The images were then smoothed with a 6 mm FWHM Gaussian filter and pre-whitened to remove temporal auto-correlation (<xref rid="bib84" ref-type="bibr">Woolrich et al., 2001</xref>). The resulting images were entered into a subject-specific general linear model with eight conditions of interest: four blocks of normal-speed and four blocks of time-compressed sentences. Each sentence was convolved with a double gamma “canonical HRF” (<xref rid="bib29" ref-type="bibr">Glover, 1999</xref>) to generate the regressors. The onset of this HRF function was aligned with the onset of every sound file and the duration of every sentence was included in the model. Temporal derivatives were also included to better fit small deviations in the expect time course. Both the data and the model were high-pass filtered at 1/200 s to remove low frequency signal drifts such as aliased cardiac or respiratory signals without affecting the more rapid, experimentally-induced frequencies such as those between mini-blocks and blocks of stimuli. Finally, each anatomical T1 scan was registered to the MNI-152 template using an affine transformation (<xref rid="bib40" ref-type="bibr">Jenkinson and Smith, 2001</xref>), which was then applied to the first-level parameter and variance estimates. These were fed into a second-level mixed-effects analysis for inferring across the population (<xref rid="bib6 bib85" ref-type="bibr">Beckmann et al., 2003; Woolrich et al., 2004</xref>).</p>
        <p>Linear weighted contrasts were used to identify three effects of interest. First, the main effect of processing auditory sentences relative to scanner noise (when no sentence was presented) was computed to identify task-relevant brain regions using a contrast of [+ 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1] where the first four conditions are the four blocks of normal-speed sentences and the last four of the blocks of time-compressed sentences. Significant activations were assessed with a cluster-based correction for multiple comparisons using a height threshold of <italic>Z</italic> &gt; 3.5 and a corrected <italic>p</italic> &lt; 0.05 cluster-extent (<xref rid="bib25" ref-type="bibr">Friston et al., 1994</xref>). This corresponded to a minimum of 123 contiguous 2 × 2 × 2 mm voxels with <italic>Z</italic>-scores of 3.5 or greater. Next, we identified the regions within this system that were significantly more active for time-compressed relative to normal speech using the same statistical criteria and a contrast of [− 1/4 − 1/4 − 1/4 − 1/4 + 1/4 + 1/4 + 1/4 + 1/4]. Finally, the critical analysis aimed to identify areas within this system involved in adapting to time-compressed speech. To do this, we used a contrast that followed the behavioural profile of adaptation, namely greater activation for the first block of time-compressed sentences than the remaining three blocks [0 0 0 0 + 1 − <sup>1</sup>/<sub>3</sub> − <sup>1</sup>/<sub>3</sub> − <sup>1</sup>/<sub>3</sub>]. It is worth noting, however, that this contrast may be confounded with linear time effects such as scanner drift or physiological noise that are not completely removed by high-pass filtering. Consequently, the contrast was inclusively masked (at <italic>Z</italic> &gt; 3.5) by two other contrasts to ensure the effects were due to adaptation rather than scanner drift. The first was the main effect of sentence processing, to limit the results to areas specifically involved in the task [+ 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1], while the second was the contrast between the first block of time-compressed sentences and the last block of normal-speed sentences [0 0 0 − 1 + 1 0 0 0]. Here we assumed that time-compressed sentences would significantly increase processing demands relative to normal sentences, consistent with the reaction time cost of switching to time-compressed sentences (see below). Importantly, this difference goes in the opposite direction to the time confound in the main contrast and helps to exclude activation unrelated to adapting to time-compressed speech. Significance was assessed using a small volume correction (<xref rid="bib86" ref-type="bibr">Worsley et al., 1996</xref>) based on the smoothness and volume of the intersection of the inclusive masks. Within this reduced volume, a voxel-wise height threshold of <italic>Z</italic> &gt; 3.0 corresponded to <italic>p</italic> &lt; 0.05, one tailed. To illustrate the regional activation profiles, the mean parameter estimates per condition per participant were extracted from the activation cluster. Note that post-hoc t-tests are Bonferroni corrected to adjust for multiple comparisons except where correcting was less conservative (as stated in the text) and significance was assessed at <italic>p</italic> &lt; 0.05.</p>
      </sec>
    </sec>
    <sec>
      <title>Results</title>
      <sec>
        <title>Behaviour</title>
        <p><xref rid="fig2" ref-type="fig">Fig. 2</xref> shows the results for the error rates and the response times recorded inside and outside the scanner. In order to illustrate the time course of adaptation, data are shown averaged over each mini-block of four trials by a filled-in circle and error bars indicating the standard error of the mean. Data from four consecutive mini-blocks was then averaged and displayed as a bar plot. These bars corresponded to the blocks of 16 sentences used in the analyses. The data from the mini-blocks were included to provide a more fine-grained temporal resolution of the adaptation process than provided by the larger blocks of 16 sentences and are shown for illustration purposes only; all statistical analyses were performed on the averages across the larger blocks of 16 sentences only. What should be clear from the figure is that despite the noisy environment of the scanner, participants were able to perform the task well. Error rates for normal-speed sentences were only 3% while time-compressed sentences were more difficult, with an average error rate of 16%. This main effect of Speech Type was significant (<italic>F</italic>(1,17) = 196.8, <italic>p</italic> &lt; 0.0001, partial <italic>η</italic><sup>2</sup> = 0.92), but the main effect of Block (<italic>F</italic>(3,51) = 1.7, <italic>p</italic> = 0.172, partial <italic>η</italic><sup>2</sup> = 0.09) and the interaction (<italic>F</italic>(3,51) = 1.1, <italic>p</italic> = 0.378, partial <italic>η</italic><sup>2</sup> = 0.06) were not. In order to determine whether there were any adaptation effects that may have been hidden by the omnibus ANOVA, the normal-speed and time-compressed sentences were re-analysed separately. Normal-speed sentences showed an effect of Block (<italic>F</italic>(3,51) = 5.6, <italic>p</italic> = 0.03, partial <italic>η</italic><sup>2</sup> = 0.25), indicating that participants got better at the task over time. Interestingly, there was no equivalent effect for time-compressed sentences (<italic>F</italic>(3,51) = 1.1, <italic>p</italic> = 0.306, partial <italic>η</italic><sup>2</sup> = 0.06), suggesting that by the time these sentences began, the participants were essentially acclimated to the task and environment. This is consistent with the fact that errors in the post-scan behavioural test did not significantly differ across blocks (<italic>F</italic>(3,51) = 2.1, <italic>p</italic> = 0.115, partial <italic>η</italic><sup>2</sup> = 0.11). In other words, the error rates suggest that participants successfully acclimated to the task within the noisy environment of the scanner before the time-compressed sentences were introduced.</p>
        <p>The analysis of RTs revealed a main effect of Speech Type (<italic>F</italic>(1,17) = 80.8, <italic>p</italic> &lt; 0.0001, partial <italic>η</italic><sup>2</sup> = 0.83), indicating that responses to time-compressed sentences were significantly slower than normal-speed sentences (790 vs. 386 ms). There was also a main effect of Block (<italic>F</italic>(3,51) = 5.0, <italic>p</italic> = 0.004, partial <italic>η</italic><sup>2</sup> = 0.23) that was largely driven by the slowed responses to the first block of time-compressed sentences. When the two types of speech types were analysed separately, normal-speed sentences showed no effect of Block (<italic>F</italic>(3,51) = 1.58, <italic>p</italic> = 0.226, partial <italic>η</italic><sup>2</sup> = 0.09), indicating that listeners did not vary significantly in their response times across the four blocks. On the other hand, the analysis of the time-compressed sentences did reveal a significant effect of Block (<italic>F</italic>(3,17) = 5.0, <italic>p</italic> = 0.004, partial <italic>η</italic><sup>2</sup> = 0.23). A series of planned <italic>t</italic>-tests showed that responses to the first block of time-compressed sentences were significantly longer than those to subsequent blocks (all paired <italic>t</italic>-tests, <italic>p</italic> &lt; 0.05). In other words, within the first 16 trials, participants had adapted to the atypical speech signal as evidenced by the fact that RTs in the subsequent blocks were on average 150 ms faster than the first block of time-compressed sentences. This was further confirmed when the RTs in the post-scan behavioural test did not show a significant effect of Block (<italic>F</italic>(3,17) = 2.34, <italic>p</italic> = 0.085, partial <italic>η</italic><sup>2</sup> = 0.13), confirming that adaptation asymptoted during the 64 trials that occurred within the scanner.</p>
      </sec>
      <sec>
        <title>Neuroimaging</title>
        <p>To begin, we compared blood oxygen level dependent (BOLD) signal across the eight auditory sentence task conditions to fixation in order to identify the system of regions involved in the task. Like previous studies (<xref rid="bib9 bib15 bib28 bib57 bib67" ref-type="bibr">Binder et al., 2000; Crinion et al., 2003; Giraud et al., 2004; Price et al., 2005; Scott et al., 2000</xref>), we found robust activation in primary auditory and auditory association cortices bilaterally, as well as in the deep frontal operculum bilaterally, left prefrontal and premotor cortices, and pre-SMA extending ventrally into the cingulate sulcus (see <xref rid="tbl1" ref-type="table">Table 1</xref> for the complete list). The activation in the operculum bilaterally corresponds with earlier findings reported on processing degraded/noisy speech input (<xref rid="bib7 bib83 bib18 bib28 bib48" ref-type="bibr">Benson et al., 2001; Wong et al., 2002; Davis and Johnsrude, 2003; Giraud et al., 2004; Meyer et al., 2004</xref>).</p>
        <p>Next, we identified the regions within this system that were more active for time-compressed relative to normal speech (<xref rid="tbl2" ref-type="table">Table 2</xref>A and <xref rid="fig3" ref-type="fig">Fig. 3</xref>). This revealed separate posterior and anterior temporal lobe foci bilaterally. One was located in the superior temporal sulcus (STS) posterior to Heschl's gyrus while the other was located in the lateral portion of Heschl's gyrus as it joins the anterior superior temporal gyrus (STG). In other words, time-compressed sentences produced greater levels of activation in STS and STG compared to normal-speed sentences, consistent with previous reports that activation in these regions increased with the level of speech compression (<xref rid="bib55 bib56" ref-type="bibr">Peelle et al., 2004; Poldrack et al., 2001</xref>). In addition, we observed activation in pre-SMA (0, + 12, + 60, <italic>Z</italic> = 4.0).</p>
        <p>Of primary interest, however, were the neural changes associated with adapting to time-compressed speech (<xref rid="tbl2" ref-type="table">Table 2</xref>B). Four regions showed adaptation-related activation profiles, illustrated in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. In the left hemisphere, one cluster was located within a region of the ventral bank of posterior STS. Within this area, there was a significant main effect of Stimulus Type (<italic>F</italic>(1,17) = 15.5, <italic>p</italic> = 0.001, partial <italic>η</italic><sup>2</sup> = 0.48) indicating greater activation for compressed relative to normal speech, a main effect of Block (<italic>F</italic>(3,51) = 5.0, <italic>p</italic> = 0.004, partial <italic>η</italic><sup>2</sup> = 0.23) and a significant interaction (<italic>F</italic>(3,51) = 4.3, <italic>p</italic> = 0.008, partial <italic>η</italic><sup>2</sup> = 0.20). When the two types of speech were analysed separately, normal-speed sentences showed no effect of Block (<italic>F</italic>(3,51) = 0.68, <italic>p</italic> = 0.568, partial <italic>η</italic><sup>2</sup> = 0.04), indicating that activation was greater than baseline and stable over all four blocks, mirroring the response time findings. The time-compressed sentences, however, did show an effect of Block (<italic>F</italic>(3,51) = 5.1, <italic>p</italic> = 0.004, partial <italic>η</italic><sup>2</sup> = 0.23) which was driven by a significant increase in the magnitude of the activation during the first block of time-compressed sentences (<italic>t</italic>(17) = 4.6, <italic>p</italic> &lt; 0.001). Activation levels in the first block of time-compressed sentences more than tripled, signifying a considerable increase in processing demands. By the third block of time-compressed sentences, however, activation had returned to normal sentence processing levels (<italic>t</italic>(17) = 1.0, <italic>p</italic> = 0.319 uncorrected). A second left hemisphere cluster located on the crest of the pre-central gyrus, a region of ventral premotor cortex (PMv), showed essentially the same pattern. Again there were significant main effects of Stimulus Type (<italic>F</italic>(1,17) = 7.4, <italic>p</italic> = 0.014, partial <italic>η</italic><sup>2</sup> = 0.31) and Block (<italic>F</italic>(3,51) = 3.9, <italic>p</italic> = 0.014, partial <italic>η</italic><sup>2</sup> = 0.19), as well as a significant interaction (<italic>F</italic>(3,51) = 3.7, <italic>p</italic> = 0.016, partial <italic>η</italic><sup>2</sup> = 0.18). Normal sentences showed no effect of Block (<italic>F</italic>(3,51) = 0.38, <italic>p</italic> = 0.762, partial <italic>η</italic><sup>2</sup> = 0.02), whereas time-compressed sentences did (<italic>F</italic>(3,51) = 4.5, <italic>p</italic> = 0.007, partial <italic>η</italic><sup>2</sup> = 0.21). As in the pSTS region, activation levels increased significantly for the first block of time-compressed sentences (<italic>t</italic>(17) = 4.0, <italic>p</italic> = 0.004) and then returned to normal levels by the third block (<italic>t</italic>(17) = 0.4, <italic>p</italic> = 0.679 uncorrected). In short, the activation profile in both left pSTS and left PMv closely matched the response time data.</p>
        <p>Adaptation-related changes in the two right hemisphere clusters, on the other hand, showed a slightly different pattern. The first region was located on the anterior crest of STG and extended into the dorsal bank of STS while the other was located more posterior in the ventral bank of STS. In both regions, activation was significantly greater than baseline for normal sentences but was not stable over the four blocks of normal sentences — instead it monotonically decreased. This was confirmed by a significant main effect of Block (both <italic>F</italic>(3,51) ≥ 35.0, <italic>p</italic> &lt; 0.0001, partial <italic>η</italic><sup>2</sup> ≥ 0.67) which was also present when normal sentences were analysed separately (<italic>F</italic>(3,51) ≥ 2.8, <italic>p</italic> ≤ 0.050, partial <italic>η</italic><sup>2</sup> ≥ 0.14). In both regions, the first block of time-compressed sentences significantly increased activation levels (anterior: <italic>t</italic>(17) = 7.0, <italic>p</italic> &lt; 0.001; posterior: <italic>t</italic>(17) = 5.7, <italic>p</italic> &lt; 0.001), but only in the more posterior cluster did these return to normal levels (for blocks 3 and 4: <italic>t</italic>(17) = 1.8, 1.0, <italic>p</italic> = 0.092 and 0.352 uncorrected). In the more anterior region, activation remained significantly greater for all blocks of time-compressed relative to normal sentences (for blocks 2–4: <italic>t</italic>(17) = 4.6, 3.5, 3.8, all <italic>p</italic> ≤ 0.012 corrected). In sum, the activation in the left and right-lateralised regions increased strongly for the time-compressed sentences before showing a sharp decline after the first blocks of time-compressed speech. However, the left and right regions differed in their activation pattern for the normal sentences with activation in the left-lateralised regions remaining constant, while activation in the right-lateralised regions declined. This was confirmed by a significant Hemisphere × Block interaction (<italic>F</italic>(3,54) = 2.9, <italic>p</italic> = 0.045) for normal-speed sentences that used mean BOLD signal from the two left and two right hemisphere areas showing adaptation effects. It is worth noting that the theoretical question of reduced statistical sensitivity for time-compressed sentences appears not to be a major concern in practice, given the large effect sizes and small error variances seen in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. In other words, the sensitivity was sufficient to detect significant BOLD signal effects for both time-compressed sentences as well as the effects of adaptation.</p>
        <p>A final set of analyses investigated whether any of the results were related to the presence of semantic violations (by virtue of including true and false sentences together). This analysis modelled true and false sentences separately to avoid the potential confound associated with semantic violations. Although the effect sizes were smaller due to the lower number of cases, the results showed a pattern identical to the analysis with the true and false sentences combined. In other words, the activations associated with adapting to time-compressed speech cannot be attributed to semantic violations present in the false sentences.</p>
      </sec>
    </sec>
    <sec>
      <title>Discussion</title>
      <p>The current results confirm and extend previous behavioural studies that demonstrate rapid on-line adaptation to atypical speech signals. After hearing just 16 sentences, participants' comprehension was both accurate and much faster than their initial responses to time-compressed speech, despite the concurrent, on-going noise of the MRI scanner. Moreover, the final behavioural test outside of the scanner demonstrated that the adaptation process completed within the first 64 trials as no additional learning took place after scanning. Instead, there was a trend towards slightly faster RTs which did not reach significance but may point to a re-tuning process that occurs after adaptation had been completed (<xref rid="bib21" ref-type="bibr">Dupoux and Green, 1997</xref>), perhaps due to different acoustic environments (i.e. with or without scanner noise).</p>
      <p>Adaptation-related changes in neural activation were observed in four separate areas: two in the right hemisphere and two in the left. In the right, the regions were both auditory association areas located in anterior and posterior portions of the STS, respectively. Both showed significant adaptation to normal sentences, with activation decreasing over the first four blocks. When time-compressed sentences were introduced, activation increased dramatically at first and then decreased after the first block, although it did not return to normal levels. This pattern of responses suggests that adaptation may have occurred at an acoustic, rather than linguistic, level for three reasons. First, the initial BOLD signal adaptation coupled with increasing accuracy rates for normal sentences appears to reflect a gradual acclimation to hearing sentences in a noisy environment. This may reflect adaptation occurring at an acoustic, rather than linguistic, level due to the energetic masking of the scanner noise (<xref rid="bib2" ref-type="bibr">Aydelott and Bates, 2004</xref>). Second, the fact that the BOLD response did not return to levels associated with normal-speed sentences suggests that activation in these areas may be driven primarily by the condensed acoustic signal rather than by its content. This interpretation is consistent with the findings of <xref rid="bib56" ref-type="bibr">Poldrack et al. (2001)</xref>, who demonstrated a linear increase in activation within right STS with increasing time compression, even when the compression level rendered the auditory sentence unintelligible. Finally, a recent study reported stronger interactions between scanner noise and acoustic processing in right hemisphere auditory areas (<xref rid="bib64" ref-type="bibr">Schmidt et al., 2008</xref>), suggesting greater sensitivity to acoustic over linguistic processing in right auditory cortex.</p>
      <p>In contrast, adaptation-related changes in the left hemisphere may be more directly related to comprehending speech. In both pSTS and PMv, activation was stable over the first four blocks of normal sentences before increasing by 2–3 fold for the first block of time-compressed sentences. Activation then decreased to the levels seen for normal sentences. This pattern more closely matched the behaviour and subjective experience of the participants who reported no difficulty with the normal-speed sentences and no difficulty for the time-compressed sentences “once they got used to them.” Again, this result is consistent with those from <xref rid="bib56" ref-type="bibr">Poldrack et al. (2001)</xref> who reported a convex response profile for activation in left STS. As time compression increased from 60% to 30% of the original sentence duration, BOLD signal increased. At 15% of a sentence's original duration it was no longer comprehensible and left STS activation reduced to baseline levels. Together with the current results, both studies suggest that adaptation in the left hemisphere regions appears to be at a linguistic level. Moreover, both studies highlight an apparent difference between right and left STS in responding to time-compressed speech: right STS appears to be driven more by the complexity of the acoustic signal while left STS responds more strongly to its linguistic content. This difference is consistent with the theoretical framework of <xref rid="bib88" ref-type="bibr">Hickok and Poeppel (2007)</xref> in which two distinct anatomical streams are involved in processing speech signals. A ventral stream runs along the STS and is primarily concerned with the content of the speech signal while the dorsal stream links posterior auditory cortex to anterior motor regions involved in articulation. Critically, the latter is strongly left lateralised, includes both pSTS and PMv, and provides an anatomical substrate for mapping acoustic speech signals onto frontal lobe articulatory networks. With respect to sensorimotor interactions, it is proposed that sensorimotor integration is subserved by the dorsal stream.</p>
      <p>The anatomy of regions involved in adaptation to time-compressed sentences helps to shed light on the nature of the adaptation mechanism. Specifically, pSTS is a region of auditory association cortex involved in speech perception (<xref rid="bib8 bib67 bib66 bib82" ref-type="bibr">Binder et al., 1996; Scott et al., 2000; Scott and Wise, 2004; Wise et al., 2001</xref>), as well as perceiving other complex, non-linguistic sounds (<xref rid="bib19 bib41 bib57" ref-type="bibr">Dick et al., 2007; Leech et al., 2009; Price et al., 2005</xref>). In contrast, the pre-central gyrus is part of the premotor cortex, which is involved in the selection and execution of complex motor sequences (<xref rid="bib37 bib63" ref-type="bibr">Hoshi and Tanji, 2007; Rushworth et al., 2003</xref>). PMv, in particular, is closely linked with articulatory motor patterns due to its strong, reciprocal connectivity to the ventral areas of primary motor cortex, which enervate the face, larynx, and tongue (<xref rid="bib5 bib12" ref-type="bibr">Barbas and Pandya, 1987; Brown et al., 2007</xref>). Speech production tasks robustly activate this region (<xref rid="bib10 bib58 bib59 bib60 bib77" ref-type="bibr">Blank et al., 2002; Pulvermüller et al., 2006; Rauschecker et al., 2008, in press; Watkins et al., 2003</xref>). The fact that both sensory and motor areas demonstrate adaptation-related activation profiles, suggests that adapting to atypical speech involves changing sensitivity not only to auditory, but also to motoric cues. One possibility is that the novel acoustic patterns of compressed speech are mapped onto articulatory motor plans as an implicit form of motor simulation. This may aid in recognizing the speech tokens, particularly in challenging listening situations. Indeed, situations such as when the speech signal is either impoverished (<xref rid="bib58 bib79" ref-type="bibr">Pulvermüller et al., 2006; Wilson et al., 2004</xref>), masked (<xref rid="bib18 bib47" ref-type="bibr">Davis and Johnsrude, 2003; Meister et al., 2007</xref>) or ambiguous (<xref rid="bib13 bib71" ref-type="bibr">Callan et al., 2004; Skipper et al., 2006</xref>) may preferentially recruit speech production regions to aid speech comprehension.</p>
      <p>Adapting to time-compressed speech has often been classified as an attention-weighing process in which listeners learn to shift their attention from task-irrelevant to task-relevant auditory cues (<xref rid="bib30 bib31 bib51" ref-type="bibr">Goldstone, 1998; Golomb et al., 2007; Nosofsky, 1986</xref>). Our results indicate that this specific form of perceptual learning may be supported by sensorimotor integration between auditory and speech production areas. For instance, the process of adjusting to distorted acoustic cues may place greater demands on verbal working memory, which engages this left sensorimotor circuit (<xref rid="bib54 bib61" ref-type="bibr">Paulesu et al., 1993; Romero et al., 2006</xref>). This account is certainly consistent with the current findings showing increased PMv activation to the initial time-compressed sentences and raises the possibility that adapting to the compressed speech signal depends at least in partly on implicit articulatory simulation to recognize speech tokens in the atypical auditory input.</p>
      <p>Finally, it is worth noting that the current findings are consistent with some, but not all, aspects of Liberman's MTSP (<xref rid="bib44 bib42" ref-type="bibr">Liberman et al., 1967; Liberman and Mattingly, 1985</xref>). Like previous studies, our data demonstrate a clear role of auditory cortex in speech comprehension. According to the original MTSP, this auditory component would be a highly specialised speech-specific module separate from the rest of the auditory system and dedicated to conveying speech to the motor system where it would be identified as a series of articulatory gestures (<xref rid="bib42" ref-type="bibr">Liberman and Mattingly 1985</xref>). In contrast, our data identifies a particular region of posterior STS that has also been shown to be involved in processing complex non-speech sounds (e.g., <xref rid="bib41" ref-type="bibr">Leech et al., 2009</xref>) and thus runs counter to a core theoretical claim of MTSP. Our finding of significant activation in PMv when adapting to time-compressed speech, on the other hand, tends to support the notion that speech production regions may be preferentially recruited to aid speech comprehension in challenging listening situations such as a distorted, degraded or masked speech signal (<xref rid="bib13 bib71" ref-type="bibr">Callan et al., 2004; Skipper et al., 2006</xref>). For example, <xref rid="bib47" ref-type="bibr">Meister et al. (2007)</xref> showed that transcranial magnetic stimulation (TMS) to left PMv disrupted speech perception when syllables were embedded in noise without affecting a similar control task of detecting tones in noise. Additional studies are, however, required to establish whether these regions are <italic>essential</italic> for the adaptation process.</p>
      <p>In summary, although ideal listening conditions facilitate speech perception, they rarely occur in real life. Normal environments are noisy with poor acoustics that degrade an incoming speech signal. As a result, the human speech recognition system seems to have evolved an opportunistic decoding approach that takes advantage of whatever information is available to assist in comprehension. Primarily this relies on auditory information, but other systems including vision (<xref rid="bib46" ref-type="bibr">McGurk and MacDonald, 1976</xref>), somatosensation (<xref rid="bib38" ref-type="bibr">Ito et al., 2009</xref>) and the motor system (D'<xref rid="bib16" ref-type="bibr">Ausilio et al., 2009</xref>) may provide important additional cues as well.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>This research was supported by the Netherlands Organization for Research (NWO) under project number 275-75-003 and the Wellcome Trust.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Adank</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>B.G.</given-names>
            </name>
            <name>
              <surname>Stuart-Smith</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
          </person-group>
          <article-title>Familiarity with a regional accent facilitates comprehension of that accent in noise</article-title>
          <source>J. Exp. Psychol.: Hum. Percept. Perform.</source>
          <year>2009</year>
          <volume>35</volume>
          <fpage>520</fpage>
          <lpage>529</lpage>
          <pub-id pub-id-type="pmid">19331505</pub-id>
        </citation>
      </ref>
      <ref id="bib2">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aydelott</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bates</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Effects of acoustic distortion and semantic context on lexical access</article-title>
          <source>Lang. Cogn. Proc.</source>
          <year>2004</year>
          <volume>19</volume>
          <fpage>29</fpage>
          <lpage>56</lpage>
        </citation>
      </ref>
      <ref id="bib3">
        <citation citation-type="other">Baddeley, A.D., Emslie, H., Nimmo-Smith, I., 1992. The Speed and Capacity of Language Processing (SCOLP) Test.</citation>
      </ref>
      <ref id="bib4">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bandettini</surname>
              <given-names>P.A.</given-names>
            </name>
            <name>
              <surname>Jesmanowicz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>van Kylen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Birn</surname>
              <given-names>R.M.</given-names>
            </name>
            <name>
              <surname>Hyde</surname>
              <given-names>J.S.</given-names>
            </name>
          </person-group>
          <article-title>Functional MRI of brain activation induced by scanner acoustic noise</article-title>
          <source>Magn. Reson. Med.</source>
          <year>1998</year>
          <volume>39</volume>
          <fpage>410</fpage>
          <lpage>416</lpage>
          <pub-id pub-id-type="pmid">9498597</pub-id>
        </citation>
      </ref>
      <ref id="bib5">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Barbas</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Pandya</surname>
              <given-names>D.N.</given-names>
            </name>
          </person-group>
          <article-title>Architecture and frontal cortical connections of the premotor cortex (area 6) in the rhesus monkey</article-title>
          <source>J. Comp. Neurol.</source>
          <year>1987</year>
          <volume>256</volume>
          <fpage>211</fpage>
          <lpage>228</lpage>
          <pub-id pub-id-type="pmid">3558879</pub-id>
        </citation>
      </ref>
      <ref id="bib6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Beckmann</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>General multilevel linear modeling for group analysis in FMRI</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>20</volume>
          <fpage>1052</fpage>
          <lpage>1063</lpage>
          <pub-id pub-id-type="pmid">14568475</pub-id>
        </citation>
      </ref>
      <ref id="bib7">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Benson</surname>
              <given-names>R.R.</given-names>
            </name>
            <name>
              <surname>Whalen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Richardson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Swainson</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names>V.P.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Liberman</surname>
              <given-names>A.M.</given-names>
            </name>
          </person-group>
          <article-title>Parametrically dissociating speech and nonspeech in the brain using fMRI</article-title>
          <source>Brain Lang.</source>
          <year>2001</year>
          <volume>78</volume>
          <fpage>364</fpage>
          <lpage>396</lpage>
          <pub-id pub-id-type="pmid">11703063</pub-id>
        </citation>
      </ref>
      <ref id="bib8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Binder</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frost</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hammeke</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Rao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Cox</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Function of the left planum temporale in auditory and linguistic processing</article-title>
          <source>Brain</source>
          <year>1996</year>
          <volume>119</volume>
          <fpage>1229</fpage>
          <lpage>1247</lpage>
        </citation>
      </ref>
      <ref id="bib9">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Binder</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Frost</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Hammeke</surname>
              <given-names>T.A.</given-names>
            </name>
            <name>
              <surname>Bellgowan</surname>
              <given-names>P.S.</given-names>
            </name>
            <name>
              <surname>Springer</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Kaufman</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Possing</surname>
              <given-names>E.T.</given-names>
            </name>
          </person-group>
          <article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title>
          <source>Cereb. Cortex</source>
          <year>2000</year>
          <volume>10</volume>
          <fpage>512</fpage>
          <lpage>528</lpage>
          <pub-id pub-id-type="pmid">10847601</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Blank</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Murphy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Warburton</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Speech production: Wernicke, Broca and beyond</article-title>
          <source>Brain</source>
          <year>2002</year>
          <volume>125</volume>
          <fpage>1829</fpage>
          <lpage>1838</lpage>
          <pub-id pub-id-type="pmid">12135973</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <citation citation-type="other">Boersma, P., Weenink, D., 2003. Praat: doing phonetics by computer.</citation>
      </ref>
      <ref id="bib12">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brown</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ngan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Liotti</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>A larynx area in the human motor cortex</article-title>
          <source>Cereb. Cortex</source>
          <year>2007</year>
          <volume>18</volume>
          <fpage>837</fpage>
          <lpage>845</lpage>
          <pub-id pub-id-type="pmid">17652461</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Callan</surname>
              <given-names>D.E.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Callan</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Akahane-Yamada</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Phonetic perceptual identification by native- and second-language speakers differentially activates brain regions involved with acoustic phonetic processing and those involved with articulatory–auditory/or- osensory internal models</article-title>
          <source>NeuroImage</source>
          <year>2004</year>
          <volume>22</volume>
          <fpage>1182</fpage>
          <lpage>1194</lpage>
          <pub-id pub-id-type="pmid">15219590</pub-id>
        </citation>
      </ref>
      <ref id="bib14">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Clarke</surname>
              <given-names>C.M.</given-names>
            </name>
            <name>
              <surname>Garrett</surname>
              <given-names>M.F.</given-names>
            </name>
          </person-group>
          <article-title>Rapid adaptation to foreign-accented English</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2004</year>
          <volume>116</volume>
          <fpage>3647</fpage>
          <lpage>3658</lpage>
          <pub-id pub-id-type="pmid">15658715</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Crinion</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Lambon-Ralph</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Warburton</surname>
              <given-names>E.A.</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.S.J.</given-names>
            </name>
          </person-group>
          <article-title>Temporal lobe regions engaged during normal speech comprehension</article-title>
          <source>Brain</source>
          <year>2003</year>
          <volume>126</volume>
          <fpage>1193</fpage>
          <lpage>1201</lpage>
          <pub-id pub-id-type="pmid">12690058</pub-id>
        </citation>
      </ref>
      <ref id="bib16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>D'Ausilio</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pulvermuller</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Salmas</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bufalari</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Begliomini</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fadiga</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Speech sounds like action: the somatotopy of speech processing in the motor system</article-title>
          <source>Curr. Biol.</source>
          <year>2009</year>
          <volume>19</volume>
          <fpage>381</fpage>
          <lpage>385</lpage>
          <pub-id pub-id-type="pmid">19217297</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dale</surname>
              <given-names>A.M.</given-names>
            </name>
          </person-group>
          <article-title>Optimal experimental design for event-related fMRI</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>8</volume>
          <fpage>109</fpage>
          <lpage>114</lpage>
          <pub-id pub-id-type="pmid">10524601</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.S.</given-names>
            </name>
          </person-group>
          <article-title>Hierarchical processing in spoken language comprehension</article-title>
          <source>J. Neurosci.</source>
          <year>2003</year>
          <volume>23</volume>
          <fpage>3423</fpage>
          <lpage>3431</lpage>
          <pub-id pub-id-type="pmid">12716950</pub-id>
        </citation>
      </ref>
      <ref id="bib19">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dick</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Saygin</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Galati</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Pitzalis</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bentrovato</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>D'Amico</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bates</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Pizzamiglio</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>What is involved and what is necessary for complex linguistic and nonlinguistic auditory processing: evidence from functional Magnetic Resonance Imaging and lesion data</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2007</year>
          <volume>19</volume>
          <fpage>799</fpage>
          <lpage>816</lpage>
          <pub-id pub-id-type="pmid">17488205</pub-id>
        </citation>
      </ref>
      <ref id="bib20">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Diehl</surname>
              <given-names>R.L.</given-names>
            </name>
            <name>
              <surname>Kluender</surname>
              <given-names>K.R.</given-names>
            </name>
          </person-group>
          <article-title>On the objects of speech perception</article-title>
          <source>Ecol. Psychol.</source>
          <year>1989</year>
          <volume>1</volume>
          <fpage>121</fpage>
          <lpage>144</lpage>
        </citation>
      </ref>
      <ref id="bib21">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dupoux</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Green</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Perceptual adjustment to highly compressed speech: effects of talker and rate changes</article-title>
          <source>J. Exp. Psychol.: Hum. Percept. Perform.</source>
          <year>1997</year>
          <volume>23</volume>
          <fpage>914</fpage>
          <lpage>927</lpage>
          <pub-id pub-id-type="pmid">9180050</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eden</surname>
              <given-names>G.F.</given-names>
            </name>
            <name>
              <surname>Joseph</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>H.E.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>C.P.</given-names>
            </name>
            <name>
              <surname>Zeffiro</surname>
              <given-names>T.A.</given-names>
            </name>
          </person-group>
          <article-title>Utilizing hemodynamic delay and dispersion to detect fMRI signal change without auditory interference: the behavior interleaved gradients technique</article-title>
          <source>Magn. Reson. Med.</source>
          <year>1999</year>
          <volume>41</volume>
          <fpage>13</fpage>
          <lpage>20</lpage>
          <pub-id pub-id-type="pmid">10025606</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Edmister</surname>
              <given-names>W.B.</given-names>
            </name>
            <name>
              <surname>Talavage</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Ledden</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Weisskoff</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>Improved auditory cortex imaging using clustered volume acquisitions</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>7</volume>
          <fpage>89</fpage>
          <lpage>97</lpage>
          <pub-id pub-id-type="pmid">9950066</pub-id>
        </citation>
      </ref>
      <ref id="bib24">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fadiga</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Craighero</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Buccino</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Rizzolatti</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Speech listening specifically modulates the excitability of tongue muscles: a TMS study</article-title>
          <source>Eur. J. Neurosci.</source>
          <year>2002</year>
          <volume>15</volume>
          <fpage>399</fpage>
          <lpage>402</lpage>
          <pub-id pub-id-type="pmid">11849307</pub-id>
        </citation>
      </ref>
      <ref id="bib25">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
            <name>
              <surname>Mazziotta</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>Assessing the significance of focal activations using their spatial extent</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1994</year>
          <volume>1</volume>
          <fpage>214</fpage>
          <lpage>220</lpage>
        </citation>
      </ref>
      <ref id="bib26">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gaab</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Gabrieli</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Glover</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Assessing the influence of scanner background noise on auditory processing. An fMRI study comparing three experimental designs with varying degrees of scanner noise</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2006</year>
          <volume>28</volume>
          <fpage>703</fpage>
          <lpage>720</lpage>
          <pub-id pub-id-type="pmid">17080440</pub-id>
        </citation>
      </ref>
      <ref id="bib27">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Galantucci</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Fowler</surname>
              <given-names>C.A.</given-names>
            </name>
            <name>
              <surname>Turvey</surname>
              <given-names>M.T.</given-names>
            </name>
          </person-group>
          <article-title>The motor theory of speech perception reviewed</article-title>
          <source>Psychonom. Bull. Rev.</source>
          <year>2006</year>
          <volume>13</volume>
          <fpage>361</fpage>
          <lpage>377</lpage>
        </citation>
      </ref>
      <ref id="bib28">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Giraud</surname>
              <given-names>A.L.</given-names>
            </name>
            <name>
              <surname>Kell</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Thierfelder</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Sterzer</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Russ</surname>
              <given-names>M.O.</given-names>
            </name>
            <name>
              <surname>Preibisch</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Contributions of sensory input auditory search and verbal comprehension to cortical activity during speech processing</article-title>
          <source>Cereb. Cortex</source>
          <year>2004</year>
          <volume>14</volume>
          <fpage>247</fpage>
          <lpage>255</lpage>
          <pub-id pub-id-type="pmid">14754865</pub-id>
        </citation>
      </ref>
      <ref id="bib29">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Glover</surname>
              <given-names>G.H.</given-names>
            </name>
          </person-group>
          <article-title>Deconvolution of impulse response in event-related BOLD fMRI</article-title>
          <source>NeuroImage</source>
          <year>1999</year>
          <volume>9</volume>
          <fpage>416</fpage>
          <lpage>429</lpage>
          <pub-id pub-id-type="pmid">10191170</pub-id>
        </citation>
      </ref>
      <ref id="bib30">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Goldstone</surname>
              <given-names>R.L.</given-names>
            </name>
          </person-group>
          <article-title>Perceptual learning</article-title>
          <source>Annu. Rev. Psychol.</source>
          <year>1998</year>
          <volume>49</volume>
          <fpage>585</fpage>
          <lpage>612</lpage>
          <pub-id pub-id-type="pmid">9496632</pub-id>
        </citation>
      </ref>
      <ref id="bib31">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Golomb</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Peelle</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Wingfield</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Effects of stimulus variability and adult aging on adaptation to time-compressed speech</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2007</year>
          <volume>121</volume>
          <fpage>1701</fpage>
          <lpage>1708</lpage>
          <pub-id pub-id-type="pmid">17407906</pub-id>
        </citation>
      </ref>
      <ref id="bib32">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Greenspan</surname>
              <given-names>S.L.</given-names>
            </name>
            <name>
              <surname>Nusbaum</surname>
              <given-names>H.C.</given-names>
            </name>
            <name>
              <surname>Pisoni</surname>
              <given-names>D.P.</given-names>
            </name>
          </person-group>
          <article-title>Perceptual learning of synthetic speech produced by rule</article-title>
          <source>J. Exp. Psychol.: Learn., Mem., Cogn.</source>
          <year>1988</year>
          <volume>14</volume>
          <fpage>421</fpage>
          <lpage>433</lpage>
          <pub-id pub-id-type="pmid">2969941</pub-id>
        </citation>
      </ref>
      <ref id="bib33">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guenther</surname>
              <given-names>F.H.</given-names>
            </name>
            <name>
              <surname>Nieto-Castanon</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ghosh</surname>
              <given-names>S.S.</given-names>
            </name>
            <name>
              <surname>Tourville</surname>
              <given-names>J.A.</given-names>
            </name>
          </person-group>
          <article-title>Representation of sound categories in auditory cortical maps</article-title>
          <source>J. Speech, Lang., Hear. Res.</source>
          <year>2004</year>
          <volume>47</volume>
          <fpage>46</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">15072527</pub-id>
        </citation>
      </ref>
      <ref id="bib34">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Haggard</surname>
              <given-names>M.P.</given-names>
            </name>
            <name>
              <surname>Akeroyd</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.R.</given-names>
            </name>
            <name>
              <surname>Summerfield</surname>
              <given-names>A.Q.</given-names>
            </name>
            <name>
              <surname>Elliot</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Gurney</surname>
              <given-names>E.M.</given-names>
            </name>
            <name>
              <surname>Bowtell</surname>
              <given-names>R.W.</given-names>
            </name>
          </person-group>
          <article-title>“Sparse” temporal sampling in auditory fMRI</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>7</volume>
          <fpage>213</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="pmid">10194620</pub-id>
        </citation>
      </ref>
      <ref id="bib35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hall</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Chambers</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Akeroyd</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Foster</surname>
              <given-names>J.R.</given-names>
            </name>
            <name>
              <surname>Coxon</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Palmer</surname>
              <given-names>A.R.</given-names>
            </name>
          </person-group>
          <article-title>Acoustic, psychophysical, and neuroimaging measurements of the effectiveness of active cancellation during auditory functional magnetic resonance imaging</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2009</year>
          <volume>125</volume>
          <fpage>347</fpage>
          <lpage>359</lpage>
          <pub-id pub-id-type="pmid">19173422</pub-id>
        </citation>
      </ref>
      <ref id="bib88">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hickok</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>The cortical organization of speech processing</article-title>
          <source>Nature Reviews Neuroscience</source>
          <year>2007</year>
          <volume>8</volume>
          <fpage>393</fpage>
          <lpage>402</lpage>
        </citation>
      </ref>
      <ref id="bib36">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hickok</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Love</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Swinney</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>E.C.</given-names>
            </name>
            <name>
              <surname>Buxton</surname>
              <given-names>R.B.</given-names>
            </name>
          </person-group>
          <article-title>Functional MR imaging during auditory word perception: a single trial presentation paradigm</article-title>
          <source>Brain Lang.</source>
          <year>1997</year>
          <volume>58</volume>
          <fpage>197</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="pmid">9184103</pub-id>
        </citation>
      </ref>
      <ref id="bib37">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hoshi</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Tanji</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Distinctions between dorsal and ventral premotor areas: anatomical connectivity and functional properties</article-title>
          <source>Curr. Opin. Neurobiol.</source>
          <year>2007</year>
          <volume>17</volume>
          <fpage>234</fpage>
          <lpage>242</lpage>
          <pub-id pub-id-type="pmid">17317152</pub-id>
        </citation>
      </ref>
      <ref id="bib38">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ito</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Tiede</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ostry</surname>
              <given-names>D.J.</given-names>
            </name>
          </person-group>
          <article-title>Somatosensory function in speech perception</article-title>
          <source>Proc. Natl. Acad. Sci.</source>
          <year>2009</year>
          <volume>106</volume>
          <fpage>1245</fpage>
          <lpage>1248</lpage>
          <pub-id pub-id-type="pmid">19164569</pub-id>
        </citation>
      </ref>
      <ref id="bib39">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bannister</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>
          <source>NeuroImage</source>
          <year>2002</year>
          <volume>17</volume>
          <fpage>825</fpage>
          <lpage>841</lpage>
          <pub-id pub-id-type="pmid">12377157</pub-id>
        </citation>
      </ref>
      <ref id="bib40">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A global optimisation method for robust affine registration of brain images</article-title>
          <source>Med. Image Anal.</source>
          <year>2001</year>
          <volume>5</volume>
          <fpage>143</fpage>
          <lpage>156</lpage>
          <pub-id pub-id-type="pmid">11516708</pub-id>
        </citation>
      </ref>
      <ref id="bib41">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Leech</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Holt</surname>
              <given-names>L.L.</given-names>
            </name>
            <name>
              <surname>Devlin</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Dick</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Expertise with non-speech sounds recruits speech-sensitive cortical regions</article-title>
          <source>J. Neurosci.</source>
          <year>2009</year>
          <volume>29</volume>
          <fpage>5234</fpage>
          <lpage>52389</lpage>
          <pub-id pub-id-type="pmid">19386919</pub-id>
        </citation>
      </ref>
      <ref id="bib42">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liberman</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Mattingly</surname>
              <given-names>I.G.</given-names>
            </name>
          </person-group>
          <article-title>The motor theory of speech perception — revised</article-title>
          <source>Cognition</source>
          <year>1985</year>
          <volume>21</volume>
          <fpage>1</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="pmid">4075760</pub-id>
        </citation>
      </ref>
      <ref id="bib43">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liberman</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Whalen</surname>
              <given-names>D.H.</given-names>
            </name>
          </person-group>
          <article-title>On the relation of speech to language</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2000</year>
          <volume>4</volume>
          <fpage>187</fpage>
          <lpage>196</lpage>
          <pub-id pub-id-type="pmid">10782105</pub-id>
        </citation>
      </ref>
      <ref id="bib44">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liberman</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Cooper</surname>
              <given-names>F.S.</given-names>
            </name>
            <name>
              <surname>Shankweiler</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Studdert-Kennedy</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Perception of the speech code</article-title>
          <source>Psychol. Rev.</source>
          <year>1967</year>
          <volume>74</volume>
          <fpage>431</fpage>
          <lpage>461</lpage>
          <pub-id pub-id-type="pmid">4170865</pub-id>
        </citation>
      </ref>
      <ref id="bib45">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>May</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Alcock</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Mwita</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>A computerized test of speed of language comprehension unconfounded by literacy</article-title>
          <source>Appl. Cogn. Psychol.</source>
          <year>2001</year>
          <volume>15</volume>
          <fpage>433</fpage>
          <lpage>443</lpage>
        </citation>
      </ref>
      <ref id="bib46">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McGurk</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>MacDonald</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Hearing lips and seeing voices</article-title>
          <source>Nature</source>
          <year>1976</year>
          <volume>264</volume>
          <fpage>746</fpage>
          <lpage>748</lpage>
          <pub-id pub-id-type="pmid">1012311</pub-id>
        </citation>
      </ref>
      <ref id="bib47">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meister</surname>
              <given-names>I.G.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Deblieck</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Iacoboni</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>The essential role of premotor cortex in speech perception</article-title>
          <source>Curr. Biol.</source>
          <year>2007</year>
          <volume>17</volume>
          <fpage>1692</fpage>
          <lpage>1696</lpage>
          <pub-id pub-id-type="pmid">17900904</pub-id>
        </citation>
      </ref>
      <ref id="bib48">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meyer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Steinhauer</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Alter</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Friederici</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Von Cramon</surname>
              <given-names>D.Y.</given-names>
            </name>
          </person-group>
          <article-title>Brain activity varies with modulation of dynamic pitch variance in sentence melody</article-title>
          <source>Brain Lang.</source>
          <year>2004</year>
          <volume>89</volume>
          <fpage>277</fpage>
          <lpage>289</lpage>
          <pub-id pub-id-type="pmid">15068910</pub-id>
        </citation>
      </ref>
      <ref id="bib49">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moulines</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Charpentier</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones</article-title>
          <source>Speech Commun.</source>
          <year>1990</year>
          <volume>9</volume>
          <fpage>453</fpage>
          <lpage>467</lpage>
        </citation>
      </ref>
      <ref id="bib50">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Murphy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Garavan</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Deriving the optimal number of events for an event-related fMRI study based on the spatial extent of activation</article-title>
          <source>NeuroImage</source>
          <year>2005</year>
          <volume>27</volume>
          <fpage>771</fpage>
          <lpage>777</lpage>
          <pub-id pub-id-type="pmid">15961321</pub-id>
        </citation>
      </ref>
      <ref id="bib51">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nosofsky</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>Attention, similarity, and the identification-specific relationship</article-title>
          <source>J. Exp. Psychol.: Gen.</source>
          <year>1986</year>
          <volume>115</volume>
          <fpage>39</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">2937873</pub-id>
        </citation>
      </ref>
      <ref id="bib52">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Okada</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hickok</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Left auditory cortex participates both in speech perception and speech production: neural overlap revealed by fMRI</article-title>
          <source>Brain Lang.</source>
          <year>2006</year>
          <volume>98</volume>
          <fpage>112</fpage>
          <lpage>117</lpage>
          <pub-id pub-id-type="pmid">16716388</pub-id>
        </citation>
      </ref>
      <ref id="bib53">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pallier</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Sebastián-Gallés</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Dupoux</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Christophe</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mehler</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Perceptual adjustment to time-compressed speech: a cross-linguistic study</article-title>
          <source>Mem. Cogn.</source>
          <year>1998</year>
          <volume>26</volume>
          <fpage>844</fpage>
          <lpage>851</lpage>
        </citation>
      </ref>
      <ref id="bib54">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Paulesu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
          </person-group>
          <article-title>The neural correlates of the verbal component of working memory</article-title>
          <source>Nature</source>
          <year>1993</year>
          <volume>362</volume>
          <fpage>342</fpage>
          <lpage>345</lpage>
          <pub-id pub-id-type="pmid">8455719</pub-id>
        </citation>
      </ref>
      <ref id="bib55">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peelle</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>McMillan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Moore</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Grossman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wingfield</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Dissociable patterns of brain activity during comprehension of rapid and syntactically complex speech: evidence from fMRI</article-title>
          <source>Brain Lang.</source>
          <year>2004</year>
          <volume>91</volume>
          <fpage>315</fpage>
          <lpage>325</lpage>
          <pub-id pub-id-type="pmid">15533557</pub-id>
        </citation>
      </ref>
      <ref id="bib56">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Poldrack</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Temple</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Protopapas</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nagarajan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tallal</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Merzenich</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gabrieli</surname>
              <given-names>J.D.E.</given-names>
            </name>
          </person-group>
          <article-title>Relations between the neural bases of dynamic auditory processing and phonological processing: evidence from fMRI</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2001</year>
          <volume>13</volume>
          <fpage>687</fpage>
          <lpage>697</lpage>
          <pub-id pub-id-type="pmid">11506664</pub-id>
        </citation>
      </ref>
      <ref id="bib57">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Price</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Thierry</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Griffiths</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Speech-specific auditory processing: where is it?</article-title>
          <source>Trends Cogn. Neurosci.</source>
          <year>2005</year>
          <volume>9</volume>
          <fpage>271</fpage>
          <lpage>276</lpage>
        </citation>
      </ref>
      <ref id="bib58">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pulvermüller</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Huss</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kherif</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Moscoso del Prado</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hauk</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Shtyrov</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Motor cortex maps articulatory features of speech sounds</article-title>
          <source>Proc. Am.Acad. Sci.</source>
          <year>2006</year>
          <volume>103</volume>
          <fpage>7865</fpage>
          <lpage>7870</lpage>
        </citation>
      </ref>
      <ref id="bib59">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rauschecker</surname>
              <given-names>A.M.M.</given-names>
            </name>
            <name>
              <surname>Pringle</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Watkins</surname>
              <given-names>K.E.E.</given-names>
            </name>
          </person-group>
          <article-title>Changes in neural activity associated with learning to articulate novel auditory pseudowords by covert repetition</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2008</year>
          <volume>29</volume>
          <fpage>1231</fpage>
          <lpage>1242</lpage>
          <pub-id pub-id-type="pmid">17948887</pub-id>
        </citation>
      </ref>
      <ref id="bib60">
        <citation citation-type="other">Rauschecker, A.M.M., Pringle, A., Watkins, K.E.E., in press. Changes in neural activity associated with learning to articulate novel auditory pseudowords by covert repetition. Human Brain Mapping.</citation>
      </ref>
      <ref id="bib61">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Romero</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Walsh</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Papagno</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>The neural correlates of phonological short-term memory: a repetitive transcranial magnetic stimulation study</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2006</year>
          <volume>18</volume>
          <fpage>1147</fpage>
          <lpage>1155</lpage>
          <pub-id pub-id-type="pmid">16839288</pub-id>
        </citation>
      </ref>
      <ref id="bib62">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rosen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Faulkner</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wilkinson</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Adaptation by normal listeners to upward spectral shifts of speech: implications for cochlear implants</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1999</year>
          <volume>106</volume>
          <fpage>3629</fpage>
          <lpage>3636</lpage>
          <pub-id pub-id-type="pmid">10615701</pub-id>
        </citation>
      </ref>
      <ref id="bib63">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rushworth</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Johansen-Berg</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Gobel</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Devlin</surname>
              <given-names>J.T.</given-names>
            </name>
          </person-group>
          <article-title>The left parietal and premotor cortices: motor attention and selection</article-title>
          <source>NeuroImage</source>
          <year>2003</year>
          <volume>20</volume>
          <issue>Suppl. 1</issue>
          <fpage>S89</fpage>
          <lpage>S100</lpage>
          <pub-id pub-id-type="pmid">14597301</pub-id>
        </citation>
      </ref>
      <ref id="bib64">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schmidt</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Zaehle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Geiser</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Boesiger</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jancke</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Silent and continuous fMRI scanning differentially modulate activation in an auditory language comprehension task</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2008</year>
          <volume>29</volume>
          <fpage>46</fpage>
          <lpage>56</lpage>
          <pub-id pub-id-type="pmid">17318832</pub-id>
        </citation>
      </ref>
      <ref id="bib65">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarzbauer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.H.</given-names>
            </name>
            <name>
              <surname>Rodd</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Johnsrude</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Interleaved silent steady state (ISSS) imaging: a new sparse imaging method applied to auditory fMRI</article-title>
          <source>NeuroImage</source>
          <year>2006</year>
          <volume>29</volume>
          <fpage>774</fpage>
          <lpage>782</lpage>
          <pub-id pub-id-type="pmid">16226896</pub-id>
        </citation>
      </ref>
      <ref id="bib66">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>The functional neuroanatomy of prelexical processing in speech perception</article-title>
          <source>Cognition</source>
          <year>2004</year>
          <volume>92</volume>
          <fpage>13</fpage>
          <lpage>45</lpage>
          <pub-id pub-id-type="pmid">15037125</pub-id>
        </citation>
      </ref>
      <ref id="bib67">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Blank</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>Rosen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wise</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title>
          <source>Brain</source>
          <year>2000</year>
          <volume>123</volume>
          <fpage>2400</fpage>
          <lpage>2406</lpage>
          <pub-id pub-id-type="pmid">11099443</pub-id>
        </citation>
      </ref>
      <ref id="bib68">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sebastián-Gallés</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Dupoux</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Costa</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mehler</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Adaptation to time-compressed speech: phonological determinants</article-title>
          <source>Percept. Psychophys.</source>
          <year>2000</year>
          <volume>62</volume>
          <fpage>834</fpage>
          <lpage>842</lpage>
          <pub-id pub-id-type="pmid">10883588</pub-id>
        </citation>
      </ref>
      <ref id="bib69">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shah</surname>
              <given-names>N.J.</given-names>
            </name>
            <name>
              <surname>Jäncke</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Grosse-Ruyken</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Muller-Gartner</surname>
              <given-names>H.W.</given-names>
            </name>
          </person-group>
          <article-title>Influence of acoustic masking noise in fMRI of the auditory cortex during phonetic discrimination</article-title>
          <source>J. Magn. Reson. Imaging</source>
          <year>1999</year>
          <volume>9</volume>
          <fpage>19</fpage>
          <lpage>25</lpage>
          <pub-id pub-id-type="pmid">10030646</pub-id>
        </citation>
      </ref>
      <ref id="bib70">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shannon</surname>
              <given-names>R.V.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>F.G.</given-names>
            </name>
            <name>
              <surname>Kamath</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Wygonski</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ekelid</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Speech recognition with primarily temporal cues</article-title>
          <source>Science</source>
          <year>1995</year>
          <volume>270</volume>
          <fpage>303</fpage>
          <lpage>304</lpage>
          <pub-id pub-id-type="pmid">7569981</pub-id>
        </citation>
      </ref>
      <ref id="bib71">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Skipper</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Nusbaum</surname>
              <given-names>H.C.</given-names>
            </name>
            <name>
              <surname>Small</surname>
              <given-names>S.L.</given-names>
            </name>
          </person-group>
          <article-title>Lending a helping hand to hearing: another motor theory of speech perception</article-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Arbib</surname>
              <given-names>M.A.</given-names>
            </name>
          </person-group>
          <source>Action to Language via the Mirror Neuron System</source>
          <year>2006</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge: MA</publisher-loc>
          <fpage>250</fpage>
          <lpage>285</lpage>
        </citation>
      </ref>
      <ref id="bib72">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stevens</surname>
              <given-names>K.N.</given-names>
            </name>
            <name>
              <surname>Klatt</surname>
              <given-names>D.H.</given-names>
            </name>
          </person-group>
          <article-title>Role of formant transitions in the voiced–voiceless distinction for stops</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>1974</year>
          <volume>55</volume>
          <fpage>654</fpage>
          <lpage>659</lpage>
        </citation>
      </ref>
      <ref id="bib73">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Talavage</surname>
              <given-names>T.M.</given-names>
            </name>
            <name>
              <surname>Edmister</surname>
              <given-names>W.B.</given-names>
            </name>
            <name>
              <surname>Ledden</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Weisskoff</surname>
              <given-names>R.M.</given-names>
            </name>
          </person-group>
          <article-title>Quantitative assessment of auditory cortex responses induced by imager acoustic noise</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>7</volume>
          <fpage>79</fpage>
          <lpage>88</lpage>
          <pub-id pub-id-type="pmid">9950065</pub-id>
        </citation>
      </ref>
      <ref id="bib74">
        <citation citation-type="other">Tamer, G., Talavage, T., Wen-Ming, L., in press. Characterizing response to acoustic imaging noise for auditory event-related fMRI. IEEE Transactions in Biomedical Engineering.</citation>
      </ref>
      <ref id="bib75">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Toni</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>de Lange</surname>
              <given-names>F.P.</given-names>
            </name>
            <name>
              <surname>Noordzij</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Hagoort</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Language beyond action</article-title>
          <source>J. Physiol. - Paris</source>
          <year>2008</year>
          <volume>102</volume>
          <fpage>71</fpage>
          <lpage>79</lpage>
          <pub-id pub-id-type="pmid">18472404</pub-id>
        </citation>
      </ref>
      <ref id="bib76">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Veltman</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Mechelli</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>The importance of distributed sampling in blocked functional Magnetic Resonance Imaging designs</article-title>
          <source>NeuroImage</source>
          <year>2002</year>
          <volume>17</volume>
          <fpage>1203</fpage>
          <lpage>1206</lpage>
          <pub-id pub-id-type="pmid">12414260</pub-id>
        </citation>
      </ref>
      <ref id="bib77">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watkins</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Strafella</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Paus</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Seeing and hearing speech excites the motor system involved in speech production</article-title>
          <source>Neuropsychologia</source>
          <year>2003</year>
          <volume>41</volume>
          <fpage>989</fpage>
          <lpage>994</lpage>
          <pub-id pub-id-type="pmid">12667534</pub-id>
        </citation>
      </ref>
      <ref id="bib78">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watkins</surname>
              <given-names>K.E.</given-names>
            </name>
            <name>
              <surname>Vargha-Khadem</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Passingham</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Connelly</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Mishkin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gadian</surname>
              <given-names>D.G.</given-names>
            </name>
          </person-group>
          <article-title>MRI analysis of an inherited speech and language disorder: structural brain abnormalities</article-title>
          <source>Brain</source>
          <year>2002</year>
          <volume>125</volume>
          <fpage>465</fpage>
          <lpage>478</lpage>
          <pub-id pub-id-type="pmid">11872605</pub-id>
        </citation>
      </ref>
      <ref id="bib79">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wilson</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Saygin</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Sereno</surname>
              <given-names>M.I.</given-names>
            </name>
            <name>
              <surname>Iacoboni</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Listening to speech activates motor areas involved in speech production</article-title>
          <source>Nat. Neurosci.</source>
          <year>2004</year>
          <volume>7</volume>
          <fpage>701</fpage>
          <lpage>702</lpage>
          <pub-id pub-id-type="pmid">15184903</pub-id>
        </citation>
      </ref>
      <ref id="bib80">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wingfield</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Peelle</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Grossman</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Speech rate and syntactic complexity as multiplicative factors in speech comprehension by young and older adults</article-title>
          <source>Aging, Neuropsychol. Cogn.</source>
          <year>2003</year>
          <volume>10</volume>
          <fpage>310</fpage>
          <lpage>322</lpage>
        </citation>
      </ref>
      <ref id="bib81">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wise</surname>
              <given-names>R.G.</given-names>
            </name>
            <name>
              <surname>Tracey</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>The role of fMRI in drug discovery</article-title>
          <source>J. Magn. Reson. Imaging</source>
          <year>2006</year>
          <volume>23</volume>
          <fpage>862</fpage>
          <lpage>876</lpage>
          <pub-id pub-id-type="pmid">16649197</pub-id>
        </citation>
      </ref>
      <ref id="bib82">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wise</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>S.K.</given-names>
            </name>
            <name>
              <surname>Blank</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Mummery</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Murphy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Warburton</surname>
              <given-names>et. al.</given-names>
            </name>
          </person-group>
          <article-title>Separate neural subsystems within ‘Wernicke's area’</article-title>
          <source>Brain</source>
          <year>2001</year>
          <volume>124</volume>
          <fpage>83</fpage>
          <lpage>95</lpage>
          <pub-id pub-id-type="pmid">11133789</pub-id>
        </citation>
      </ref>
      <ref id="bib83">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wong</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Pisoni</surname>
              <given-names>D.B.</given-names>
            </name>
            <name>
              <surname>Learn</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gandour</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Miyamoto</surname>
              <given-names>R.T.</given-names>
            </name>
            <name>
              <surname>Hutchins</surname>
              <given-names>G.D.</given-names>
            </name>
          </person-group>
          <article-title>PET imaging of differential cortical activation by monoaural speech and nonspeech stimuli</article-title>
          <source>Hear. Res.</source>
          <year>2002</year>
          <volume>166</volume>
          <fpage>9</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="pmid">12062754</pub-id>
        </citation>
      </ref>
      <ref id="bib84">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Woolric h</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Ripley</surname>
              <given-names>B.D.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title>
          <source>NeuroImage</source>
          <year>2001</year>
          <volume>14</volume>
          <fpage>1370</fpage>
          <lpage>1386</lpage>
          <pub-id pub-id-type="pmid">11707093</pub-id>
        </citation>
      </ref>
      <ref id="bib85">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Jenkinson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>J.M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>Fully Bayesian spatio-temporal modeling of FMRI data</article-title>
          <source>IEEE Trans. Med. Imaging</source>
          <year>2004</year>
          <volume>23</volume>
          <fpage>213</fpage>
          <lpage>231</lpage>
          <pub-id pub-id-type="pmid">14964566</pub-id>
        </citation>
      </ref>
      <ref id="bib86">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Marrett</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Neelin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Vandal</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>A.C.</given-names>
            </name>
          </person-group>
          <article-title>A unified statistical approach for determining significant signals in images of cerebral activation</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1996</year>
          <volume>4</volume>
          <fpage>58</fpage>
          <lpage>83</lpage>
        </citation>
      </ref>
      <ref id="bib87">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zaehle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Schmidt</surname>
              <given-names>C.F.</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Baumann</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Baltes</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Boesiger</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jancke</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of “silent” clustered and sparse temporal fMRI acquisitions in tonal and speech perception tasks</article-title>
          <source>NeuroImage</source>
          <year>2007</year>
          <volume>37</volume>
          <fpage>1195</fpage>
          <lpage>1204</lpage>
          <pub-id pub-id-type="pmid">17644001</pub-id>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Presentation of blocks of normal-speed (N) sentences and time-compressed (TC) sentences (top) in the experiment, plus sequence of events in the presentation of one sentence (bottom).</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Behavioural results in the scanner (In-scanner) and outside the scanner in the post-test (Post-test) for the average error rates (top) and the average response times (bottom). Averages for the error rates and response times are represented per mini-block of 4 sentences (black error bars, 4 per block) and averaged across the blocks (1–4) of 16 sentences (underlying grey bars). The averages for blocks 1–4 (16 sentences per block) were used for analysing both the behavioural and functional imaging data. Filled circles represent a mean over subjects of a four-sentence mini-block and error bars represent standard error of the mean. Dark bars refer to normal-speed sentences and lighter bars indicate time-compressed (TC) sentences.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>Activation for time-compressed relative to normal sentences (green) is superimposed on activation for all sentences relative to the fixation baseline (red) and shown on the mean structural image from the 18 participants.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Adaptation-related activation patterns in the left (top row) and right (bottom row) hemispheres are shown in blue superimposed on all sentences relative to baseline (red). For each of the four regions, bar plots indicate changes in BOLD signal relative to baseline for the four blocks of normal sentences (orange) and the four blocks of time-compressed sentences (blue).</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <table-wrap position="float" id="tbl1">
      <label>Table 1</label>
      <caption>
        <p>Activation for auditory sentences (normal-speed and time-compressed) relative to the fixation baseline.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top">Region</th>
            <th valign="top">Hemisphere</th>
            <th colspan="3" valign="top">Peak coordinate</th>
            <th valign="top"><italic>Z</italic>-score</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="6" valign="top">Temporal lobe</td>
          </tr>
          <tr>
            <td valign="top"> STG/STS</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 58</td>
            <td align="char" valign="top">− 16</td>
            <td align="char" valign="top">− 6</td>
            <td valign="top">6.6</td>
          </tr>
          <tr>
            <td valign="top"> STG/STS</td>
            <td valign="top">R</td>
            <td align="char" valign="top">+ 62</td>
            <td align="char" valign="top">− 12</td>
            <td align="char" valign="top">− 8</td>
            <td valign="top">7.0</td>
          </tr>
          <tr>
            <td colspan="6" valign="top">Frontal</td>
          </tr>
          <tr>
            <td valign="top"> Frontal operculum</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 32</td>
            <td align="char" valign="top">+ 22</td>
            <td align="char" valign="top">− 6</td>
            <td valign="top">5.5</td>
          </tr>
          <tr>
            <td valign="top"> Frontal operculum</td>
            <td valign="top">R</td>
            <td align="char" valign="top">+ 40</td>
            <td align="char" valign="top">+ 22</td>
            <td align="char" valign="top">− 8</td>
            <td valign="top">4.7</td>
          </tr>
          <tr>
            <td valign="top"> Pars opercularis</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 48</td>
            <td align="char" valign="top">+ 12</td>
            <td align="char" valign="top">+ 18</td>
            <td valign="top">5.4</td>
          </tr>
          <tr>
            <td valign="top"> Pars orbitalis</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 52</td>
            <td align="char" valign="top">+ 32</td>
            <td align="char" valign="top">− 8</td>
            <td valign="top">4.6</td>
          </tr>
          <tr>
            <td valign="top"> Pre-SMA</td>
            <td valign="top">B</td>
            <td align="char" valign="top">± 4</td>
            <td align="char" valign="top">+ 2</td>
            <td align="char" valign="top">+ 48</td>
            <td valign="top">5.4</td>
          </tr>
          <tr>
            <td valign="top"> Cingulate sulcus</td>
            <td valign="top">B</td>
            <td align="char" valign="top">0</td>
            <td align="char" valign="top">+ 22</td>
            <td align="char" valign="top">+ 40</td>
            <td valign="top">5.9</td>
          </tr>
          <tr>
            <td colspan="6" valign="top">Parietal</td>
          </tr>
          <tr>
            <td valign="top"> Anterior SMG</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 42</td>
            <td align="char" valign="top">− 22</td>
            <td align="char" valign="top">+ 50</td>
            <td valign="top">4.6</td>
          </tr>
          <tr>
            <td colspan="6" valign="top">Subcortical</td>
          </tr>
          <tr>
            <td valign="top"> Thalamus</td>
            <td valign="top">L</td>
            <td align="char" valign="top">− 8</td>
            <td align="char" valign="top">− 16</td>
            <td align="char" valign="top">0</td>
            <td valign="top">4.9</td>
          </tr>
          <tr>
            <td valign="top"> Medial geniculate body</td>
            <td valign="top">B<xref rid="tblfn1" ref-type="table-fn">a</xref></td>
            <td align="char" valign="top">− 10</td>
            <td align="char" valign="top">− 28</td>
            <td align="char" valign="top">− 10</td>
            <td valign="top">4.2</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Coordinates are in MNI standard space.</p>
        </fn>
      </table-wrap-foot>
      <table-wrap-foot>
        <fn id="tblfn1">
          <label>a</label>
          <p>This activation presented bilaterally but was only significant in the left hemisphere where its extent blurs into the other thalamic activation.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap position="float" id="tbl2">
      <label>Table 2</label>
      <caption>
        <p>Activation associated with time-compressed sentences.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top">Region</th>
            <th valign="top">Hemisphere</th>
            <th valign="top">Peak coordinate</th>
            <th valign="top"/>
            <th valign="top"/>
            <th valign="top"><italic>Z</italic>-score</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="6" valign="top">A. Time-compressed relative to normal sentences</td>
          </tr>
          <tr>
            <td valign="top"> Anterior STG/STS</td>
            <td valign="top">L</td>
            <td valign="top">− 60</td>
            <td valign="top">− 14</td>
            <td align="char" valign="top">0</td>
            <td valign="top">4.7</td>
          </tr>
          <tr>
            <td valign="top"> Posterior STG/STS</td>
            <td valign="top">L</td>
            <td valign="top">− 58</td>
            <td valign="top">− 46</td>
            <td align="char" valign="top">+ 4</td>
            <td valign="top">4.7</td>
          </tr>
          <tr>
            <td valign="top"> Anterior STG/STS</td>
            <td valign="top">R</td>
            <td valign="top">+ 64</td>
            <td valign="top">− 14</td>
            <td align="char" valign="top">0</td>
            <td valign="top">4.7</td>
          </tr>
          <tr>
            <td valign="top"> Posterior STG/STS</td>
            <td valign="top">R</td>
            <td valign="top">+ 56</td>
            <td valign="top">− 32</td>
            <td align="char" valign="top">+ 4</td>
            <td valign="top">4.0</td>
          </tr>
          <tr>
            <td valign="top"> Pre-SMA</td>
            <td valign="top">B</td>
            <td valign="top">0</td>
            <td valign="top">+ 12</td>
            <td align="char" valign="top">+ 60</td>
            <td valign="top">4.3</td>
          </tr>
          <tr>
            <td valign="top"> Cingulate sulcus</td>
            <td valign="top">B</td>
            <td valign="top">0</td>
            <td valign="top">+ 22</td>
            <td align="char" valign="top">+ 44</td>
            <td valign="top">4.3</td>
          </tr>
          <tr>
            <td valign="top"> Frontal operculum<xref rid="tblfn2" ref-type="table-fn">a</xref></td>
            <td valign="top">L</td>
            <td valign="top">− 36</td>
            <td valign="top">+ 24</td>
            <td align="char" valign="top">− 4</td>
            <td valign="top">3.4</td>
          </tr>
          <tr>
            <td valign="top"> Frontal operculum<xref rid="tblfn2" ref-type="table-fn">a</xref></td>
            <td valign="top">R</td>
            <td valign="top">+ 36</td>
            <td valign="top">+ 25</td>
            <td align="char" valign="top">+ 2</td>
            <td valign="top">4.1</td>
          </tr>
          <tr>
            <td colspan="6" valign="top">B. Adaptation-related changes</td>
          </tr>
          <tr>
            <td valign="top"> Posterior STS</td>
            <td valign="top">L</td>
            <td valign="top">− 54</td>
            <td valign="top">− 52</td>
            <td align="char" valign="top">2</td>
            <td valign="top">3.6</td>
          </tr>
          <tr>
            <td valign="top"> Ventral premotor</td>
            <td valign="top">L</td>
            <td valign="top">− 50</td>
            <td valign="top">+ 14</td>
            <td align="char" valign="top">+ 12</td>
            <td valign="top">3.1</td>
          </tr>
          <tr>
            <td valign="top"> Anterior STG</td>
            <td valign="top">R</td>
            <td valign="top">+ 58</td>
            <td valign="top">− 8</td>
            <td align="char" valign="top">− 4</td>
            <td valign="top">3.3</td>
          </tr>
          <tr>
            <td valign="top"> Posterior STS</td>
            <td valign="top">R</td>
            <td valign="top">+ 64</td>
            <td valign="top">− 40</td>
            <td align="char" valign="top">0</td>
            <td valign="top">3.5</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn>
          <p>Coordinates are in MNI standard space.</p>
        </fn>
      </table-wrap-foot>
      <table-wrap-foot>
        <fn id="tblfn2">
          <label>a</label>
          <p>Although there was activation in the frontal operculum, it was not extensive enough to reach significance using the cluster test and here is reported only for completeness.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </floats-wrap>
</article>