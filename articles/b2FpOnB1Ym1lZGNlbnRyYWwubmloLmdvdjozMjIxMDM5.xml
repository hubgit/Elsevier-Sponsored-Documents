<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3221039</article-id>
      <article-id pub-id-type="pmid">20044007</article-id>
      <article-id pub-id-type="publisher-id">YNIMG6875</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.077</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Perceptual shape sensitivity to upright and inverted faces is reflected in neuronal adaptation</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Gilaie-Dotan</surname>
            <given-names>Sharon</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
          <xref rid="fn1" ref-type="fn">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Gelbard-Sagiv</surname>
            <given-names>Hagar</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="fn1" ref-type="fn">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Malach</surname>
            <given-names>Rafael</given-names>
          </name>
          <email>rafi.malach@gmail.com</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>Department of Neurobiology, Weizmann Institute of Science, Rehovot 76100, Israel</aff>
      <aff id="aff2"><label>b</label>Wellcome Centre for Neuroimaging, University College London, London, UK</aff>
      <aff id="aff3"><label>c</label>Institute of Cognitive Neuroscience, University College London, London, UK</aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. Fax: +972 8 934 4131. <email>rafi.malach@gmail.com</email></corresp>
        <fn id="fn1">
          <label>1</label>
          <p>These authors contributed equally to this work.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>01</day>
        <month>4</month>
        <year>2010</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <day>01</day>
        <month>4</month>
        <year>2010</year>
      </pub-date>
      <volume>50</volume>
      <issue>2-3</issue>
      <fpage>383</fpage>
      <lpage>395</lpage>
      <history>
        <date date-type="received">
          <day>5</day>
          <month>10</month>
          <year>2009</year>
        </date>
        <date date-type="rev-recd">
          <day>2</day>
          <month>12</month>
          <year>2009</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>12</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2010 Elsevier Inc.</copyright-statement>
        <copyright-year>2009</copyright-year>
        <copyright-holder>Elsevier Inc.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Using an fMR-adaptation paradigm for different face morphing levels we have recently demonstrated a narrow neuronal tuning to faces even at the sub-exemplar level which was tightly related to perceptual discrimination (Gilaie-Dotan and Malach, 2007). However, it is unclear whether this relationship is unique to faces or is a general property of object representations including unfamiliar objects, and whether the adaptation tuning is due to physical changes in the stimulus or to changes in perceptual discrimination. Here we compared the same face-morph paradigm for upright and inverted faces, thus modulating familiarity and perceptual discrimination effects while equating all low-level features. We found, as expected, a perceptual “inversion effect”, i.e. a significant reduction in inverted face discrimination. Importantly, the fMR-adaptation tuning in the fusiform face area (FFA) changed in accordance with the different perceptual sensitivity both for upright and inverted faces. Additional object selective regions displayed differential tuning widths to the two categories. Our results are compatible with a model by which the ability of human observers to discriminate objects depends on the shape tuning properties of individual neurons.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Face perception</kwd>
        <kwd>fMRI</kwd>
        <kwd>Human cortex</kwd>
        <kwd>Object recognition</kwd>
        <kwd>Ventral stream</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec>
      <title>Introduction</title>
      <p>Human perception manifests a remarkable capacity for discriminating subtle shape changes in visual objects while maintaining a robust invariance for other optical parameters such as position, size and contrast. A large body of research has by now demonstrated that these invariances are accomplished gradually along the visual hierarchy leading from primary, retinotopic visual areas—which are sensitive to low-level parameters such as position (<xref rid="bib25 bib50" ref-type="bibr">Grill-Spector et al., 1998; Sereno et al., 1995</xref>), size and contrast (<xref rid="bib4" ref-type="bibr">Avidan et al., 2002a</xref>), to high order visual areas which show a reduced sensitivity to low-level, local image features, yet exhibit enhanced sensitivity to more holistic aspects of the stimulus such as shape changes (<xref rid="bib26" ref-type="bibr">Grill-Spector et al., 2001</xref>).</p>
      <p>In particular, high order object-related regions often display object category selectivity—with prominent examples of face-related (<xref rid="bib3 bib29 bib32 bib35 bib45" ref-type="bibr">Allison et al., 1994; Ishai et al., 1999; Kanwisher et al., 1997; Levy et al., 2001; Puce et al., 1995</xref>), place-related (<xref rid="bib1 bib9 bib29 bib35" ref-type="bibr">Aguirre et al., 1998; Epstein and Kanwisher, 1998; Ishai et al., 1999; Levy et al., 2001</xref><xref rid="bib36" ref-type="bibr">, 2004a</xref>), and more general object-related categories (<xref rid="bib23 bib26 bib27 bib39 bib53" ref-type="bibr">Grill-Spector, 2003; Grill-Spector et al., 2001; Hasson et al., 2003; Malach et al., 1995; Tootell et al., 1996</xref>).</p>
      <p>While a large body of experimental data has accumulated regarding the various aspects of human object representations, an important, still unresolved question concerns the role of <italic>individual neurons</italic> in determining the remarkable behavioral shape sensitivity of human observers. In particular, it is still not clear whether human shape sensitivity is already present in the shape tuning of individual neurons or whether it is a product of a joint population activity while the individual neuronal selectivity is far coarser.</p>
      <p>To illustrate this issue more concretely, consider two extreme alternatives that can be envisioned in the relationship between individual neurons' shape tuning and the psychophysical recognition performance. In the first model, akin to the extensively discussed “population vector” concept, the shape tuning curves of the neurons are broad and overlapping. The tuning curve of each neuron in such a model is far coarser than the actual discrimination performance of the observer. Narrow behavioral tuning can then be computed by using the <italic>relationship</italic> between individual neurons, e.g. the ratio of firing rates, as the code of the represented object. A canonic example of such vector representation is that of color coding where activity in three broadly tuned and overlapping color sensitive receptors generates a representation of numerous narrowly tuned hues. Similar population coding schemes have been proposed for direction of motor movement generation in the primate cortex (<xref rid="bib20" ref-type="bibr">Georgopoulos et al., 1986</xref>) and for motion detection and discrimination in MT+ (<xref rid="bib30" ref-type="bibr">Jazayeri and Movshon, 2006</xref>). More recently the notion of broadly tuned neuronal representations was extended to faces, both in fMRI (<xref rid="bib31" ref-type="bibr">Jiang et al., 2006</xref>), and in single cell recordings of monkeys (<xref rid="bib17" ref-type="bibr">Freiwald et al., 2009</xref>). A schematic illustration of this model is presented in <xref rid="fig1" ref-type="fig">Fig. 1</xref>A (III) and we will refer to it as the “population tuning” coding model. At the other extreme, one could envision a scheme in which the shape tuning sensitivity of the observer is derived from the tuning of <italic>individual</italic> shape-selective neurons. In such shape tuning model the limiting factor on the discrimination performance is the independent tuning curves of individual neurons and no additional “sharpening” is achieved by the group relationships among these different neurons. A paradigmatic example of this kind of model with “narrow tuning” is the cochlear representation of sound frequencies. Intriguingly, such hyper-fine tuning of single cortical neurons that matches human sound discrimination has recently been discovered also in single neuron recordings from human auditory cortex (<xref rid="bib6" ref-type="bibr">Bitterman et al., 2008</xref>). This model is illustrated in <xref rid="fig1" ref-type="fig">Figs. 1</xref>A (I) and (II) and we will refer to it as the “independent neurons tuning” coding model.</p>
      <p>It is important to emphasize that the independent neurons tuning model <italic>does not</italic> imply a “grandmother” type scheme—in which only few neurons are activated by each object image. To the contrary, even in an independent neurons tuning scheme a large neuronal population might be activated by each object image. The difference between the two models is rather in the <italic>source</italic> of the behavioral shape sensitivity. While in the population model this is an emergent property of the inter-relationships within the population, in the independent neurons model the behavioral shape sensitivity is a direct reflection of the tuning properties of the individual neurons. We elaborate further on these points in the discussion.</p>
      <p>An important prediction that could easily distinguish between these two alternative models is as follows: in the independent tuning model, stimuli for which human observers show fine or coarse discriminations should have neuronal representations manifesting narrow or coarse tuning respectively. These alternatives are illustrated as “narrow” and “broad” tuning in <xref rid="fig1" ref-type="fig">Figs. 1</xref>A (I) and (II). In contrast, no such direct relationship is expected from the population coding model, since here it is the <italic>relationship</italic> between the neurons rather than their independent tuning selectivity that matters. Rather, in a population model the reduced selectivity is typically due to a reduction in the number of channels—e.g. the loss of a color pigment associated with “color blindness”.</p>
      <p>When employing the method of BOLD fMRI, it is impossible to directly examine the shape selectivity at the level of individual neurons since the BOLD signal represents the averaged activity of a large number of neurons. A possible means for targeting the neuronal responses may be offered through adaptation (also termed repetition suppression) effects (<xref rid="bib24" ref-type="bibr">Grill-Spector and Malach, 2001</xref>). By repeatedly presenting a single object image, one presumably could target the neurons that are selectively activated by this object image and suppress their activation (i.e. the adaptation baseline). If we then parametrically change the shape of the object (e.g. through morphing), we may reveal, at least qualitatively, the sensitivity (tuning) of the neuronal population specifically engaged in representing that aspect of the object image (evident by release from adaptation).</p>
      <p>In a previous research, using upright morphed faces (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>B left panel) and an fMRI-adaptation paradigm, we found that cortical face representations appear to follow the independent neurons coding (<xref rid="fig1" ref-type="fig">Fig. 1</xref>C (I)) showing a strikingly narrow, sub-exemplar tuning as revealed by neuronal adaptation (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>). This narrow neuronal tuning matched quite well the behavioral discrimination performance of subjects.</p>
      <p>However, given the special status of upright faces (<xref rid="bib12 bib14 bib42" ref-type="bibr">Farah, 1996; Farah et al., 1998; McKone et al., 2007</xref>), it could be the case that faces engage specialized representations which do not generalize to other shape categories.</p>
      <p>In the present study we thus used fMR-adaptation effects to compare shape tuning for upright faces with that for <italic>inverted faces</italic> (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>B) across different regions in object-selective cortex. The advantage of using upright and inverted faces is three-fold: (i) manipulating performance: it is well established that the human discrimination performance is disproportionately impaired for inverted compared to upright faces. This effect, termed the face inversion effect (FIE), has been extensively studied and documented (e.g. (<xref rid="bib13 bib22 bib44 bib46 bib47 bib54 bib58 bib59" ref-type="bibr">Farah et al., 1995; Goffaux and Rossion, 2007; Murray et al., 2000; Rhodes et al., 1993; Rossion and Gauthier, 2002; Valentine, 1988; Yin, 1969; Yovel and Kanwisher, 2005</xref>)); (ii) manipulating familiarity: faces are one of the categories which humans are known to be highly familiar with whereas inverted faces are an opposite extreme with humans having no or very limited experience with; (iii) controlling for physical differences: In contrast to this robust behavioral difference between upright and inverted faces, the low level feature composition of upright and inverted faces is identical.</p>
      <p>A prediction of the independent neurons code, is that the behavioral difference in shape sensitivity that exists between upright and inverted faces (the FIE) should lead to different profiles of adaptation for upright and for inverted faces. These predictions are shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>C (left panels I and II). In the population code scheme on the other hand, one would expect a broad adaptation profile regardless of face orientation (<xref rid="fig1" ref-type="fig">Fig. 1</xref>C (III)), with perhaps a higher level of activation for the upright faces.</p>
      <p>Our results replicate our previous findings of narrow tuning to upright faces (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>). However, for inverted faces, we find a significantly broader adaptation profile in the FFA. Importantly, this broadening nicely correlates with the reduced perceptual ability to detect shape differences in these images, thus supporting a model of independent-neuronal code in the human FFA.</p>
    </sec>
    <sec sec-type="materials|methods" id="sec2">
      <title>Materials and methods</title>
      <sec>
        <title>Subjects</title>
        <p>12 healthy subjects (6 women, ages 22–31, average age 28) participated each in the three fMRI experiments (upright morph, inverted morph, and visual localizer) and in two behavioral experiments (upright morph and inverted morph) that took place outside the scanner following the fMRI scan (on the same day). All subjects underwent a short training session of 2 minutes prior to the scan on a different set of stimuli. All subjects had normal or corrected-to-normal vision. The experiments were undertaken with the understanding and written informed consent of each subject to participate in the fMRI experiments. The Tel Aviv Sourasky Medical Center approved the experimental protocol.</p>
      </sec>
      <sec>
        <title>Magnetic resonance imaging setup and acquisition</title>
        <p>Scanning was done on a Siemens MAGNETOM Trio A Tim System 3T scanner, equipped with a standard Siemens head coil, at the Helen and Norman Asher Center for Human Brain Imaging in the Weizmann Institute of Science. Blood oxygenation level-dependent (BOLD) contrast was obtained with gradient-echo echo-planar imaging (EPI) sequence. The time repetition (TR) used was 3000 ms, echo time (TE) = 30 ms, flip angle = 90°, field of view = 240 × 240 mm<sup>2</sup>, matrix size = 80 × 80, the scanned volume consisted of 46 nearly axial slices of 3-mm thickness (no gap) with an in plane resolution of 3 × 3 mm<sup>2</sup>, covering the entire cortex. A whole-brain T1 magnetization prepared rapid acquisition gradient echo (MPRAGE) sequence was acquired on each subject to allow accurate cortical segmentation, reconstruction, and volume-based statistical analysis. Here the TR was 2300 ms, TE = 2.98 ms, flip angle = 9°, field of view = 256 × 256 mm<sup>2</sup> and the scanned volume consisted of 160 slices of 1 mm thickness with 0.5 mm gap, and in plane resolution of 1 × 1 mm<sup>2</sup>. When this detailed anatomical scan was not performed next to the functional experiment, a shorter and less detailed T1 MPRAGE (96 slices of 2 mm thickness and 1 mm gap, TE = 2.67 ms and the rest of parameters identical) was performed and was automatically (Brain Voyager QX) coregistered to the more detailed anatomical scan. Button presses were recorded during the fMRI experiments via a response box (Current Designs MR safe fiber optic response pad, bimanual 8 model) from which we analyzed the behavior performance. Due to technical reasons in one subject we failed to collect behavior data during the inverted faces fMRI experiment.</p>
      </sec>
      <sec>
        <title>Stimuli</title>
        <p>Stimuli were generated on a PC, projected via an LCD projector (Epson PowerLite 74c) onto a screen positioned at the back end of the MRI tunnel, and viewed through a tilted mirror, positioned over the subject's forehead. Stimuli were based on 78 original different color photographs of male faces taken from 2 databases (mainly CVL Face Database (<ext-link ext-link-type="uri" xlink:href="http://www.lrv.fri.uni-lj.si/facedb.html">http://www.lrv.fri.uni-lj.si/facedb.html</ext-link>) and also AR Face Database (<xref rid="bib40" ref-type="bibr">Martinez and Benavente, 1998</xref>)). Frontal images were chosen of mostly Caucasian, with neutral expression, mouth closed, no facial hair, and no glasses. The original images were then processed using Adobe Photoshop 6 in the following manner: rotated to upright, such that the line connecting the eyes was horizontal; aligned to each other by rescaling to a common face size and location and then aligned by the middle vertical line crossing the nose of each face and by the horizontal line that passes below the eyes; the background was set to black, and the neck was cropped naturally (as if wearing high-neck black shirt); hairstyle was set above the ears and images were cropped to 600 × 600 pixels (10° × 10°).</p>
        <p>The morphing of the original images was done using MorphMan 4.0 (STOIK Imaging, Moscow, Russia). The morphing was done in sets of 13 different original images, where one face (source face) was morphed to the other 12 different faces (target faces) (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>). The main alignment features for the morph included hairline, lips, nose, eyes, eyebrows, and the external contour.</p>
        <p>A black image (matching the black background of the face stimuli) was used during the fixation periods. A red fixation dot of 4 × 4 pixels (0.07° × 0.07°) appeared at the center of the screen throughout the experiment.</p>
      </sec>
      <sec>
        <title>Block design upright and inverted morph fMRI experiments</title>
        <p>The experiment lasted 480 s and included 4 conditions: identical, 1/3 morph, 2/3 morph, and different. Each condition was repeated 6 times in a controlled and counterbalanced block design paradigm. Each block lasted 12 s, with interleaving 6-s fixations between blocks. The first and last fixations lasted 21 and 15 s, respectively. A block consisted of 12 different stimuli; each stimulus was presented for 1000 ms.</p>
        <p>Consecutive images were slightly shifted in an equally balanced manner (across conditions, across Euclidean distance) in all conditions to avoid motion cue confounds and to eliminate tactics of retinal differences. All 11 translations within a block were equal in size, following a translation path along 12 symmetrical points (with regard to the <italic>x</italic> axis and <italic>y</italic> axis) in a 2D square (maximum size 0.57° × 0.57°). Average translation between 2 consecutive images over all translations in the experiment was 0.24° (minimum 0.15°, maximum 0.32°). The average Euclidean distance of the shifted images was as follows—identical: 23.60, 1/3 morph: 22.80, 2/3 morph: 21.54, and different: 26.23. Average Euclidean distance in a block across the whole experiment was 23.49 (see details below). Subjects' task was to fixate and respond via a response box whether a face image was the same face image (should report “same”) or a different face image (should report “different”) than the previous one.</p>
        <p>To allow for a 1-back recognition task, in the identical condition, we introduced occasionally a matching 1/3 morph image (of the 6 identical blocks, 2 blocks contained no 1/3 morph image, 2 blocks contained one 1/3 morph image [derived from the block's identical face], and 2 blocks contained two 1/3 morph images), whereas in the other conditions, occasional repetitions were inserted (in each of the 1/3 morph, 2/3 morph, and different conditions, of the 6 blocks per condition, 2 contained no repetitions, 2 contained 1 repetition, and 2 blocks contained 2 repetitions). The experiment was run in 2 versions such that each subject did one of the versions for the upright experiment, and the other version for the inverted experiment. For each of the experiments (upright, inverted) the versions were counterbalanced across subjects. Each version consisted of 222 different male facial images made of 78 (6 sets × 13 faces per set) different original faces (of different men) and 144 (6 sets × 12 target faces per set × 2 morph levels [1/3 and 2/3]) morphed images. The difference between the versions was that in each version the source face for the morph of each set was a different face (out of the same 13 faces that composed the set). Therefore only the morphed images were different between the two versions. See Stimuli subsection for further details. Importantly, for a specific subject the morphed images were different in the upright and the inverted experiments since each subject did a different version for the upright and the inverted experiments.</p>
        <p>The inverted morph experiment was identical to the upright morph experiment in all aspects except for the faces being inverted upside-down.</p>
        <p>For each pair of images (K, J) in a block, for each color channel (R, G, B) separately, <italic>Euclidean distance</italic> was defined by <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msqrt><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> with K, J measured in (0,255) units. The block distance for each channel was defined as the average distance across all pairs (K, J) in the block. The average Euclidean distance was defined as the average distance over all blocks and all the 3 RGB channels.</p>
      </sec>
      <sec>
        <title>Visual localizer fMRI experiment</title>
        <p>This experiment lasted 564 s and included 5 conditions: grayscale still photographs (500 × 500 pixels or 9° × 9° of visual angle) of upright faces, inverted faces, houses, common man-made objects, and geometric texture patterns. The inverted faces stimuli were created from the upright facial stimuli by a 180° rotation. The faces stimuli were different than those used in the morph experiments. Each condition was repeated 7 times in a controlled and counterbalanced block design paradigm. Each block lasted 9 s, with interleaving 6 s fixations between blocks. The first and last fixations lasted 12 and 15 s, respectively. A block consisted of 9 different stimuli; each stimulus was presented for 800 ms and was followed by a 200 ms fixation screen. A central white fixation point (0.07° × 0.07°) was present throughout the experiment. One or two repetitions of the same image occurred in each block. Subject's task was to report by a button press whether the presented stimulus was identical to the previous stimulus or not.</p>
      </sec>
      <sec>
        <title>Behavioral experiment</title>
        <p>This experiment was aimed at defining the profile of difference perception according to the morph levels (0–45% in 5% steps, and 100%). Events lasted 3 s: 1200 ms of fast image presentation (117 ms picture + 83 ms fixation, 6 times) + 1800 ms fixation. Each event consisted of 6 stimuli. On trials with non-identical stimuli (5%–45% and 100% morph levels), all 6 presented images were different. For identical trials (0% morph) all 6 images were identical. On each trial subjects were asked to report “different” if any change at all was noticed between the 6 briefly presented face images or “same”—if no change was noticed. The event-presentation setup was similar to the one in the rapid event-related fMRI experiment in our previous report (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>). The experiment included 12 conditions (11 morph levels and fixation), each repeated 12 times in a counterbalanced order. The first and last fixations lasted 2 s each, altogether lasting 436 s.</p>
      </sec>
      <sec>
        <title>Data analysis: fMRI</title>
        <p>fMRI data were analyzed with the BrainVoyager QX software package (R. Goebel, Brain Innovation, Maastricht, The Netherlands) and with complementary in-house software. The first 2 images of each functional scan were discarded. The functional images were superimposed on 2D anatomic images and incorporated into the 3D data sets through trilinear interpolation. The complete data set was transformed into Talairach space (<xref rid="bib51" ref-type="bibr">Talairach and Tournoux, 1988</xref>). Preprocessing of functional scans included 3D motion correction, slice scan time correction, linear trend removal, and filtering out of low frequencies up to 3 cycles per experiment. No spatial smoothing was applied to the data. For display purposes only, the cortical surface of a normalized Talairach brain was reconstructed from a 3D SPGR scan. The procedure included segmentation of the white matter using a grow-region function, the smooth covering of a sphere around the segmented region, and the expansion of the reconstructed white matter into the gray matter. The surface of each hemisphere was then unfolded, cut along the calcarine sulcus and additional predefined anatomical landmarks on the medial side, and flattened (as can be seen in the flattened cortical maps in <xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p>
      </sec>
      <sec>
        <title>Statistical analysis: fMRI</title>
        <p>The statistical analysis was based on the general linear model (<xref rid="bib18" ref-type="bibr">Friston et al., 1994</xref>). A standard hemodynamic response function (Two Gamma HRF function, 5 s to response peak, 15 s to undershoot peak) was applied to a boxcar predictor constructed for each experimental condition except fixation, and the model was independently fitted to the signal of each voxel. A coefficient was calculated for each predictor using a least squares algorithm.</p>
      </sec>
      <sec>
        <title>Regions of Interest: definition and analysis</title>
        <p>Regions of interest (ROI) were defined for each subject separately using the visual localizer experiment data with minimum cluster size of at least 6 contiguous functional voxels with <italic>p</italic> &lt; 0.05, uncorrected. Face-selective FFA and OFA regions were defined by upright faces &gt; houses contrast. FFA was determined as the face-selective region in the posterior aspect of the fusiform gyrus (see <xref rid="app1" ref-type="sec">Supplementary Material</xref> for alternative FFA definitions we have applied), and was defined for 11 of the 12 subjects. OFA was determined as the face-selective region residing in the lateral–occipital aspect of the cortex in the vicinity of the inferior occipital sulcus or gyrus (IOS or IOG respectively), and was defined in 11 of the 12 subjects. PPA and TOS place-selective regions were defined by houses &gt; upright faces contrast. PPA was determined as the house-selective region residing in the parahippocampal gyrus or the adjacent collateral sulcus, while the dorsal transverse occipital sulcus (TOS) was determined as the house-selective region in the occipito-parietal aspect of the cortex beyond retinotopic cortex in the vicinity of the transverse occipital sulcus. PPA and TOS were each defined in all the 12 subjects. Object-selective LO was defined by the objects &gt; textures contrast in the lateral–occipital aspect of the cortex in the vicinity of the inferior occipital sulcus or gyrus (IOS or IOG respectively) and was determined in all of the 12 subjects.</p>
        <p>In order to obtain the activation and adaptation profiles of the MRI experiments for each region of interest (e.g. <xref rid="fig2" ref-type="fig">Fig. 2</xref>B for FFA, and see also <xref rid="app1" ref-type="sec">Supplementary Material</xref>), the experimental time courses of activation were sampled from each subject's predefined ROI (see details above). These time courses are displayed in <xref rid="app1" ref-type="sec">Supplementary Fig. 3</xref>. For each time course the percent signal change (PSC; relative to the preceding fixation block) over two time points along a block (for both morph experiments the time points were 9 s (3rd TR) and 12 s (4th TR) after stimulus onset, for the localizer experiment 6 s and 9 s after stimulus onset) was averaged, then this was averaged across all the repetitions of a specific condition. For each condition, the average response profile was calculated as the average across all subjects. Additional ROI adaptation profiles that are based on the peak response for each condition in each of the subjects are provided in <xref rid="app1" ref-type="sec">Supplementary Fig. 2</xref> accompanied by corresponding statistical analysis.</p>
        <p>Since the extent of adaptation is a relative measure, in order to analyze the adaptation effects in a similar manner across subjects the PSC was normalized for each subject separately. This was done by dividing the PSC of each of the conditions in a specific experiment by the subject's maximal PSC over all the experiment's conditions. The normalized PSC presented in the Figures and text is the average of the normalized PSC over all the subjects. The statistical analysis that accompanies the normalized activation levels (ANOVA and post hoc tests) is based on individual subjects' normalized activation values. <xref rid="app1" ref-type="sec">Supplementary Figs. 2 and 3</xref> display the PSC (peak response histograms and full time courses) without the normalization procedure.</p>
      </sec>
      <sec>
        <title>Correlation analysis</title>
        <p>The relationship between the perceptual similarity of the morphed faces and the adaptation tuning width was quantitatively estimated for each ROI by plotting the two parameters in a scatter plot (<xref rid="fig2" ref-type="fig">Fig. 2</xref>C, <xref rid="fig4" ref-type="fig">Fig. 4</xref>, <xref rid="app1" ref-type="sec">Supplementary Fig. 4</xref>). For each condition the normalized PSC response (<italic>y</italic>-axis) was plotted against the perceived difference (<italic>x</italic>-axis). Single subject data are presented for the FFA in <xref rid="app1" ref-type="sec">Supplementary Fig. 4</xref>. <xref rid="fig2" ref-type="fig">Fig. 2</xref>C data present average values of 2 sets of subjects (even or odd numbered subjects). <xref rid="fig4" ref-type="fig">Fig. 4</xref> displays correlation and significance values for each region of interest.</p>
      </sec>
      <sec>
        <title>Multi subject analysis—Localizer fMRI experiment</title>
        <p>In the multi-subject analysis, time courses of all subjects were converted into Talairach space and <italic>z</italic>-normalized. The multi-subject maps that are displayed in <xref rid="fig5" ref-type="fig">Fig. 5</xref> were obtained using a random effect (RE) procedure (<xref rid="bib19" ref-type="bibr">Friston et al., 1999</xref>). Statistical significance levels were calculated taking into account the individual voxel significance, a minimum cluster size of 6 functional voxels, and the probability threshold of a false detection of any given cluster within the entire cortical surface (<xref rid="bib15" ref-type="bibr">Forman et al., 1995</xref>). This was achieved using a Monte Carlo simulation (AlphaSim by B. Douglas Ward, a software module in (<xref rid="bib8" ref-type="bibr">Cox, 1996</xref>)). For visualization purposes the maps were projected on a flattened Talairach normalized brain.</p>
      </sec>
      <sec>
        <title>Statistical analysis: Behavior</title>
        <p>In order to determine whether at a specific morph level there was a significant behavioral difference between upright and inverted faces, the subjects' discrimination results from both experiments were compared via a paired 2-tailed <italic>t</italic>-test. Significant results are displayed in <xref rid="fig2" ref-type="fig">Fig. 2</xref>A.</p>
      </sec>
    </sec>
    <sec id="sec1">
      <title>Results</title>
      <sec>
        <title>Behavioral tuning</title>
        <p>The fMR-adaptation experiment was run separately for upright and inverted faces, replicating the experimental paradigm we recently reported on (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>). Subjects' task was a one-back discrimination task in which they were required to report whether a face image was identical to or different from the previously presented face image via a response box (two alternative forced choice). Low-level motion cues and differences between subsequent images were controlled for (see Materials and methods for details). In addition, following the fMRI experimental scans, we measured the subjects' performance outside the scanner (behavioral experiment) with a similar experimental task to better quantify the perceptual difference detection levels along the morph axis for upright and for inverted faces.</p>
        <p>In each of the two behavioral experiments (upright and inverted) subjects observed very brief sequences of varying degrees of morphing (ranging from 0% to 100% morph) and had to report whether they noticed a difference (“different”) or not (“same”). These behavioral results are presented in <xref rid="fig2" ref-type="fig">Fig. 2</xref>A. The behavioral curve for the upright faces (presented in gray) confirmed the narrow behavioral tuning curve we reported on earlier (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>), where at 30% morph there was a significant perception of face difference (difference perception of 77% ± 6%), and at 35% morph there was already a difference detection rate of 90% ± 4%. Importantly, the behavioral tuning curve for the inverted faces (indicated in red) was significantly broader compared to the upright faces starting from low-intermediate morph levels of 25% morph (<italic>p</italic> &lt; 0.03, paired 2-tailed <italic>t</italic>-test). Note that this broader tuning showed a significant difference throughout all the intermediate morph levels and up to the 67% morph (<italic>p</italic> &lt; 0.05 for 25%–67% levels, with <italic>p</italic> &lt; 0.001 for 30%–45% levels, paired 2-tailed <italic>t</italic>-test), where the difference perception for the inverted faces reached a level of 90% correct. These differences were manifested in a steeper non linear increase in shape difference detection as a function of morph level during upright face presentation, as compared to a more gradual, monotonic increase for the inverted faces.</p>
      </sec>
      <sec>
        <title>Adaptation effects in the FFA</title>
        <p>A primary aim of this study was to find the relationship between the FFA's adaptation effects to upright and inverted faces and the behavioral sensitivity for face differences. The location of the FFA was determined for each subject separately (upright faces &gt; houses) using a separate localizer experiment. <xref rid="fig2" ref-type="fig">Fig. 2</xref>B depicts the adaptation profiles of the FFA to both upright (left; gray) and inverted faces (right; red). The activation profile of the FFA to the different categories in the localizer experiment can be seen on the top right of the panel. Upright faces showed a trend for somewhat higher activation compared to inverted faces (BOLD PSC, faces: 1.71 ± 0.1, inverted faces: 1.49 ± 0.1; see Supporting Material for further statistics on upright faces vs. inverted faces), while objects and houses activated this region to a much lesser extent. Activations to textures were close to baseline level (BOLD PSC, objects: 0.94 ± 0.14, houses: 0.54 ± 0.08, textures: 0.24 ± 0.13). As expected for the upright morphed faces experiment, the adaptation profile (left) was narrow and replicated our earlier results (<xref rid="bib21" ref-type="bibr">Gilaie-Dotan and Malach, 2007</xref>), where in the 1/3 morph condition (33% morph level) there was a full release from adaptation. This was manifested in a significant signal increase in the 1/3 morph condition, while there was no significant difference between the activation levels of the 1/3 morph condition and the different-faces condition (identical vs. 1/3 morph: <italic>p</italic> &lt; 0.001; 1/3 morph vs. different: <italic>p</italic> &gt; 0.93 (Bonferroni/Dunn <italic>post-hoc</italic>); see further details in <xref rid="app1" ref-type="sec">Supplementary Material</xref>).</p>
        <p>A significant correspondence could be seen when comparing the behavioral shape tuning for upright faces with the corresponding fMR-adaptation profile (gray profile in <xref rid="fig2" ref-type="fig">Figs. 2</xref>A and B).</p>
        <p>The critical issue however was whether FFA's adaptation profile for the inverted faces would follow the broader behavioral tuning profile (<xref rid="fig2" ref-type="fig">Fig. 2</xref>A red profile). As can be seen in <xref rid="fig2" ref-type="fig">Fig. 2</xref>B (right), the adaptation profile for the inverted faces was indeed broader than that of the upright faces. For the inverted faces we found a significant release from adaptation only in the 2/3 morph condition (Bonferroni/Dunn <italic>post-hoc</italic>: identical vs. 1/3 morph: <italic>p</italic> = 0.012 (N.S.); identical vs. 2/3 morph: <italic>p</italic> &lt; 0.003; see <xref rid="app1" ref-type="sec">Supplementary Material</xref> for further details), and unlike the upright faces, there was a significant difference between the level of activation to the 2/3 morph condition and the activation to the different-faces condition (2/3 morph vs. different: <italic>p</italic> &lt; 0.005 (Bonferroni/Dunn <italic>post-hoc</italic>), see further details in <xref rid="app1" ref-type="sec">Supplementary Material</xref>). Furthermore, when we directly compared the release from adaptation in the different morph conditions in the upright and inverted faces experiments (ANOVA with main factors: (i) face orientation, and (ii) morph level) we found that there was a significant interaction between face orientation and morph level (<italic>p</italic> &lt; 0.05, <italic>F</italic> &gt; 1), and <italic>post-hoc</italic> analysis revealed that for upright faces the release from adaptation was similar for the 1/3 morph and the different conditions (<italic>p</italic> &gt; 0.93) whereas for inverted faces the release from adaptation in these two conditions was significantly different (<italic>p</italic> &lt; 0.05).</p>
        <p>To examine the possibility that the differences in the adaptation profiles between the upright and inverted faces were somehow related to the use of upright faces in the ROI definition (FFA, defined by upright faces vs. houses) we additionally examined alternative definitions of the FFA that might reveal higher selectivity to inverted faces. These results are presented in <xref rid="app1" ref-type="sec">Supplementary Figs. 1A and 1B</xref>. It is clear that even when the FFA was determined by preference to inverted faces over houses (<xref rid="app1" ref-type="sec">Supplementary Fig. 1A</xref>) or over textures (<xref rid="app1" ref-type="sec">Supplementary Fig. 1B</xref>), the adaptation profiles were essentially unchanged (statistical analysis failed to reveal any significant differences between these profiles, <italic>p</italic> &gt; 0.17, see <xref rid="app1" ref-type="sec">Supplementary Material</xref> for more details).</p>
        <p>Thus, in the fusiform gyrus the neuronal population displayed a different adaptation tuning width for the two categories being processed (upright or inverted faces). These tuning properties appeared to nicely and consistently correspond to the behavioral shape sensitivity in that narrower behavioral sensitivity for upright faces was reflected in a narrower adaptation recovery profile.</p>
        <p>In order to quantitatively estimate the relationship between the perceived similarity of the morphed faces and the adaptation tuning width, we plotted the two parameters in a scatter plot. <xref rid="fig2" ref-type="fig">Fig. 2</xref>C depicts the results of this analysis (see <xref rid="app1" ref-type="sec">Supplementary Figure 4</xref> for same analysis based on single subject data). As can be seen in the Fig., the correlation between behavioral and fMR-adaptation selectivities revealed by this analysis was positive, strong and significant (Pearson correlation coefficient <italic>R</italic> = 0.8245, coefficient of determination <italic>R</italic><sup>2</sup> = 0.6799 with <italic>p</italic> &lt; 0.0001, non-directional). This analysis reinforces the correspondence between the perceptual discrimination of the subjects and the fMRI adaptation effects in the FFA.</p>
      </sec>
      <sec>
        <title>Adaptation effects in additional regions</title>
        <p>In addition to the FFA we examined the profiles of adaptation in additional predefined category-selective ROIs. As was done for the FFA, these regions were defined for each subject separately by the common functional definitions using the localizer experiment (see Materials and methods for further details). The borders of these regions, based on multi-subject analysis, are indicated by different colors in <xref rid="fig5" ref-type="fig">Fig. 5</xref> for orientation purposes. The fMRI adaptation profiles of these regions, based on individual subject sampling, are shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref> (see <xref rid="app1" ref-type="sec">Supplementary Fig. 2</xref> and <xref rid="app1" ref-type="sec">Supplementary Fig. 3</xref> for adaptation profiles based on peak response and full time courses).</p>
        <p>As can be seen in these figures, the face-selective occipital face area (OFA) displayed a pattern of adaptation that resembled that of the FFA (narrow tuning to upright faces while broader tuning to inverted faces, see <xref rid="fig3" ref-type="fig">Fig. 3</xref>A), albeit more noisy and not reaching significance (no main effects or interactions were found, upright faces: hemisphere: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.93, condition: <italic>F</italic> = 2.46, <italic>p</italic> = 0.096, interaction: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.72; inverted faces: hemisphere: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.73, condition: <italic>F</italic> = 2.961, <italic>p</italic> = 0.059, interaction: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.54).</p>
        <p>Interestingly, in the object related lateral occipital (LO) area (defined by objects &gt; textures) we found significantly narrow tuning to both upright and inverted faces, which for inverted faces was narrower than the discrimination performance (<xref rid="fig3" ref-type="fig">Fig. 3</xref>B). ANOVA analysis results for upright faces revealed significant effect for condition (<italic>F</italic> &gt; 1, <italic>p</italic> &lt; 0.01), and no other effects (hemisphere: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.81, interaction: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.47). <italic>Post-hoc</italic> analysis revealed narrow tuning to upright faces (Bonferroni/Dunn: upright identical-1/3morph: <italic>p</italic> &lt; 0.002, 1/3morph not significantly different from 2/3morph (<italic>p</italic> &gt; 0.17) or from different (<italic>p</italic> &gt; 0.67)). For inverted faces we found a condition effect (<italic>F</italic> &gt; 1, <italic>p</italic> &lt; 0.003), no hemisphere effect (<italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.92), and no interaction (<italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.79). <italic>Post-hoc</italic> Bonferroni/Dunn: significant identical-1/3morph (<italic>p</italic> &lt; 0.008), 1/3morph-2/3morph not significant (<italic>p</italic> &gt; 0.74), and 1/3morph-different not significant (<italic>p</italic> &gt; 0.29).</p>
        <p>In the place-related regions, both ventrally in the parahippocampal place area (PPA, <xref rid="fig3" ref-type="fig">Fig. 3</xref>C) and dorsally in the transverse occipital sulcus (TOS) area (<xref rid="fig3" ref-type="fig">Fig. 3</xref>D) we found broad tuning to inverted faces, while for upright faces we found no significant difference between the identical and different face conditions. In the PPA for upright faces no effects were found (hemisphere: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.55, condition: <italic>F</italic> = 1.7, <italic>p</italic> &gt; 0.18, interaction: <italic>F</italic> = 1.02, <italic>p</italic> &gt; 0.39). Further analysis comparing strictly identical and different responses (1-tailed paired <italic>t</italic>-test) revealed a borderline difference (<italic>p</italic> = 0.078).</p>
        <p>For inverted faces, only an effect of condition was found (<italic>F</italic> = 3.88, <italic>p</italic> &lt; 0.02; hemisphere: <italic>F</italic> = 4.07, <italic>p</italic> = 0.068, interaction: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.56). <italic>Post-hoc</italic> for identical-1/3morph was only significant according to Fisher's PLSD but not by Bonferroni/Dunn: <italic>p</italic> = 0.0312, N.S., identical-2/3morph: <italic>p</italic> &gt; 0.18, however identical-different was significant <italic>p</italic> &lt; 0.003. When directly comparing the identical and different conditions we found a significant adaptation effect for the inverted faces (<italic>p</italic> &lt; 0.0004, 1-tailed paired <italic>t</italic>-test).</p>
        <p>Thus, it seems that the PPA was more sensitive to inverted faces than to upright faces. Observing the response amplitude difference in the localizer data also seems to indicate preferential activation to inverted compared to upright faces.</p>
        <p>No effects were found for upright faces in the TOS (hemisphere: <italic>p</italic> &gt; 0.63, condition: <italic>p</italic> &gt; 0.57, interaction: <italic>p</italic> &gt; 0.13) even when comparing directly identical and different faces condition (<italic>p</italic> &gt; 0.31, 1-tailed paired <italic>t</italic>-test). For inverted faces a trend was found only for condition (<italic>F</italic> = 2.55, <italic>p</italic> = 0.072), hemisphere: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.85, interaction: <italic>F</italic> &lt; 1, <italic>p</italic> &gt; 0.81<xref rid="fn2" ref-type="fn">2</xref>. A direct comparison between identical and different conditions revealed a significant adaptation effect (<italic>p</italic> &lt; 0.008, 1-tailed paired <italic>t</italic>-test).</p>
        <p>A correlation analysis as the one performed for the FFA (<xref rid="fig2" ref-type="fig">Fig. 2</xref>C), was performed for OFA, LO, PPA, and TOS as well. A summary of the correlation values (between activity and behavior) is presented in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. Note that the FFA exhibited the strongest correlation to the behavior (<xref rid="bib59" ref-type="bibr">Yovel and Kanwisher, 2005</xref>).</p>
        <p>Thus, similar to what we found in the FFA, in these additional category-related regions we also observed that the same neuronal population had a different tuning width according to the category being processed.</p>
      </sec>
      <sec>
        <title>Differences in cortical activations to upright and inverted faces</title>
        <p>The localizer experiment that was conducted enabled us to also compare category-selective activations directly. This experiment included images of upright and inverted faces as well as man-made objects, houses and textures. The maps showing the main patterns of activations are presented in <xref rid="fig5" ref-type="fig">Fig. 5</xref>. The figure depicts the activation to the inverted and upright faces relative to the well known category-related regions in high order visual cortex (see panels A and B). More specifically, as can be seen in <xref rid="fig5" ref-type="fig">Fig. 5</xref>A, preferential activation to inverted faces relative to textures was evident in high-order visual cortex and included face-selective regions (FFA, OFA denoted by red borders), object-selective LO (denoted in blue) and parietal activations but did not extend into place-related regions (denoted in green). Similarly to inverted faces, upright faces (<xref rid="fig5" ref-type="fig">Fig. 5</xref>B) activated, in addition to face-related regions, object-related regions but not place-related regions. When observing the inverted faces' and upright faces' activation patterns compared to textures, it seems that they largely overlapped. Dissimilarities were observed in the more dorsal face-sensitive regions where upright faces activated OFA and STS to a greater extent, while inverted-faces activated parietal cortex to a greater extent.</p>
        <p>However, when contrasting directly the activation patterns to inverted and upright faces (<xref rid="fig5" ref-type="fig">Fig. 5</xref>C), we find a surprisingly prominent preferential activation to inverted faces in mid-level visual areas overlapping place-sensitive representations as well.</p>
      </sec>
    </sec>
    <sec>
      <title>Discussion</title>
      <sec>
        <title>The perceptual inversion effect is reflected in fMRI adaptation</title>
        <p>The main outcome of the present study is the observation that the behavioral face inversion effect (FIE), i.e. the reduced discrimination ability for inverted faces compared to upright ones was reflected in the tuning of adaptation effects in ventral stream high order visual areas, and particularly the fusiform gyrus (see <xref rid="fig2" ref-type="fig">Fig. 2</xref>). Specifically, we found that the fMRI adaptation effect was substantially more invariant to physical shape changes when these were imposed on inverted faces as opposed to upright faces (cf. <xref rid="fig2" ref-type="fig">Fig. 2</xref>B right and left panels). More generally, our results reveal that human shape discrimination ability as reflected in behavioral performance follows the tuning of the fMRI adaptation effects in high order visual areas.</p>
      </sec>
      <sec>
        <title>Can adaptation effects be informative about single unit activity?</title>
        <p>The aim of this study was to use adaptation effects in order obtain an indirect measure of the tuning of the underlying neuronal population. However, it is still not fully established to what extent can neuronal tuning be accurately deduced from such an indirect measure. Recent single unit studies of the neuronal adaptation in primate infero-temporal cortex, have supported the notion that adaptation effects can be observed at the single cell level (<xref rid="bib38 bib56" ref-type="bibr">Liu et al., 2009; Verhoef et al., 2008</xref>), and as expected, can be largely attributed to habituation or fatigue of neuronal inputs (<xref rid="bib43 bib49" ref-type="bibr">McMahon and Olson, 2007; Sawamura et al., 2006</xref>). Similar manipulations of repetition duration in both macaque IT single cells study and fMRI human FFA study provide further evidence for the link between BOLD and neuronal adaptation (<xref rid="bib11 bib38" ref-type="bibr">Fang et al., 2007; Liu et al., 2009</xref>). However under carefully selected conditions single unit adaptation effects have been shown to deviate from the fMR adaptation profile (<xref rid="bib43 bib49" ref-type="bibr">McMahon and Olson, 2007; Sawamura et al., 2006</xref>). Thus, because of its indirect measure, and its dependence on averaged population activity, it is not possible, at present, to deduce a precise quantitative estimate of the neuronal tuning width from the adaptation effects (a complementary difficulty is of course inherent in single unit recordings which sample a small subset of the neuronal population). Consequently even under ideal conditions, one should not expect a precise correspondence between behavioral performance and the fMR adaptation profile. On the other hand, so far the adaptation method has proven the most sensitive measure of human neuronal selectivity—for example, in consistently showing exemplar sensitivity in ventral stream areas. Concerning the present study, even disregarding precise quantitative identity between adaptation profile and behavioral performance, our results show a significant correspondence between the two phenomena which is sufficient to provide important constraints on possible models of neuronal representations (see further below).</p>
      </sec>
      <sec>
        <title>Task-related influences</title>
        <p>Could the adaptation effects reported here be related to the task—e.g. fatigue or reduced attention?</p>
        <p>Our fMRI adaptation measures were obtained while subjects performed a perceptual discrimination task. It should be noted that shape discrimination of inverted faces was a far more difficult task and thus likely required more attentional resources, yet we found <italic>less</italic> signal recovery in the inverted case.</p>
      </sec>
      <sec>
        <title>Model predictions vs. the observed findings</title>
        <p>The two extreme models presented in the introduction lead to contrasting experimental outcomes along two dimensions. One dimension is the relationship between perceptual discriminability and the sensitivity of single neurons. A straight forward prediction of the independent neuron coding model was that perceptual discriminability should correspond to the neuronal tuning selectivity. The second less obvious dimension, relates to magnitude of the neuronal representation engaged by each image—i.e. the number of responsive neurons. Note that in the independent neuron coding model, because the tuning width of neurons to the inverted faces is broader, it may paradoxically <italic>increase</italic> the number of activated neurons for the inverted face images and thus may compensate for lower activation of these neurons.</p>
        <p>Considering the alternative extreme model, i.e. a population coding scheme, here the two predictions are inverted. While for the first dimension we expect no change in the tuning width of individual neurons associated with reduced perceptual discrimination, the lower perceptual performance for inverted faces could be reflected in the number of neuronal “channels”—i.e. it could be associated with a reduction in the <italic>population size</italic> of neurons activated by the inverted face images (e.g. akin to the reduction of color channels in color blindness).</p>
        <p>Both aspects of the observed results, at least with regards to the FFA, are compatible with the independently tuned model:</p>
        <p>First, considering the amplitude of the BOLD signal it is interesting to note that apart from a trend in the FFA (see <xref rid="app1" ref-type="sec">Supplementary Material</xref> for further details), the fMRI response to inverted faces was of equal or even higher magnitude compared to upright faces (<xref rid="fig5" ref-type="fig">Fig. 5</xref>C). Similar results have been reported by a number of previous studies (<xref rid="bib2 bib10 bib28 bib33" ref-type="bibr">Aguirre et al., 1999; Epstein et al., 2006; Haxby et al., 1999; Kanwisher et al., 1998</xref>), but see (<xref rid="bib59" ref-type="bibr">Yovel and Kanwisher, 2005</xref>). These results illustrate that caution should be exercised when taking fMRI signal amplitude as a definitive marker of cortical specialization. As we have argued elsewhere (<xref rid="bib5 bib21" ref-type="bibr">Avidan et al., 2002b; Gilaie-Dotan and Malach, 2007</xref>) the fMRI signal is a product of both the activation amplitude of individual neurons as well as the <italic>number</italic> of neurons that are activated. An increase in amplitude of activation of individual neurons could, for example, explain the reported correlation between fMRI amplitude in the FFA and the face inversion effect (<xref rid="bib59" ref-type="bibr">Yovel and Kanwisher, 2005</xref>). However, the high fMRI signal in response to inverted faces may also be attributed to a larger number of neurons activated by each inverted face compared to an upright one.</p>
        <p>Second, the tuning width of neurons, as reflected in the adaptation effect was significantly broader for the inverted compared to the upright faces. These results are compatible with previous studies (e.g. (<xref rid="bib41 bib59" ref-type="bibr">Mazard et al., 2006; Yovel and Kanwisher, 2005</xref>)) showing higher adaptation for upright faces. In fact, while observing significant adaptation for upright faces, Yovel and Kanwisher did not find a significant adaptation for inverted faces. This accentuated difference in adaptation profile compared to the present study could be a result of differences in the experimental designs (e.g. block vs. event related design or different sets of face images).</p>
        <p>In additional relevant research, a study using morphed famous faces (<xref rid="bib48" ref-type="bibr">Rotshtein et al., 2004</xref>) has shown a tight correlation between the perceptual sensitivity to face identity changes and fMRI adaptation effects. A different study (<xref rid="bib16" ref-type="bibr">Fox et al., 2009</xref>) has shown that adaptation effects in face-sensitive regions follow the perceived change in facial expression or identity. Finally, <xref rid="bib57" ref-type="bibr">Welchman et al. (2005</xref>) have shown that perceived 3D shape changes from various cues are tightly linked to the BOLD adaptation effects in LOC and MT+. Together with the present results, all these studies are compatible with the notion that the adaptation profile in high-order visual cortex is coupled to the perceptual discrimination of the subjects.</p>
        <p>It appears then that despite the caveats discussed above regarding the indirect nature of the adaptation method, the most parsimonious interpretation of the present results is in favor of the independently tuned model over the population model. Thus, our results suggest that the behavioral discriminability of faces can be attributed to the independent shape tuning width of <italic>individual</italic> neurons rather than some inter-neuronal computation performed at the population level by coarsely tuned neurons.</p>
      </sec>
      <sec>
        <title>Extension of our results to other object categories</title>
        <p>Our results indicate that the independent-neurons coding principle is not unique to the highly familiar and specialized category of upright faces, but can be extended and generalized to unfamiliar visual categories such as inverted faces. Although inverted faces differ from common objects on many dimensions, the advantage of face inversion lies in the similar low-level composition of the upright and inverted faces, and consequently the ability to impose identical shape changes under conditions of drastically different perceptual discriminability. Furthermore, the similarity between the pattern of activation to inverted faces and the more general object-related activity (see <xref rid="fig5" ref-type="fig">Fig. 5</xref>) further supports the extension of the independent neurons coding scheme to more general object representations.</p>
        <p>If indeed inverted faces can be considered as a general object category, then our results suggest that the difference between the representation of highly familiar and behaviorally significant objects such as upright faces and those of unfamiliar shapes lies in the <italic>tuning width of individual neurons</italic> rather than in the basic coding scheme of the neuronal representation. Further studies using additional object categories are needed to fully confirm this conjecture. While single unit recordings in human object representations have been so far unavailable to directly verify this conclusion, it is interesting to note that exquisitely narrow shape tuning has been well documented for single units involved in shape representation in monkey IT (<xref rid="bib52" ref-type="bibr">Tanaka, 1996</xref>).</p>
        <p>It is important to emphasize at this point that positing the highly complex neuronal representation of objects in terms of two extreme alternatives is an oversimplification. For example, the models assume that the representation is “holistic”—i.e. the basic representational elements are entire faces or at least large and complex object fragments. However, certainly at various stages of these representations, local image elements, such as informative “fragments”, also play an important role (<xref rid="bib34" ref-type="bibr">Lerner et al., 2008</xref>). Similarly, recent evidence from single unit recordings in monkeys point to important contributions of certain face feature parameters such as iris size to the neuronal activation (<xref rid="bib17" ref-type="bibr">Freiwald et al., 2009</xref>). However, while not ruling out these aspects, the present results highlight that an important component of the representation, particularly at high levels of the hierarchy such as the FFA, is a holistic representation in which neurons are tightly tuned to specific faces.</p>
      </sec>
      <sec>
        <title>A neuron of many faces</title>
        <p>It may seem paradoxical how neurons can remain both sharply tuned in a holistic fashion to specific face templates yet at the same time belong to a distributed representation consisting of many, likely millions of neurons (<xref rid="bib37" ref-type="bibr">Levy et al., 2004b</xref>). A possible solution to such conundrum is a unique type of a receptive field, which can be metaphorically thought of as a “totem pole” of different face templates. In other words, each neuron serves as an “OR” function and shows sensitivity to a large collection of different templates. However, the neurons are also sharply tuned to each template, so any change or deformation of the template's shape leads to drastic reduction in the neuronal activation. It is important to note that under such a scheme, where each neuron represents a multitude of <italic>different</italic> object images, the firing of each neuron in isolation is completely ambiguous, hence devoid of much informational value. However, making the plausible assumption that neurons differ in the exact constellation of images they are sensitive to, then the integrated activity of a neuronal assembly sharing a single common object template can fully resolve such ambiguity and constitutes a well defined representation of this specific object. It is easy to demonstrate that such a scheme allows a truly vast combinatorial space in which all possible object images both familiar and unfamiliar can be easily accommodated (<xref rid="bib37" ref-type="bibr">Levy et al., 2004b</xref>) without the need of specific exemplar exposure or training.</p>
        <p>To conclude, our results suggest a model in which the shape discrimination of human object representations can be traced to the responses of individual neurons. However, such fine discrimination at the level of single neurons can be accommodated only by allowing each neuron to represent, in a holistic manner, numerous different object templates.</p>
      </sec>
      <sec>
        <title>Compatibility with face representation models</title>
        <p>The scheme we suggest for facial representations was focused on our ability to discriminate between different face or object images. However, it is important to clarify that the narrow tuning we report here for the shape of upright faces does not necessarily generalize to other image dimensions. For example, despite their narrow shape sensitivity, high order object representations display a relatively large invariance to changes in other image dimensions such as size, location, and contrast (<xref rid="bib4" ref-type="bibr">Avidan et al., 2002a</xref>).</p>
        <p>Another interesting dimension to consider is that of familiarity effects. Recent studies have introduced the concept of a “face space” (<xref rid="bib7 bib55" ref-type="bibr">Bruce et al., 1991; Valentine and Endo, 1992</xref>) whereby an important dimension in recognizing the identity of faces concerns their morph distance from a “norm” or an average face (<xref rid="bib61 bib62 bib60" ref-type="bibr">Leopold et al., 2001; Leopold et al., 2006; Rhodes and Jeffery, 2006</xref>). Note that in this formulation the movement from one face exemplar to another is orthogonal to the movement towards and away from the average face—and hence our study is not informative about this dimension. However, if human face representations indeed follow a face-space architecture, then our independent-neurons model leads to an interesting prediction: the adaptation profile should be much broader when morphing faces along the “identity” axis towards or away from the averaged face as compared to when morphing faces between different exemplars. A comprehensive, parametric study, in which faces will be morphed along various dimensions in face-space will be needed to fully clarify these significant issues.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aguirre</surname>
              <given-names>G.K.</given-names>
            </name>
            <name>
              <surname>Zarahn</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>D'Esposito</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>An area within human ventral cortex sensitive to “building” stimuli: evidence and implications</article-title>
          <source>Neuron</source>
          <volume>21</volume>
          <year>1998</year>
          <fpage>373</fpage>
          <lpage>383</lpage>
          <pub-id pub-id-type="pmid">9728918</pub-id>
        </element-citation>
      </ref>
      <ref id="bib2">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aguirre</surname>
              <given-names>G.K.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>D'Esposito</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Stimulus inversion and the responses of face and object-sensitive cortical areas</article-title>
          <source>NeuroReport</source>
          <volume>10</volume>
          <year>1999</year>
          <fpage>189</fpage>
          <lpage>194</lpage>
          <pub-id pub-id-type="pmid">10094160</pub-id>
        </element-citation>
      </ref>
      <ref id="bib3">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Allison</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ginter</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>McCarthy</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Nobre</surname>
              <given-names>A.C.</given-names>
            </name>
            <name>
              <surname>Puce</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Luby</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Spencer</surname>
              <given-names>D.D.</given-names>
            </name>
          </person-group>
          <article-title>Face recognition in human extrastriate cortex</article-title>
          <source>J. Neurophysiol.</source>
          <volume>71</volume>
          <year>1994</year>
          <fpage>821</fpage>
          <lpage>825</lpage>
          <pub-id pub-id-type="pmid">8176446</pub-id>
        </element-citation>
      </ref>
      <ref id="bib4">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Avidan</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Harel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hendler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ben-Bashat</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zohary</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Contrast sensitivity in human visual areas and its relationship to object recognition</article-title>
          <source>J. Neurophysiol.</source>
          <volume>87</volume>
          <year>2002</year>
          <fpage>3102</fpage>
          <lpage>3116</lpage>
          <pub-id pub-id-type="pmid">12037211</pub-id>
        </element-citation>
      </ref>
      <ref id="bib5">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Avidan</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Hendler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zohary</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of the neuronal selectivity underlying low fMRI signals</article-title>
          <source>Curr. Biol.</source>
          <volume>12</volume>
          <year>2002</year>
          <fpage>964</fpage>
          <lpage>972</lpage>
          <pub-id pub-id-type="pmid">12123569</pub-id>
        </element-citation>
      </ref>
      <ref id="bib6">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bitterman</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Mukamel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Fried</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Nelken</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Ultra-fine frequency tuning revealed in single neurons of human auditory cortex</article-title>
          <source>Nature</source>
          <volume>451</volume>
          <year>2008</year>
          <fpage>197</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="pmid">18185589</pub-id>
        </element-citation>
      </ref>
      <ref id="bib7">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruce</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Doyle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dench</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Burton</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Remembering facial configurations</article-title>
          <source>Cognition</source>
          <volume>38</volume>
          <year>1991</year>
          <fpage>109</fpage>
          <lpage>144</lpage>
          <pub-id pub-id-type="pmid">2049903</pub-id>
        </element-citation>
      </ref>
      <ref id="bib8">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cox</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title>
          <source>Comput. Biomed. Res.</source>
          <volume>29</volume>
          <year>1996</year>
          <fpage>162</fpage>
          <lpage>173</lpage>
          <pub-id pub-id-type="pmid">8812068</pub-id>
        </element-citation>
      </ref>
      <ref id="bib9">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Epstein</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>A cortical representation of the local visual environment</article-title>
          <source>Nature</source>
          <volume>392</volume>
          <year>1998</year>
          <fpage>598</fpage>
          <lpage>601</lpage>
          <pub-id pub-id-type="pmid">9560155</pub-id>
        </element-citation>
      </ref>
      <ref id="bib10">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Epstein</surname>
              <given-names>R.A.</given-names>
            </name>
            <name>
              <surname>Higgins</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Parker</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Aguirre</surname>
              <given-names>G.K.</given-names>
            </name>
            <name>
              <surname>Cooperman</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Cortical correlates of face and scene inversion: a comparison</article-title>
          <source>Neuropsychologia</source>
          <volume>44</volume>
          <year>2006</year>
          <fpage>1145</fpage>
          <lpage>1158</lpage>
          <pub-id pub-id-type="pmid">16303149</pub-id>
        </element-citation>
      </ref>
      <ref id="bib11">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Murray</surname>
              <given-names>S.O.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Duration-dependent FMRI adaptation and distributed viewer-centered face representation in human visual cortex</article-title>
          <source>Cereb. Cortex</source>
          <volume>17</volume>
          <year>2007</year>
          <fpage>1402</fpage>
          <lpage>1411</lpage>
          <pub-id pub-id-type="pmid">16905593</pub-id>
        </element-citation>
      </ref>
      <ref id="bib12">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Farah</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Is face recognition 'special'? Evidence from neuropsychology</article-title>
          <source>Behav. Brain. Res.</source>
          <volume>76</volume>
          <year>1996</year>
          <fpage>181</fpage>
          <lpage>189</lpage>
          <pub-id pub-id-type="pmid">8734052</pub-id>
        </element-citation>
      </ref>
      <ref id="bib13">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Farah</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Tanaka</surname>
              <given-names>J.W.</given-names>
            </name>
            <name>
              <surname>Drain</surname>
              <given-names>H.M.</given-names>
            </name>
          </person-group>
          <article-title>What causes the face inversion effect?</article-title>
          <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
          <volume>21</volume>
          <year>1995</year>
          <fpage>628</fpage>
          <lpage>634</lpage>
          <pub-id pub-id-type="pmid">7790837</pub-id>
        </element-citation>
      </ref>
      <ref id="bib14">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Farah</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>K.D.</given-names>
            </name>
            <name>
              <surname>Drain</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tanaka</surname>
              <given-names>J.N.</given-names>
            </name>
          </person-group>
          <article-title>What is “special” about face perception?</article-title>
          <source>Psychol. Rev.</source>
          <volume>105</volume>
          <year>1998</year>
          <fpage>482</fpage>
          <lpage>498</lpage>
          <pub-id pub-id-type="pmid">9697428</pub-id>
        </element-citation>
      </ref>
      <ref id="bib15">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Forman</surname>
              <given-names>S.D.</given-names>
            </name>
            <name>
              <surname>Cohen</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Fitzgerald</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Eddy</surname>
              <given-names>W.F.</given-names>
            </name>
            <name>
              <surname>Mintun</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Noll</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Improved assessment of significant activation in functional magnetic-resonance-imaging (Fmri)—use of a cluster-size threshold</article-title>
          <source>Magn.Reson.Med.</source>
          <volume>33</volume>
          <year>1995</year>
          <fpage>636</fpage>
          <lpage>647</lpage>
          <pub-id pub-id-type="pmid">7596267</pub-id>
        </element-citation>
      </ref>
      <ref id="bib16">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fox</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Moon</surname>
              <given-names>S.Y.</given-names>
            </name>
            <name>
              <surname>Iaria</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Barton</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>The correlates of subjective perception of identity and expression in the face network: an fMRI adaptation study</article-title>
          <source>NeuroImage</source>
          <volume>44</volume>
          <year>2009</year>
          <fpage>569</fpage>
          <lpage>580</lpage>
          <pub-id pub-id-type="pmid">18852053</pub-id>
        </element-citation>
      </ref>
      <ref id="bib17">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Freiwald</surname>
              <given-names>W.A.</given-names>
            </name>
            <name>
              <surname>Tsao</surname>
              <given-names>D.Y.</given-names>
            </name>
            <name>
              <surname>Livingstone</surname>
              <given-names>M.S.</given-names>
            </name>
          </person-group>
          <article-title>A face feature space in the macaque temporal lobe</article-title>
          <source>Nat. Neurosci.</source>
          <volume>12</volume>
          <year>2009</year>
          <fpage>1187</fpage>
          <lpage>1196</lpage>
          <pub-id pub-id-type="pmid">19668199</pub-id>
        </element-citation>
      </ref>
      <ref id="bib18">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Homes</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Frackwowiak</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>
          <source>Hum. Brain. Mapp.</source>
          <volume>2</volume>
          <year>1994</year>
          <fpage>189</fpage>
          <lpage>210</lpage>
        </element-citation>
      </ref>
      <ref id="bib19">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Buchel</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Worsley</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Multisubject fMRI studies and conjunction analyses</article-title>
          <source>NeuroImage</source>
          <volume>10</volume>
          <year>1999</year>
          <fpage>385</fpage>
          <lpage>396</lpage>
          <pub-id pub-id-type="pmid">10493897</pub-id>
        </element-citation>
      </ref>
      <ref id="bib20">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Georgopoulos</surname>
              <given-names>A.P.</given-names>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names>A.B.</given-names>
            </name>
            <name>
              <surname>Kettner</surname>
              <given-names>R.E.</given-names>
            </name>
          </person-group>
          <article-title>Neuronal population coding of movement direction</article-title>
          <source>Science</source>
          <volume>233</volume>
          <year>1986</year>
          <fpage>1416</fpage>
          <lpage>1419</lpage>
          <pub-id pub-id-type="pmid">3749885</pub-id>
        </element-citation>
      </ref>
      <ref id="bib21">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gilaie-Dotan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Sub-exemplar shape tuning in human face-related areas</article-title>
          <source>Cereb. Cortex.</source>
          <volume>17</volume>
          <year>2007</year>
          <fpage>325</fpage>
          <lpage>338</lpage>
          <pub-id pub-id-type="pmid">16525131</pub-id>
        </element-citation>
      </ref>
      <ref id="bib22">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Goffaux</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Rossion</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Face inversion disproportionately impairs the perception of vertical but not horizontal relations between features</article-title>
          <source>J.Exp. Psychol.:. Hum. Percept. Perform.</source>
          <volume>33</volume>
          <year>2007</year>
          <fpage>995</fpage>
          <lpage>1002</lpage>
          <pub-id pub-id-type="pmid">17683242</pub-id>
        </element-citation>
      </ref>
      <ref id="bib23">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grill-Spector</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>The neural basis of object perception</article-title>
          <source>Curr. Opin. Neurobiol.</source>
          <volume>13</volume>
          <year>2003</year>
          <fpage>159</fpage>
          <lpage>166</lpage>
          <pub-id pub-id-type="pmid">12744968</pub-id>
        </element-citation>
      </ref>
      <ref id="bib24">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grill-Spector</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>fMR-adaptation: a tool for studying the functional properties of human cortical neurons</article-title>
          <source>Acta. Psychol. (Amst).</source>
          <volume>107</volume>
          <year>2001</year>
          <fpage>293</fpage>
          <lpage>321</lpage>
          <pub-id pub-id-type="pmid">11388140</pub-id>
        </element-citation>
      </ref>
      <ref id="bib25">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grill-Spector</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kushnir</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Hendler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Edelman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Itzchak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>A sequence of object-processing stages revealed by fMRI in the human occipital lobe</article-title>
          <source>Hum. Brain. Mapp.</source>
          <volume>6</volume>
          <year>1998</year>
          <fpage>316</fpage>
          <lpage>328</lpage>
          <pub-id pub-id-type="pmid">9704268</pub-id>
        </element-citation>
      </ref>
      <ref id="bib26">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grill-Spector</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kourtzi</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>The lateral occipital complex and its role in object recognition</article-title>
          <source>Vision. Res.</source>
          <volume>41</volume>
          <year>2001</year>
          <fpage>1409</fpage>
          <lpage>1422</lpage>
          <pub-id pub-id-type="pmid">11322983</pub-id>
        </element-citation>
      </ref>
      <ref id="bib27">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Harel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Levy</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale mirror-symmetry organization of human occipito-temporal object areas</article-title>
          <source>Neuron</source>
          <volume>37</volume>
          <year>2003</year>
          <fpage>1027</fpage>
          <lpage>1041</lpage>
          <pub-id pub-id-type="pmid">12670430</pub-id>
        </element-citation>
      </ref>
      <ref id="bib28">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
            <name>
              <surname>Ungerleider</surname>
              <given-names>L.G.</given-names>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names>V.P.</given-names>
            </name>
            <name>
              <surname>Schouten</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Hoffman</surname>
              <given-names>E.A.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>The effect of face inversion on activity in human neural systems for face and object perception</article-title>
          <source>Neuron</source>
          <volume>22</volume>
          <year>1999</year>
          <fpage>189</fpage>
          <lpage>199</lpage>
          <pub-id pub-id-type="pmid">10027301</pub-id>
        </element-citation>
      </ref>
      <ref id="bib29">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ishai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ungerleider</surname>
              <given-names>L.G.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schouten</surname>
              <given-names>H.L.</given-names>
            </name>
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
          </person-group>
          <article-title>Distributed representation of objects in the human ventral visual pathway</article-title>
          <source>Proc. Natl. Acad. Sci. U. S. A.</source>
          <volume>96</volume>
          <year>1999</year>
          <fpage>9379</fpage>
          <lpage>9384</lpage>
          <pub-id pub-id-type="pmid">10430951</pub-id>
        </element-citation>
      </ref>
      <ref id="bib30">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jazayeri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Movshon</surname>
              <given-names>J.A.</given-names>
            </name>
          </person-group>
          <article-title>Optimal representation of sensory information by neural populations</article-title>
          <source>Nat. Neurosci.</source>
          <volume>9</volume>
          <year>2006</year>
          <fpage>690</fpage>
          <lpage>696</lpage>
          <pub-id pub-id-type="pmid">16617339</pub-id>
        </element-citation>
      </ref>
      <ref id="bib31">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Rosen</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zeffiro</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Vanmeter</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Blanz</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Riesenhuber</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of a shape-based model of human face discrimination using FMRI and behavioral techniques</article-title>
          <source>Neuron</source>
          <volume>50</volume>
          <year>2006</year>
          <fpage>159</fpage>
          <lpage>172</lpage>
          <pub-id pub-id-type="pmid">16600863</pub-id>
        </element-citation>
      </ref>
      <ref id="bib32">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>McDermott</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chun</surname>
              <given-names>M.M.</given-names>
            </name>
          </person-group>
          <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>
          <source>J. Neurosci.</source>
          <volume>17</volume>
          <year>1997</year>
          <fpage>4302</fpage>
          <lpage>4311</lpage>
          <pub-id pub-id-type="pmid">9151747</pub-id>
        </element-citation>
      </ref>
      <ref id="bib33">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Nakayama</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>The effect of face inversion on the human fusiform face area</article-title>
          <source>Cognition</source>
          <volume>68</volume>
          <year>1998</year>
          <fpage>B1</fpage>
          <lpage>11</lpage>
          <pub-id pub-id-type="pmid">9775518</pub-id>
        </element-citation>
      </ref>
      <ref id="bib61">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Leopold</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>O'Toole</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Vetter</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Blanz</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Prototype-referenced shape encoding revealed by high-level aftereffects</article-title>
          <source>Nat. Neurosci.</source>
          <volume>4</volume>
          <year>2001</year>
          <fpage>89</fpage>
          <lpage>94</lpage>
          <pub-id pub-id-type="pmid">11135650</pub-id>
        </element-citation>
      </ref>
      <ref id="bib62">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Leopold</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Bondar</surname>
              <given-names>I.V.</given-names>
            </name>
            <name>
              <surname>Giese</surname>
              <given-names>M.A.</given-names>
            </name>
          </person-group>
          <article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title>
          <source>Nature</source>
          <volume>442</volume>
          <year>2006</year>
          <fpage>572</fpage>
          <lpage>575</lpage>
          <pub-id pub-id-type="pmid">16862123</pub-id>
        </element-citation>
      </ref>
      <ref id="bib34">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lerner</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Epshtein</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Ullman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Class information predicts activation by object fragments in human object areas</article-title>
          <source>J. cogn. Neurosci.</source>
          <volume>20</volume>
          <year>2008</year>
          <fpage>1189</fpage>
          <lpage>1206</lpage>
          <pub-id pub-id-type="pmid">18284342</pub-id>
        </element-citation>
      </ref>
      <ref id="bib35">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Levy</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Avidan</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Hendler</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Center-periphery organization of human object areas</article-title>
          <source>Nat. Neurosci.</source>
          <volume>4</volume>
          <year>2001</year>
          <fpage>533</fpage>
          <lpage>539</lpage>
          <pub-id pub-id-type="pmid">11319563</pub-id>
        </element-citation>
      </ref>
      <ref id="bib36">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Levy</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Harel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Functional analysis of the periphery effect in human building related areas</article-title>
          <source>Hum. Brain. Mapp.</source>
          <volume>22</volume>
          <year>2004</year>
          <fpage>15</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="pmid">15083523</pub-id>
        </element-citation>
      </ref>
      <ref id="bib37">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Levy</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hasson</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>One picture is worth at least a million neurons</article-title>
          <source>Curr. Biol.</source>
          <volume>14</volume>
          <year>2004</year>
          <fpage>996</fpage>
          <lpage>1001</lpage>
          <pub-id pub-id-type="pmid">15182673</pub-id>
        </element-citation>
      </ref>
      <ref id="bib38">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Murray</surname>
              <given-names>S.O.</given-names>
            </name>
            <name>
              <surname>Jagadeesh</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Time course and stimulus dependence of repetition-induced response suppression in inferotemporal cortex</article-title>
          <source>J. Neurophysiol.</source>
          <volume>101</volume>
          <year>2009</year>
          <fpage>418</fpage>
          <lpage>436</lpage>
          <pub-id pub-id-type="pmid">18987118</pub-id>
        </element-citation>
      </ref>
      <ref id="bib39">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Reppas</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Benson</surname>
              <given-names>R.R.</given-names>
            </name>
            <name>
              <surname>Kwong</surname>
              <given-names>K.K.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kennedy</surname>
              <given-names>W.A.</given-names>
            </name>
            <name>
              <surname>Ledden</surname>
              <given-names>P.J.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Rosen</surname>
              <given-names>B.R.</given-names>
            </name>
            <name>
              <surname>Tootell</surname>
              <given-names>R.B.</given-names>
            </name>
          </person-group>
          <article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title>
          <source>Proc. Natl. Acad. Sci. U. S. A.</source>
          <volume>92</volume>
          <year>1995</year>
          <fpage>8135</fpage>
          <lpage>8139</lpage>
          <pub-id pub-id-type="pmid">7667258</pub-id>
        </element-citation>
      </ref>
      <ref id="bib40">
        <mixed-citation publication-type="other">Martinez, A., and Benavente, R. (1998). The AR Face Database. CVC Technical Report #24.</mixed-citation>
      </ref>
      <ref id="bib41">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mazard</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schiltz</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Rossion</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Recovery from adaptation to facial identity is larger for upright than inverted faces in the human occipito-temporal cortex</article-title>
          <source>Neuropsychologia</source>
          <volume>44</volume>
          <year>2006</year>
          <fpage>912</fpage>
          <lpage>922</lpage>
          <pub-id pub-id-type="pmid">16229867</pub-id>
        </element-citation>
      </ref>
      <ref id="bib42">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McKone</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Duchaine</surname>
              <given-names>B.C.</given-names>
            </name>
          </person-group>
          <article-title>Can generic expertise explain special processing for faces?</article-title>
          <source>Trends. Cogn. Sci.</source>
          <volume>11</volume>
          <year>2007</year>
          <fpage>8</fpage>
          <lpage>15</lpage>
          <pub-id pub-id-type="pmid">17129746</pub-id>
        </element-citation>
      </ref>
      <ref id="bib43">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McMahon</surname>
              <given-names>D.B.</given-names>
            </name>
            <name>
              <surname>Olson</surname>
              <given-names>C.R.</given-names>
            </name>
          </person-group>
          <article-title>Repetition suppression in monkey inferotemporal cortex: relation to behavioral priming</article-title>
          <source>J. Neurophysiol.</source>
          <volume>97</volume>
          <year>2007</year>
          <fpage>3532</fpage>
          <lpage>3543</lpage>
          <pub-id pub-id-type="pmid">17344370</pub-id>
        </element-citation>
      </ref>
      <ref id="bib44">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Murray</surname>
              <given-names>J.E.</given-names>
            </name>
            <name>
              <surname>Yong</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Rhodes</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Revisiting the perception of upside-down faces</article-title>
          <source>Psychol. Sci.</source>
          <volume>11</volume>
          <year>2000</year>
          <fpage>492</fpage>
          <lpage>496</lpage>
          <pub-id pub-id-type="pmid">11202495</pub-id>
        </element-citation>
      </ref>
      <ref id="bib45">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Puce</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Allison</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gore</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>McCarthy</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Face-sensitive regions in human extrastriate cortex studied by functional MRI</article-title>
          <source>J. Neurophysiol.</source>
          <volume>74</volume>
          <year>1995</year>
          <fpage>1192</fpage>
          <lpage>1199</lpage>
          <pub-id pub-id-type="pmid">7500143</pub-id>
        </element-citation>
      </ref>
      <ref id="bib60">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rhodes</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Jeffery</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Adaptive norm-based coding of facial identity</article-title>
          <source>Vision Res.</source>
          <volume>46</volume>
          <year>2006</year>
          <fpage>2977</fpage>
          <lpage>2987</lpage>
          <pub-id pub-id-type="pmid">16647736</pub-id>
        </element-citation>
      </ref>
      <ref id="bib46">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rhodes</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Brake</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Atkinson</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>What's lost in inverted faces?</article-title>
          <source>Cognition</source>
          <volume>47</volume>
          <year>1993</year>
          <fpage>25</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">8482070</pub-id>
        </element-citation>
      </ref>
      <ref id="bib47">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rossion</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Gauthier</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>How does the brain process upright and inverted faces?</article-title>
          <source>Behav. Cogn. Neurosci. Rev.</source>
          <volume>1</volume>
          <year>2002</year>
          <fpage>63</fpage>
          <lpage>75</lpage>
          <pub-id pub-id-type="pmid">17715586</pub-id>
        </element-citation>
      </ref>
      <ref id="bib48">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rotshtein</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Henson</surname>
              <given-names>R.N.</given-names>
            </name>
            <name>
              <surname>Treves</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Driver</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Morphing Marilyn into Maggie dissociates physical and identity face representations in the brain</article-title>
          <source>Nat. Neurosci.</source>
          <year>2004</year>
        </element-citation>
      </ref>
      <ref id="bib49">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sawamura</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Orban</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Vogels</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Selectivity of neuronal adaptation does not match response selectivity: a single-cell study of the FMRI adaptation paradigm</article-title>
          <source>Neuron</source>
          <volume>49</volume>
          <year>2006</year>
          <fpage>307</fpage>
          <lpage>318</lpage>
          <pub-id pub-id-type="pmid">16423703</pub-id>
        </element-citation>
      </ref>
      <ref id="bib50">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sereno</surname>
              <given-names>M.I.</given-names>
            </name>
            <name>
              <surname>Dale</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Reppas</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Kwong</surname>
              <given-names>K.K.</given-names>
            </name>
            <name>
              <surname>Belliveau</surname>
              <given-names>J.W.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Rosen</surname>
              <given-names>B.R.</given-names>
            </name>
            <name>
              <surname>Tootell</surname>
              <given-names>R.B.</given-names>
            </name>
          </person-group>
          <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>
          <source>Science</source>
          <volume>268</volume>
          <year>1995</year>
          <fpage>889</fpage>
          <lpage>893</lpage>
          <pub-id pub-id-type="pmid">7754376</pub-id>
        </element-citation>
      </ref>
      <ref id="bib51">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Talairach</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Tournoux</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <chapter-title>Co-Planar Stereotaxic Atlas of the Human Brain</chapter-title>
          <year>1988</year>
          <publisher-name>Thieme Medical Publishers</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib52">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tanaka</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Inferotemporal cortex and object vision</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <volume>19</volume>
          <year>1996</year>
          <fpage>109</fpage>
          <lpage>139</lpage>
          <pub-id pub-id-type="pmid">8833438</pub-id>
        </element-citation>
      </ref>
      <ref id="bib53">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tootell</surname>
              <given-names>R.B.</given-names>
            </name>
            <name>
              <surname>Dale</surname>
              <given-names>A.M.</given-names>
            </name>
            <name>
              <surname>Sereno</surname>
              <given-names>M.I.</given-names>
            </name>
            <name>
              <surname>Malach</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>New images from human visual cortex</article-title>
          <source>Trends. Neurosci.</source>
          <volume>19</volume>
          <year>1996</year>
          <fpage>481</fpage>
          <lpage>489</lpage>
          <pub-id pub-id-type="pmid">8931274</pub-id>
        </element-citation>
      </ref>
      <ref id="bib54">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Valentine</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Upside-down faces: a review of the effect of inversion upon face recognition</article-title>
          <source>Br. J. Psychol.</source>
          <volume>79</volume>
          <issue>Pt 4</issue>
          <year>1988</year>
          <fpage>471</fpage>
          <lpage>491</lpage>
          <pub-id pub-id-type="pmid">3061544</pub-id>
        </element-citation>
      </ref>
      <ref id="bib55">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Valentine</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Endo</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Towards an exemplar model of face processing: the effects of race and distinctiveness</article-title>
          <source>Q. J. Exp. Psychol. A.</source>
          <volume>44</volume>
          <year>1992</year>
          <fpage>671</fpage>
          <lpage>703</lpage>
          <pub-id pub-id-type="pmid">1615169</pub-id>
        </element-citation>
      </ref>
      <ref id="bib56">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Verhoef</surname>
              <given-names>B.E.</given-names>
            </name>
            <name>
              <surname>Kayaert</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Franko</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Vangeneugden</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Vogels</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Stimulus similarity-contingent neural adaptation can be time and cortical area dependent</article-title>
          <source>J. Neurosci.</source>
          <volume>28</volume>
          <year>2008</year>
          <fpage>10631</fpage>
          <lpage>10640</lpage>
          <pub-id pub-id-type="pmid">18923039</pub-id>
        </element-citation>
      </ref>
      <ref id="bib57">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Welchman</surname>
              <given-names>A.E.</given-names>
            </name>
            <name>
              <surname>Deubelius</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Conrad</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Bulthoff</surname>
              <given-names>H.H.</given-names>
            </name>
            <name>
              <surname>Kourtzi</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>3D shape perception from combined depth cues in human visual cortex</article-title>
          <source>Nat. Neurosci.</source>
          <volume>8</volume>
          <year>2005</year>
          <fpage>820</fpage>
          <lpage>827</lpage>
          <pub-id pub-id-type="pmid">15864303</pub-id>
        </element-citation>
      </ref>
      <ref id="bib58">
        <mixed-citation publication-type="other">Yin, R.K. (1969). Looking at upside-down faces. Looking at upside-down faces 81, 141-145.</mixed-citation>
      </ref>
      <ref id="bib59">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yovel</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>The neural basis of the behavioral face-inversion effect</article-title>
          <source>Curr. Biol.</source>
          <volume>15</volume>
          <year>2005</year>
          <fpage>2256</fpage>
          <lpage>2262</lpage>
          <pub-id pub-id-type="pmid">16360687</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="app1" sec-type="supplementary-material">
      <label>Appendix A</label>
      <title>Supplementary data</title>
      <p>
        <supplementary-material content-type="local-data" id="d30e1707">
          <caption>
            <title>Supplementary material</title>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
      </p>
    </sec>
    <ack>
      <title>Acknowledgments</title>
      <p>We would like to thank Smadar Ovadia for help with brain reconstruction. This study was supported by the Israel Science Foundation 160/07 and Minerva grants to Rafael Malach. We thank the Computer Vision Laboratory, Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia, and the Secondary School Centre, Velenje, Slovenia, for allowing us to use the CVL Face Database (<ext-link ext-link-type="uri" xlink:href="http://www.lrv.fri.uni-lj.si/facedb.html">http://www.lrv.fri.uni-lj.si/facedb.html</ext-link>).</p>
    </ack>
    <fn-group>
      <fn id="fn2">
        <label>2</label>
        <p>Although not significant, we did run the <italic>post-hoc</italic> analysis to test which differences contributed to the condition effect. The identical-1/3morph did not pass Bonferonni/Dunn post-hoc significance (<italic>p</italic> = 0.0265, N.S.), but was found significant by Fisher’s PLSD post-hoc test (<italic>p</italic> = 0.0265), where 1/3morph was not significantly different from complete release (1/3morph-2/3morph: <italic>p</italic> &gt; 0.57, 2/3morph-different: <italic>p</italic> &gt; 0.49).</p>
      </fn>
    </fn-group>
  </back>
  <floats-group>
    <fig id="fig1">
      <label>Fig. 1</label>
      <caption>
        <p>Shape tuning of individual neurons: optional models, experimental design, and model predictions. (A) Two different models for the role of individual neurons in determining the behavioral shape sensitivity of human observers. In the “independent-neurons code” model the behavioral shape sensitivity is already present at the level of individual neurons. Neurons can be either narrowly tuned (I) or broadly tuned (II), corresponding to the behavioral shape discrimination ability of the observer. Alternatively, in a “population code” (III) model the behavioral sensitivity of the observer is a product of the relationship between neurons whose individual selectivity is far coarser. The colored curves represent the shape tuning curve of individual neurons (firing rate on the <italic>Y</italic> axis, distance between objects on the <italic>X</italic> axis). (B) Experimental design of the morph experiment for upright faces (left) and inverted faces (right). Faces were morphed into several different face exemplars. The morph sequences are represented along the <italic>X</italic> axis. Faces at four stages along the morph sequence (identical, 1/3 morph, 2/3 morph and different) were used to build the four different conditions for the block design experiments. (C) Model predictions according to the experimental design laid out in (B). In a narrow independent-neurons model (I), a slight shift in face shape (1/3 morph, see (B)) would be sufficient to move the activation to a different set of neurons leading to a full release from adaptation. In the broad independent-neurons model (II) recovery from adaptation requires a larger shape change. Finally, in the population model (III) the neuronal tuning can be extremely broad requiring a change in object category (category tuning) to achieve recovery from adaptation.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Fig. 2</label>
      <caption>
        <p>Behavior and FFA's adaptation effects. (A) Behavioral face discrimination for upright and inverted faces. The figure compares the measured ability of subjects to detect differences between the morphed faces (level of morphing on the <italic>x</italic>-axis) for upright (gray) and inverted (red) faces. The results displayed are combined from the psychophysical experiments and behavior during the scanning (delineated by green contour, see <xref rid="sec1" ref-type="sec">Results</xref> and <xref rid="sec2" ref-type="sec">Materials and methods</xref> for more details). Asterisks–for a specific morph level–indicate a significant difference between the discrimination performance of upright and inverted faces at this morph level (small = <italic>p</italic> &lt; 0.05, large = <italic>p</italic> &lt; 0.001, paired 2-tailed <italic>t</italic>-test on <italic>n</italic> = 12 subjects). Error bars denote S.E.M. Note the higher discriminability of upright faces compared to inverted ones, reflecting the well known “face inversion effect”. (B) FFA's adaptation profiles for upright and inverted faces (<italic>n</italic> = 11). Adaptation levels in the FFA at different morph levels are depicted for upright (left, gray) and inverted (right, red) faces. Note that the adaptation to inverted faces shows less shape selectivity (greater shape invariance) manifested in significant adaptation effects even at the 2/3 morph level. No such adaptation was found for the upright faces (left panel, gray). Note the similarity between the adaptation profiles and the behavioral discriminability (A) which supports the independent-neurons model (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). Histogram on top right presents FFA's activation profile for the localizer experiment. Error bars denote S.E.M. Note the similar activation levels (as compared with the adaptation levels) to upright and inverted faces despite the significant differences in behavior. (C) Correlation between behavioral discrimination and fMRI adaptation in the FFA for upright and inverted faces. Each symbol specifies average perceived difference (<italic>X</italic> axis) vs. FFA's average normalized activation (<italic>Y</italic> axis). Gray (red) circles and triangles represent upright (inverted) faces data (over the odd or the even subjects respectively, <italic>n</italic> = 10). Regression line is denoted in black (its equation and the coefficient of determination (<italic>R</italic><sup>2</sup> = 0.6799 with <italic>p</italic> &lt; 0.0001, non-directional) are displayed on the right). Note that the same analysis for the single subject data yielded a highly significant correlation as well with <italic>R</italic><sup>2</sup> = 0.3649, <italic>p</italic> &lt; 0.001, non-directional (see <xref rid="app1" ref-type="sec">Supplementary Fig. 4</xref>).</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Fig. 3</label>
      <caption>
        <p>fMRI adaptation profiles of additional category selective regions. Histograms from the upright morphed faces, inverted morphed faces and localizer experiments (same presentation format as in <xref rid="fig2" ref-type="fig">Fig. 2</xref>B) for (A) face selective OFA, (B) object selective LO, (C) place selective PPA, (D) place selective TOS. See results for further details.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Fig. 4</label>
      <caption>
        <p>Correlation between behavioral discrimination and fMRI activation by region (based on the upright and inverted faces experiments' data together). Same analysis that was applied on FFA data in <xref rid="fig2" ref-type="fig">Fig. 2</xref>C, was applied here to additional ROIs. Correlation strength (<italic>R</italic><sup>2</sup>) is represented on the <italic>Y</italic> axis. Asterisks above each bar indicate the significance of the correlation. These results are based on data from 10 subjects for FFA and OFA and on data from 11 subjects for LO, PPA and TOS. Note that the FFA shows the strongest correlation to behavior performance.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="fig5">
      <label>Fig. 5</label>
      <caption>
        <p>Upright and inverted faces maps. Multi-subject analysis (Random effects (RE), <italic>N</italic> = 12 subjects) of the localizer experiment is presented both in inflated hemisphere format (A (top), posterior view) and in unfolded cortical format (A (bottom), B, C). (A) High order areas activated by inverted faces. Yellow to orange patches denote regions that were activated above baseline and showed significantly higher activation to inverted faces over textures (<italic>p</italic> &lt; 0.001 to <italic>p</italic> &lt; 10<sup>− 8</sup>, corrected, see scale bar at top right). Colored lines represent the borders of category selective regions. Face-selective regions (red) were defined by faces vs. houses, place-selective regions (green) by houses vs. faces, and object-selective regions (blue) by objects vs. textures. (B) High order areas activated by upright faces. Yellow to orange patches denote regions that were activated above baseline and showed significantly higher activation to upright faces over textures (presentation format and statistical thresholds as in (A)). (C) Cortical areas showing preferential activation to inverted compared to upright faces. Yellow to orange patches denote regions that were activated above baseline and showed significantly higher activation to inverted faces over upright faces (presentation format and statistical thresholds as in (A)). Anatomical abbreviations: LH—left hemisphere, RH—right hemisphere, D—dorsal, V—ventral, P—posterior, A—anterior, FFA—fusiform face area, LO—lateral occipital, OFA—occipital face area, PPA—parahippocampal place area, TOS—transverse occipital sulcus, STS—superior temporal sulcus, IPS—intraparietal sulcus, LS—lateral sulcus (insula), CS—central sulcus, PreCS—precentral sulcus, IFS—inferior frontal sulcus. Note the extensive activation to inverted faces which includes most of the face-selective regions (red contour) and object-selective cortex (blue contour, cf. panel A and B) and extends dorsally. The preferential activation to inverted over upright faces was localized mainly to place and object-related cortex.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
  </floats-group>
</article>