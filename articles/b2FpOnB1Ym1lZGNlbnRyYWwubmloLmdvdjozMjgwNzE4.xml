<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Med Image Anal</journal-id>
      <journal-title-group>
        <journal-title>Medical Image Analysis</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1361-8415</issn>
      <issn pub-type="epub">1361-8423</issn>
      <publisher>
        <publisher-name>Elsevier</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3280718</article-id>
      <article-id pub-id-type="pmid">21624846</article-id>
      <article-id pub-id-type="publisher-id">MEDIMA601</article-id>
      <article-id pub-id-type="doi">10.1016/j.media.2011.05.006</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Color treatment in endoscopic image classification using multi-scale local color vector patterns</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Häfner</surname>
            <given-names>M.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Liedlgruber</surname>
            <given-names>M.</given-names>
          </name>
          <email>mliedl@cosy.sbg.ac.at</email>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="cor1" ref-type="corresp">⁎</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Uhl</surname>
            <given-names>A.</given-names>
          </name>
          <email>uhl@cosy.sbg.ac.at</email>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Vécsei</surname>
            <given-names>A.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Wrba</surname>
            <given-names>F.</given-names>
          </name>
          <xref rid="aff4" ref-type="aff">d</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>St. Elisabeth Hospital, Department for Internal Medicine, Vienna, Austria</aff>
      <aff id="aff2"><label>b</label>University of Salzburg, Department of Computer Sciences, 5020 Salzburg, Austria</aff>
      <aff id="aff3"><label>c</label>St. Anna Children’s Hospital, Endoscopy Unit, Vienna, Austria</aff>
      <aff id="aff4"><label>d</label>Medical University of Vienna, Department of Clinical Pathology, Vienna, Austria</aff>
      <author-notes>
        <corresp id="cor1"><label>⁎</label>Corresponding author. <email>mliedl@cosy.sbg.ac.at</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <month>1</month>
        <year>2012</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="ppub"/>. -->
      <pub-date pub-type="ppub">
        <month>1</month>
        <year>2012</year>
      </pub-date>
      <volume>16</volume>
      <issue>1</issue>
      <fpage>75</fpage>
      <lpage>86</lpage>
      <history>
        <date date-type="received">
          <day>18</day>
          <month>10</month>
          <year>2010</year>
        </date>
        <date date-type="rev-recd">
          <day>15</day>
          <month>4</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>5</day>
          <month>5</month>
          <year>2011</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2012 Elsevier B.V.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Elsevier B.V.</copyright-holder>
        <license>
          <license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract abstract-type="graphical">
        <sec>
          <title>Graphical abstract</title>
          <p>In this work we propose a novel multi-scale operator which is based on the full color information within an image. In order to evaluate the method, we extract features from endoscopic images using this operator and classify the images according to the respective class of polyps.<fig id="f0050" position="anchor"><graphic xlink:href="fx1"/></fig></p>
        </sec>
        <sec>
          <title>Highlights</title>
          <p>► Compared to other LBP-based operators LCVP uses all color information available, yet yielding a more compact descriptor for an image. ► LCVP is up to 7.5 times faster compared to other LBP-based methods evaluated. ► In terms of a classification of polyps the accuracy of LCVP differs insignificantly only from previously developed methods.</p>
        </sec>
      </abstract>
      <abstract>
        <p>In this work we propose a novel method to describe local texture properties within color images with the aim of automated classification of endoscopic images. In contrast to comparable Local Binary Patterns operator approaches, where the respective texture operator is almost always applied to each color channel separately, we construct a color vector field from an image. Based on this field the proposed operator computes the similarity between neighboring pixels. The resulting image descriptor is a compact 1D-histogram which we use for a classification using the <italic>k</italic>-nearest neighbors classifier.</p>
        <p>To show the usability of this operator we use it to classify magnification-endoscopic images according to the pit pattern classification scheme. Apart from that, we also show that compared to previously proposed operators we are not only able to get competitive classification results in our application scenario, but that the proposed operator is also able to outperform the other methods either in terms of speed, feature compactness, or both.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Classification</kwd>
        <kwd>Colon cancer</kwd>
        <kwd>Local binary patterns</kwd>
        <kwd>Multi-scale</kwd>
        <kwd>Color</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="s0005">
      <label>1</label>
      <title>Introduction</title>
      <p>Today, the third most common malignant disease in western countries is colon cancer. Therefore, a regular colon examination is recommended, especially for people at an age of 50 years and older. Currently the gold standard for colon examination is colonoscopy, which is performed by using a colonoscope. Modern colonoscopes are able to take pictures from inside the colon which allows to obtain images for a computer-assisted analysis with the goal of detecting tumorous lesions. To get highly detailed images a magnifying endoscope can be used (<xref rid="b0025" ref-type="bibr">Bruno, 2003</xref>). Such an endoscope represents a significant advance in colonoscopy as it provides images which are up to 150-fold magnified, thus uncovering the fine surface structure of the mucosa as well as small lesions.</p>
      <p>There already exists some previous work devoted to an automated cancer staging employing different colonoscopic imaging modalities or videos. For classic white-light endoscopy several studies have demonstrated that computer-based image analysis is capable of detecting colorectal polyps (<xref rid="b0095 b0155" ref-type="bibr">Karkanis, 2003; Maroulis et al., 2003</xref>) in endoscopic video frames to a certain extent and to perform a first assessment of the malignant potential of these polyps (<xref rid="b0110 b0100 b0085 b0080" ref-type="bibr">Krishnan et al., 1998; Karkanis et al., 2001; Iakovidis et al., 2005, 2006</xref>). Narrow-band-imaging (NBI) has been shown to facilitate discrimination between neoplastic and non-neoplastic polyps relying on features of the observed microvasculature to some extent (<xref rid="b0050 b0195 b0200" ref-type="bibr">Gross et al., 2009; Stehle et al., 2009; Tischendorf et al., 2010</xref>). Confocal laser endomicroscopic images have also been used to differentiate lesions into the categories neoplastic and benign (<xref rid="b0010 b0015" ref-type="bibr">André et al., 2009a,b</xref>) using a dense variant of the bag-of-visual-words features.</p>
      <p>While diagnostic accuracies of automated staging techniques employing the imaging modalities described so far range between 70% and 94%, classification accuracies ranging from 95% up to 99% depending on employed features and classification schemes (see e.g. <xref rid="b0125 b0060 b0065 b0055" ref-type="bibr">Kwitt and Uhl, 2007; Häfner et al., 2009a,b, 2010</xref>) have been achieved using high-magnification chromo-colonoscopy (where contrast enhancement is achieved by actual staining during colonoscopy).</p>
      <p>We specifically target at the classification of the latter type of imagery based on the pit pattern scheme. In our previous work in this field (<xref rid="b0125" ref-type="bibr">Kwitt and Uhl, 2007</xref>), for example, we used the Dual-Tree Complex Wavelet Transform (DT-CWT). While the DT-CWT allows to analyze content within an image in an approximately shift-invariant manner and is directionally selective, additional computational burden is introduced due to the necessity of applying the wavelet transform to an image multiple times.</p>
      <p>Another work based on Fourier-features has been presented in (<xref rid="b0055" ref-type="bibr">Häfner et al., 2010</xref>). In this work features are extracted from Fourier transformed images. To be able to focus on dominant information within the images the features are extracted from a certain number of frequency bands with configurable bandwidths only. Since however the set of optimal frequency bands is not known at beforehand (neither the ideal number of frequency bands nor the respective bandwidths) different feature selection techniques are compared to an evolutionary algorithm. While we achieve rather high classification accuracies using the evolutionary algorithm, the respective computational demand is very high.</p>
      <p>An ensemble classification system, combining a set of different previously developed methods for classification, has been applied in (<xref rid="b0070" ref-type="bibr">Häfner et al., 2009c</xref>). Compared to the single methods, the ensemble classifier delivered a superior performance regarding the classification accuracy. However, to be able to build a fast ensemble classifier system it is also important that the single methods have a low computational demand, in terms of feature extraction as well as for the classification of an image. In addition to these aspects with respect to computational performance issues, a high error diversity of the techniques combined in ensemble classifiers is required to significantly enhance results as compared to the single classifiers’ results. Besides the overall aim to develop techniques which are computationally efficient to be computed, we also focus on developing methods suited for inclusion in ensemble classifiers which rely on significantly different feature sets compared to earlier work (<xref rid="b0125 b0055" ref-type="bibr">Kwitt and Uhl, 2007; Häfner et al., 2010</xref>).</p>
      <p>In this work we propose a novel color texture operator, called the Local Color Vector Patterns operator (LCVP). This operator introduces a novel way to deal with color in Local Binary Patterns (LBP) based texture descriptors. In addition, LCVP offers a high compactness in terms of the features generated, leading to a fast classification. Another advantage of the compact texture descriptors is the small amount of time required to generate and classify them. We use the LCVP operator for an automated classification of visual data acquired by a magnifying colonoscope corresponding to different types of pit patterns. An overview of our system for endoscopic image classification is shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref>. After the acquisition of endoscopic images and collecting the respective histologic classification, features are extracted using LCVP and classified using the <italic>k</italic>-NN classifier (based on the leave-one-patient-out cross-validation protocol). In the final step, the class predictions of the classifier are compared against the ground truth, providing the classification accuracy of the system.</p>
      <p>In Section <xref rid="s0010" ref-type="sec">2</xref> we review the classification of pit patterns of the colonic mucosa. Section <xref rid="s0015" ref-type="sec">3</xref> summarizes related work on using color-based LBP methods. The Multi-scale Block LBP operator, which is the basis for this work, is reviewed in Section <xref rid="s0020" ref-type="sec">4</xref>. The proposed color texture operator is introduced in Section <xref rid="s0025" ref-type="sec">5</xref>, followed by the classification of the resulting features in Section <xref rid="s0050" ref-type="sec">6</xref>. Experimental results and configuration details of the classification system proposed are given in Section <xref rid="s0055" ref-type="sec">7</xref>, where we also compare the proposed scheme to other ways of dealing with color in LBP-based schemes. Section <xref rid="s0085" ref-type="sec">8</xref> concludes the paper.</p>
    </sec>
    <sec id="s0010">
      <label>2</label>
      <title>Pit pattern classification</title>
      <p>Polyps of the colon are a frequent finding and are usually divided into metaplastic, adenomatous, and malignant. As resection of all polyps is time-consuming, it is imperative that those polyps which warrant endoscopic resection can be distinguished: polypectomy of metaplastic lesions is unnecessary and removal of invasive cancer may be hazardous. For these reasons, assessing the malignant potential of lesions at the time of colonoscopy is important as this would allow to perform targeted biopsy. While such systems are still not standard-of-care, the aim of developing such automated polyp classification systems is to avoid random and, probably, unnecessary biopsies. Hence, such systems could potentially help to save time, lower the cost for a colonoscopy procedure, and reduce the risk of complications during the procedure.</p>
      <p>The most commonly used classification system to distinguish between non-neoplastic and neoplastic lesions in the colon is the pit pattern classification, originally reported by <xref rid="b0115" ref-type="bibr">Kudo et al. (1994)</xref>. This system allows to differentiate between normal mucosa, hyperplastic lesions (non-neoplastic), adenomas (a pre-malignant condition), and malignant cancer based on the visual pattern of the mucosal surface. Thus this classification scheme is a convenient tool to decide which lesions need not, which should, and which most likely can not be removed endoscopically. The mucosal pattern as seen after dye staining and by using magnification endoscopy shows a high agreement with the histopathologic diagnosis. Due to the visual nature of this classification it is also a convenient choice for an automated image classification.</p>
      <p>In this classification scheme exist five main types according to the mucosal surface of the colon, as illustrated in <xref rid="f0010" ref-type="fig">Fig. 2</xref>. Type III is divided into types III-S and III-L, designating the size of the pit structure. It has been suggested that type I and II pattern are characteristic of non-neoplastic lesions (benign and non-tumorous), type III and IV are found on adenomatous polyps, and type V are strongly suggestive of invasive carcinoma, thus highly indicative for cancer (<xref rid="b0120" ref-type="bibr">Kudo et al., 1996</xref>).</p>
      <p>Furthermore lesions of type I and II can be grouped into non-neoplastic lesions and types III–V can be grouped into neoplastic lesions. This allows a grouping of lesions into two classes, which is more relevant in clinical practice as indicated in a study by <xref rid="b0105" ref-type="bibr">Kato et al. (2006)</xref>.</p>
      <p>Using a magnifying colonoscope together with indigo carmine dye spraying, the mucosal crypt pattern on the surface of colonic lesions can be observed (<xref rid="b0120" ref-type="bibr">Kudo et al., 1996</xref>). Several studies found a good correlation between the mucosal pit pattern and the histological findings, where especially techniques using magnifying colonoscopes led to excellent results (<xref rid="b0105" ref-type="bibr">Kato et al., 2006</xref>).</p>
      <p>From <xref rid="f0010" ref-type="fig">Fig. 2</xref> we notice that pit pattern types I to IV can be characterized fairly well, whereas type V is a composition of unstructured pits. At a first glance this classification scheme seems to be straightforward and easy to be applied. But it needs some exercising to achieve fairly good results (<xref rid="b0075" ref-type="bibr">Hurlstone, 2002</xref>). Apart from that, similar to the reported inter-observer variability of NBI-based colonoscopy (<italic>κ</italic> ≈ 0.57 (<xref rid="b0175" ref-type="bibr">Rastogi et al., 2009</xref>), <italic>κ</italic> ≈  0.63 (<xref rid="b0030" ref-type="bibr">Chang et al., 2009</xref>), <italic>κ</italic> ≈ 0.69 (<xref rid="b0170" ref-type="bibr">Raghavendra et al., 2010</xref>)) inter-observer variability of magnification chromo-endoscopy in the interpretation of pit patterns of colonic lesions has been described (<italic>κ</italic> ≈ 0.56 (<xref rid="b0205" ref-type="bibr">Zanoni et al., 2007</xref>), <italic>κ</italic> ≈ 0.64 (<xref rid="b0030" ref-type="bibr">Chang et al., 2009</xref>)). This work aims at allowing computer-assisted pit pattern classification in order to enhance the quality of differential diagnosis.</p>
      <p>The topical staining used in chromo-endoscopy has the effect of visually enhancing mucosal crypt patterns or vascular features. Depending on the color dye applied, enhanced features appear in different colors such as red, violet, or blue (<xref rid="b0190" ref-type="bibr">Song et al., 2007</xref>). Hence, it is important – for the medical expert as well as for an automated polyp classification system – to have color images at hand.</p>
    </sec>
    <sec id="s0015">
      <label>3</label>
      <title>Related work on color in LBP operators</title>
      <p>The LBP operator, introduced by <xref rid="b0160" ref-type="bibr">Ojala et al. (1996)</xref>, has become a very popular technique to describe the local properties of texture. This operator and extensions to it have been used already in many different areas of research such as for example medical image processing, face recognition, and image retrieval – just to mention a few. However, the LBP operator is usually either applied to grayscale images, or, in case of color images, the most common way to deal with color is to combine color channels by concatenating LBP histograms from color channels, which have been processed separately using LBP (e.g. <xref rid="b0145" ref-type="bibr">Mäenpää and Pietikäinen, 2004</xref>).</p>
      <p>Another, more specific color-based LBP operator is the Opponent Color LBP operator (OCLBP) (<xref rid="b0150" ref-type="bibr">Mäenpää et al., 2002</xref>) which exploits the inter-channel relationships between color channels. For this purpose the LBP transform is computed for all possible pairs of color channels, thresholding the neighbors from one color channels to a pixel from a second channel. Assuming that the histogram is not quantized, this results in nine different combinations, each yielding a histogram containing 256 bins. Concatenating these histograms results in a total number of 2304 bins. This LBP variant has been used for example in (<xref rid="b0005" ref-type="bibr">Ameling et al., 2009</xref>) for texture-based polyp detection in endoscopic videos and in (<xref rid="b0165" ref-type="bibr">Pietikäinen et al., 2002</xref>) for color texture classification.</p>
      <p>The work presented in (<xref rid="b0145" ref-type="bibr">Mäenpää and Pietikäinen, 2004</xref>) is an extensive study, evaluating the performance of color-based LBP (concatenation of LBP histograms originating from different color channels) compared to color histograms and grayscale features in the context of image retrieval. In addition to the traditional techniques, color histogram concatenation and the OCLBP operator are evaluated. Similar experiments and comparisons can be found in (<xref rid="b0150 b0165" ref-type="bibr">Mäenpää et al., 2002; Pietikäinen et al., 2002</xref>).</p>
      <p>In (<xref rid="b0035" ref-type="bibr">Connah and Finlayson, 2006</xref>) joint LBP histograms have been proposed. In this work the LBP operator is again applied to each color channel separately. But instead of concatenating 1D histograms three-dimensional joint histograms are created to capture relationships between different color channels. By using uniform patterns (<xref rid="b0140" ref-type="bibr">Mäenpää et al., 2000</xref>), only binary patterns containing either zero or two bit transitions between 0 and 1 (and vice versa) are considered for the resulting histograms, resulting in a reduced number of 58 binary patterns. The remaining binary patterns are accumulated in an additional bin, hence a total of 59 bins is used. For three-dimensional histograms the final histograms used as features for classification contain 59<sup>3</sup> = 205,379 histogram bins. These features have been used for example in (<xref rid="b0135" ref-type="bibr">Mackiewicz et al., 2008</xref>) to segment endoscopic videos. Recent work aiming at the classification of magnification-endoscopic images is also based on joint LBP histograms (<xref rid="b0065" ref-type="bibr">Häfner et al., 2009b</xref>). However, we used two color channels only to lower the dimensionality of the features. To compensate for the eventual loss in discriminative information, we considered all possible patterns instead of the restricted set of uniform patterns which results in 256<sup>2</sup> = 65,536 histogram bins.</p>
      <p><xref rid="t0005" ref-type="table">Table 1</xref> provides a brief overview of the different methods found in the literature, which incorporate color information into LBP-based systems.</p>
    </sec>
    <sec id="s0020">
      <label>4</label>
      <title>Basis for the local color vector patterns operator</title>
      <p>Since the original LBP operator (<xref rid="b0160" ref-type="bibr">Ojala et al., 1996</xref>) is very sensitive to noise and our images suffer from noise, we base our work on the Multi-scale Block LBP operator (MB-LBP), which has been proposed recently (<xref rid="b0130" ref-type="bibr">Liao et al., 2007</xref>). To achieve a higher robustness the comparison between pixel values in LBP is replaced by a comparison of pixel block intensity averages, which are computed for each pixel (hence, the blocks overlap). This operator is applied to each color channel <italic>C</italic><sub><italic>a</italic></sub> of an image <italic>I</italic> separately (<italic>a</italic> = 1, 2, 3).</p>
      <p>This process is shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>a. The average intensity of the center block, which has a size of <italic>K</italic> × <italic>K</italic> pixels and is centered at the pixel position denoted by the white box, is compared against the intensity averages of the neighboring blocks (the center positions of these blocks are denoted by the dark gray boxes). The blue boxes surrounding the dark gray boxes denote the subwindows of pixels used to compute the respective intensity averages (in this example the blocks used are of size 3 × 3, hence <italic>K</italic> = 3).</p>
      <p>This is equivalent to a convolution of <italic>C</italic><sub><italic>a</italic></sub> with a square averaging filter kernel of size <italic>K</italic> × <italic>K</italic>. The result of the convolution is denoted by <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The intensity of a pixel is compared against the intensities of the neighboring pixels, as depicted in <xref rid="f0015" ref-type="fig">Fig. 3</xref>b. In this case the neighboring pixels are not necessarily the pixels adjacent to the center pixel. The reason for this is that the position of the neighboring pixels must coincide with the block centers shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>a since these pixels now contain the intensity values for the respective neighboring blocks. As a consequence the distance of the neighboring pixels used for intensity comparisons depends on the size of the averaging filter kernel used. The averaging used in this work is based on a whole-point symmetric extension which allows us to compute LBP values near and on the border of an image as well. For <italic>K</italic> = 1 the averaging has no effect and MB-LBP reduces to LBP.</p>
      <p>The thresholding result within a pre-processed color channel <inline-formula><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for a pixel located at (<italic>i</italic>, <italic>j</italic>) is computed as<disp-formula id="e0005"><label>(1)</label><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.35em"/><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⩾</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>x</italic><sub><italic>n</italic></sub> and <italic>y</italic><sub><italic>n</italic></sub> denote the position of the <italic>n</italic>th neighboring pixel (these are ordered in a clock-wise fashion, starting with the top left neighbor). Using <italic>T</italic><sub><italic>n</italic>,<italic>i</italic>,<italic>j</italic></sub>, the LBP number can be formulated as<disp-formula id="e0010"><label>(2)</label><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="italic">LBP</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext></mml:math></disp-formula>with <italic>N</italic> = 8 since we are using a 8-neighborhood. The computation of the LBP number for an example pixel neighborhood is illustrated in <xref rid="f0020" ref-type="fig">Fig. 4</xref>.</p>
      <p>As shown in <xref rid="f0025" ref-type="fig">Fig. 5</xref> the MB-LBP operator significantly reduces the influence of noise. Choosing a small value for <italic>K</italic> preserves more detail but the operator gets more sensitive to noise, whereas higher values for <italic>K</italic> lower the sensitivity to noise but also result in loss of detail. Hence, adjusting the parameter <italic>K</italic> allows to find a trade-off between noise and detail loss. In addition, multiple resolutions of an image can be created by using different values for <italic>K</italic>. Based on this fact, the multi-scale LCVP operator in Section <xref rid="s0045" ref-type="sec">5.4</xref> is introduced.</p>
    </sec>
    <sec id="s0025">
      <label>5</label>
      <title>The local color vector patterns operator (LCVP)</title>
      <p>In (<xref rid="b0065" ref-type="bibr">Häfner et al., 2009b</xref>) we combined MB-LBP with joint color LBP histograms to classify endoscopic images and exploit inter-channel relationships between different color channels (we refer to this method as joint color multi-scale block LBP (JC-MB-LBP) from now on). While the results of this method are superior compared to many of our previous approaches it has two major drawbacks. First, as already pointed out in Section <xref rid="s0015" ref-type="sec">3</xref>, we used a combination of two color channels resulting in histograms containing 256<sup>2</sup> bins. The result was a rather poor performance in terms of classification speed. Secondly, the choice of the color channels used is somewhat arbitrary. Therefore, we conducted experiments with all possible combinations of two channels to find the combination performing best in terms of the classification accuracy in (<xref rid="b0065" ref-type="bibr">Häfner et al., 2009b</xref>) (equivalent to a manual feature selection). This results in an additional computational burden. But this is necessary since the different possible combinations might yield considerably different classification accuracies. While we could have used all color channels available this would have resulted in histograms containing 256<sup>3</sup> bins which would have ended up in an even more time consuming classification.</p>
      <p>The computational burden of using OCLBP histograms (see Section <xref rid="s0015" ref-type="sec">3</xref>) for classification is definitely lower compared to JC-MB-LBP. But things get worse as soon as multiple image scales come into play when introducing the multi-scale variant of OCLBP. This is quite obvious since the time for histogram comparison during classification is then approximately multiplied by the number of resolution stages used.</p>
      <sec id="s0030">
        <label>5.1</label>
        <title>Motivation for the LCVP operator</title>
        <p>Since the JC-MB-LBP operator yielded very promising results already in (<xref rid="b0065" ref-type="bibr">Häfner et al., 2009b</xref>) we introduce the LCVP operator with the aim of coping with the main limitations of JC-MB-LBP: the computational demand in terms of the classification and the fact that not all color information available is used. Hence, our goal is to develop a texture descriptor which, compared to JC-MB-LBP, utilizes all color information available, while being more compact and computable in a smaller amount of time.</p>
        <p>It is out of question that the primary goal in medical image classification systems must be a high diagnostic accuracy. However, considering the fact that an endoscopic procedure should allow a real-time diagnosis (to enable the examiner to set an appropriate reaction like taking a biopsy or similar) it is also important to create algorithms steering into that direction, and thus to develop fast and efficient feature extraction and classification methods.</p>
      </sec>
      <sec id="s0035">
        <label>5.2</label>
        <title>Threshold-based LCVP operator</title>
        <p>The main difference between LCVP and other local color texture operators similar to or derived from LBP is the way we process pixels. As described before, most work published so far using LBP or some variant of LBP in the context of color processing is based on computing the LBP transform considering pixel intensities within single color channels only. Thus, each color channel is processed separately. Although OCLBP at least incorporates the relationship between pairs of channels there exists no LBP variant using the full color information of a pixel at once.</p>
        <p>LCVP by contrast treats each pixel as a 3D color vector, as illustrated in <xref rid="f0030" ref-type="fig">Fig. 6</xref>. This means that, in contrast to LBP, we are not able to use a simple thresholding of intensity values to get the binary sequence making up the LCVP number for a pixel. Our solution to this problem is to use functions measuring the similarity between color vectors. Along with a threshold such a function again produces a binary output. Possible choices for such similarity measures are (<xref rid="b0185" ref-type="bibr">Smolka and Venetsanopoulos, 2007</xref>)<disp-formula id="e0070"><label>(3)</label><alternatives><textual-form specific-use="jats-markup"><italic>F</italic><sub>1</sub>(<italic>v</italic><sub>1</sub>,<italic>v</italic><sub>2</sub>) = ‖<italic>v</italic><sub>1</sub> - <italic>v</italic><sub>2</sub>‖</textual-form><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">‖</mml:mo></mml:math></alternatives></disp-formula><disp-formula id="e0075"><label>(4)</label><alternatives><textual-form specific-use="jats-markup"><italic>F</italic><sub>2</sub>(<italic>v</italic><sub>1</sub>,<italic>v</italic><sub>2</sub>) = | ‖<italic>v</italic><sub>1</sub>‖ - ‖<italic>v</italic><sub>2</sub>‖ |</textual-form><mml:math id="M6" altimg="si6.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mspace width="0.35em"/><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mspace width="0.35em"/><mml:mo stretchy="false">|</mml:mo></mml:math></alternatives></disp-formula>where ∥·∥ denotes the length of a vector in Euclidean space. The measures <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> are illustrated in <xref rid="f0035" ref-type="fig">Fig. 7</xref>. <italic>F</italic><sub>1</sub> corresponds to the magnitude of the difference vector between two color vectors <italic>v</italic><sub>1</sub> and <italic>v</italic><sub>2</sub>, approximating the perceptual difference between two colors. <italic>F</italic><sub>2</sub> is the absolute difference between the lengths of <italic>v</italic><sub>1</sub> and <italic>v</italic><sub>2</sub>.</p>
        <p><xref rid="f0040" ref-type="fig">Fig. 8</xref> shows different 2D-color planes which are either transformed to the HSV color space or to the LAB color space. As we notice from this figure, in case of RGB images the length of a color vector influences the brightness (value) only, while the color tone (hue) is only dependent of the color vector direction. As a consequence, <italic>F</italic><sub>2</sub> approximately corresponds to the brightness difference between two color vectors. However, when converting an image to the LAB color space the length of a color vector reflects color changes (<italic>a</italic><sup>∗</sup> and <italic>b</italic><sup>∗</sup>) as well as brightness changes (<italic>L</italic><sup>∗</sup>).</p>
        <p>Similar to MB-LBP we first apply an averaging filter to each color channel <italic>C</italic><sub><italic>a</italic></sub> of an image <italic>I</italic> to obtain the pre-processed image <italic>I</italic><sup>∗</sup>. In analogy to Eq. <xref rid="e0005" ref-type="disp-formula">(1)</xref>, the thresholding for a color vector located at (<italic>i</italic>, <italic>j</italic>) within <italic>I</italic><sup>∗</sup> can be expressed as<disp-formula id="e0015"><label>(5)</label><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.35em"/><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>x</italic><sub><italic>n</italic></sub> and <italic>y</italic><sub><italic>n</italic></sub> denote the position of the <italic>n</italic>th neighboring color vector (ordered in a clock-wise fashion, starting at the top left color vector), <italic>F</italic><sub><italic>p</italic></sub>(·, ·) denotes a similarity measure as defined in Eqs. <xref rid="e0070 e0075" ref-type="disp-formula">(3) and (4)</xref>, and <italic>t</italic> is a threshold allowing to control the strictness of the operator with <italic>t</italic> &gt; 0.</p>
        <p>Using <inline-formula><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> the final LCVP number based on color vectors can be formulated as<disp-formula id="e0020"><label>(6)</label><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi mathvariant="italic">LCVP</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mtext>.</mml:mtext></mml:math></disp-formula>Similar to MB-LBP, the parameter <italic>K</italic> we used to pre-process our images (averaging filter), allows us to find a trade-off between the influence of noise and loss of information. In addition, by adjusting <italic>K</italic> we are able to investigate an image at different scales, which is the basis for the multi-scale LCVP operator introduced in Section <xref rid="s0045" ref-type="sec">5.4</xref>.</p>
        <p>The color texture descriptor for an image is obtained by creating a 1D histogram with 256 bins based on the LCVP values for the respective image.</p>
      </sec>
      <sec id="s0040">
        <label>5.3</label>
        <title>Avoiding thresholds</title>
        <p>Although the LCVP operator as described above already uses the entire color information available and results in a very compact descriptor, we still have the problem that we do not know the optimal choice for the threshold value <italic>t</italic> in Eq. <xref rid="e0015" ref-type="disp-formula">(5)</xref>.</p>
        <p>If we assume the color vectors having component values between 0 and 255, we quickly realize that the number of possible choices for <italic>t</italic> is rather big (between 0 and <inline-formula><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:mn>256</mml:mn><mml:msqrt><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> in case of <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub>).</p>
        <p>A simple way to obviate the need for a threshold is to compute the aggregated similarity between a color vector and its neighbors. For this purpose the similarity from one color vector to all other color vectors belonging to the neighborhood is computed and summed up. The motivation behind using the aggregated measure is to obtain a single value for each color vector within the neighborhood which reflects the overall similarity of a vector to the remaining vectors belonging to the neighborhood.</p>
        <p>As shown in <xref rid="f0045" ref-type="fig">Fig. 9</xref> this computation is performed for each color vector position (<italic>x</italic><sub><italic>l</italic></sub>, <italic>y</italic><sub><italic>l</italic></sub>) within the neighborhood and can be expressed more formally as<disp-formula id="e0025"><label>(7)</label><mml:math id="M11" altimg="si11.gif" overflow="scroll"><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">kl</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where (<italic>x</italic><sub><italic>k</italic></sub>, <italic>y</italic><sub><italic>k</italic></sub>) denotes the position of the <italic>k</italic>th color vector within the neighborhood with <italic>l</italic> = 1, … , 9 (numbered row-wise, starting from the top left color vector). <italic>δ</italic><sub><italic>kl</italic></sub> denotes the Kronecker delta defined as<disp-formula id="e0030"><label>(8)</label><mml:math id="M12" altimg="si12.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">kl</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.35em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>.</mml:mtext></mml:math></disp-formula>Using Eq. <xref rid="e0025" ref-type="disp-formula">(7)</xref> we can rewrite Eq. <xref rid="e0015" ref-type="disp-formula">(5)</xref> as<disp-formula id="e0035"><label>(9)</label><mml:math id="M13" altimg="si13.gif" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.35em"/><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⩾</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mtext>,</mml:mtext><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>x</italic><sub><italic>n</italic></sub> and <italic>y</italic><sub><italic>n</italic></sub> denote the position of the <italic>n</italic>-th neighboring color vector (ordered in a clock-wise fashion, starting at the top left color vector). Based on this equation we use Eq. <xref rid="e0020" ref-type="disp-formula">(6)</xref> to compute the LCVP numbers and, subsequently, the histogram for the LCVP transformed image.</p>
        <p>Based on Eq. <xref rid="e0025" ref-type="disp-formula">(7)</xref> we define the aggregated similarity measures <italic>A</italic><sup>(1)</sup> and <italic>A</italic><sup>(2)</sup> based on the similarity measures <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub>. While these measures do not depend on a threshold value, we must note that the computational demand is higher compared to the threshold-based versions. This is due to the additional computations introduced by Eq. <xref rid="e0025" ref-type="disp-formula">(7)</xref>.</p>
      </sec>
      <sec id="s0045">
        <label>5.4</label>
        <title>Multi-scale extension to LCVP (MS-LCVP)</title>
        <p>As already pointed out above, different values for the averaging filter kernel width <italic>K</italic> allow us to apply LCVP at different scales. To get a descriptor considering multiple scales of an image we extract 1D histograms with different choices for <italic>K</italic> and concatenate the resulting histograms. Hence, the final LCVP histogram <italic>H</italic> for an image is obtained by<disp-formula id="e0040"><label>(10)</label><alternatives><textual-form specific-use="jats-markup"><italic>H</italic> = <italic>H</italic><sub><italic>K</italic><sub>1</sub></sub><italic>⊕</italic>…<italic>⊕</italic><italic>H</italic><sub><italic>K</italic><sub><italic>S</italic></sub></sub>,</textual-form><mml:math id="M14" altimg="si14.gif" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>⊕</mml:mi><mml:mo>…</mml:mo><mml:mi>⊕</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext>,</mml:mtext></mml:math></alternatives></disp-formula>where <inline-formula><alternatives><textual-form specific-use="jats-markup"><italic>H</italic><sub><italic>K</italic><sub><italic>s</italic></sub></sub></textual-form><mml:math id="M15" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> denotes the histogram obtained by using LCVP with a filter kernel width of <italic>K</italic><sub><italic>s</italic></sub>, <italic>S</italic> is the number of scales used, and ⊕ denotes the histogram concatenation. Starting with a kernel width of <italic>K</italic><sub>1</sub> for the finest scale, the subsequent kernel widths <italic>K</italic><sub><italic>s</italic></sub> for coarser scales are computed as<disp-formula id="e0045"><label>(11)</label><alternatives><textual-form specific-use="jats-markup"><italic>K</italic><sub><italic>s</italic></sub> = <italic>K</italic><sub>1</sub> + 2·(<italic>s</italic> - 1) with 1 &lt; <italic>s</italic> ⩽ <italic>S</italic></textual-form><mml:math id="M16" altimg="si16.gif" overflow="scroll"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mspace width="2em"/><mml:mi mathvariant="normal">with</mml:mi><mml:mspace width="2em"/><mml:mn>1</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>s</mml:mi><mml:mo>⩽</mml:mo><mml:mi>S</mml:mi></mml:math></alternatives></disp-formula>where <italic>s</italic> denotes the scale the kernel width should be computed for. This corresponds to increasing the filter kernel width between two successive scales by 2.</p>
      </sec>
    </sec>
    <sec id="s0050">
      <label>6</label>
      <title>Classification</title>
      <p>To estimate the classification accuracy of our system we use leave-one-patient-out cross-validation (LOPO-CV). In this setup one image out of the database is considered as an unknown image. The remaining images are used to train the classifier (omitting those images which originate from the same patient as the image left out). The class of the unknown image is then predicted by the system. These steps (training and prediction) are repeated for each image, yielding an estimate of the overall classification accuracy.</p>
      <p>To actually classify an unknown image (not contained in the training set) we use the <italic>k</italic>-nearest-neighbor classifier (<italic>k</italic>-NN). This rather weak classifier has been chosen to emphasize more on quantifying the discriminative power of the features used. The distance between two histograms <italic>H</italic><sub>1</sub> and <italic>H</italic><sub>2</sub> is computed by using the discrete Bhattacharyya distance metric. We chose this metric since, compared to the histogram intersection, it already delivered slightly higher results in earlier work (<xref rid="b0065" ref-type="bibr">Häfner et al., 2009b</xref>). This distance metric is defined as<disp-formula id="e0050"><label>(12)</label><mml:math id="M17" altimg="si17.gif" overflow="scroll"><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mtext>,</mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:msqrt><mml:mtext>,</mml:mtext></mml:math></disp-formula>where <italic>B</italic> denotes the number of bins within the histograms, and <italic>H</italic><sub>1,<italic>i</italic></sub> and <italic>H</italic><sub>2,<italic>i</italic></sub> denote the values of the <italic>i</italic>th bin of <italic>H</italic><sub>1</sub> and <italic>H</italic><sub>2</sub>, respectively. Prior to computing the distance between two histograms each histogram is normalized such that the bins sum up to 1.</p>
    </sec>
    <sec id="s0055">
      <label>7</label>
      <title>Experiments</title>
      <sec id="s0060">
        <label>7.1</label>
        <title>Experimental setup</title>
        <p>The image database used throughout our experiments consists of 716 color images of size of 256 × 256 acquired between the years 2005 and 2009 at the Department of Gastroenterology and Hepatology (Medical University of Vienna) using a zoom-colonoscope (Olympus Evis Exera CF-Q160ZI/L) with a magnification factor of 150. To obtain the images 40 patients underwent colonoscopy.</p>
        <p>Lesions found during colonoscopy have been examined after application of dye-spraying with indigocarmine, as routinely performed in colonoscopy. Biopsies or mucosal resection have been performed in order to get a histopathological diagnosis. Biopsies have been taken from type I, II, and type V lesions, as those lesions need not to be removed or cannot be removed endoscopically. Type III and IV lesions have been removed endoscopically. Out of all acquired images, histopathological classification resulted in 198 non-neoplastic and 518 neoplastic cases. The detailed classification results, which are used as ground truth for our experiments, are shown in <xref rid="t0010" ref-type="table">Table 2</xref>. Since some patients appear in more than one class, the total sum of patients shown in <xref rid="t0010" ref-type="table">Table 2</xref> is slightly higher than 40.</p>
        <p>Since the choice of the <italic>k</italic>-value for the <italic>k</italic>-NN classifier may have an impact on the classification rates, we chose a set of values (leading to the best overall classification rates in most cases) and carried out the classification of all images for each of these values. We then computed the mean classification rates over the overall classification rates achieved using the different choices for <italic>k</italic>. In addition we computed the respective deviations of the results from these mean values (maximum absolute difference between the mean rates and each experimental result for the different choices for <italic>k</italic>). The set of <italic>k</italic>-values used is {3, 5, 7, 9}. The reason for choosing odd values only is to avoid draws during the classification. While such draws could be resolved by assigning an unknown image to the class with the highest a priori probability this would lead to biased results due to the rather high number of neoplastic images in our image database.</p>
        <p>In order to be able to compare the classification accuracy and runtime performance of the LCVP operator to other color-based LBP operators, we also conducted experiments using the OCLBP operator and JC-MB-LBP operator. To allow a comparison of the classification results, we use the feature extraction and classification methodologies, as described in the previous sections, for these operators too (single scale and multi-scale). To make a comparison in terms of runtime performance feasible, all operators have been implemented and compared using Java 1.6. The runtime measurements have been carried out on a machine equipped with an Intel Core i7 CPU at 2.66 GHz (single-threaded), running Linux.</p>
        <p>We also conducted all experiments in two different color spaces, namely the RGB color space and the CIELAB color space (<xref rid="b0045" ref-type="bibr">Gonzalez and Woods, 2002</xref>). The RGB color space has been chosen since the endoscopic images have been supplied to us in this color format. The CIELAB color space, on the other hand, is an interesting alternative to the RGB color space since it corresponds more to human perception. Apart from that, the relative perceptual distance between two colors in the CIELAB space can be approximated by computing the Euclidean distance between the according color vectors (<xref rid="b0090" ref-type="bibr">Jain, 1989</xref>), which perfectly corresponds to our similarity measure <italic>F</italic><sub>1</sub> (and therefore also to <italic>A</italic><sup>(1)</sup>). To investigate the overall importance of color information we also conducted experiments using JC-MB-LBP with grayscale images (1D-histograms instead of 2D-histograms).</p>
        <p>The single scale results for the different methods presented in the next section have been obtained using a filter kernel width of <italic>K</italic> = 7. While we also conducted experiments with other values, this choice almost always yielded the best overall classification results (in case of all methods investigated). It must be noted, however, that this value is of course image resolution dependent and most probably will differ for images with lower or higher resolutions.</p>
        <p>In case of multi-scale experiments <italic>K</italic><sub>1</sub> has been set to 5. Using three scales and Eq. <xref rid="e0045" ref-type="disp-formula">(11)</xref> we end up with the filter kernel widths 5, 7, and 9 for the different scales. Hence, besides using the scale used in the single scale experiments we also use one finer and one coarser scale. This set of filter widths has been used throughout all multi-scale experiments.</p>
        <p>It must be noted that we also apply an averaging filter to the images in case of the OCLBP operator. This way we are able to introduce different scales for the OCLBP operator the same way we do it in case of LCVP and JC-MB-LBP.</p>
        <p>Since the JC-MB-LBP operator uses two color channels only, we conducted experiments for all different pairs of color channels. Concerning the average classification accuracy computed over different choices for <italic>k</italic>, only the channel combination yielding the highest overall classification for the respective <italic>k</italic>-value is taken into consideration. In case of the RGB experiments this was always the combination RB (red and blue channel), while in case of the CIELAB color space either the color channel combination <italic>L</italic><sup>∗</sup><italic>a</italic><sup>∗</sup> or <italic>L</italic><sup>∗</sup><italic>b</italic><sup>∗</sup> yielded the highest classification accuracies.</p>
        <p>In order to be able to assess whether two different methods produce statistically significant differences in the results obtained we employ McNemar’s test (<xref rid="b0040" ref-type="bibr">Everitt, 1977</xref>). For two methods <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> this test statistic keeps track of the number of images which are misclassified by method <italic>M</italic><sub>1</sub> but classified correctly by method <italic>M</italic><sub>2</sub> (denoted by <italic>n</italic><sub>01</sub>) and vice versa (denoted by <italic>n</italic><sub>10</sub>). The test statistic, which is approximately Chi Square distributed (with one degree of freedom), is then computed as<disp-formula id="e0055"><label>(13)</label><mml:math id="M18" altimg="si18.gif" overflow="scroll"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mtext>.</mml:mtext></mml:math></disp-formula>From <italic>T</italic> the <italic>p</italic>-value can be computed as<disp-formula id="e0060"><label>(14)</label><mml:math id="M19" altimg="si19.gif" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="M20" altimg="si20.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>χ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the cumulative distribution function of the Chi Square distribution with one degree of freedom. The null-hypothesis <italic>H</italic><sub>0</sub> for McNemar’s test is that the outcomes of <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> lead to equal error rates. Given a fixed significance level <italic>α</italic>, there is evidence that the methods <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> produce significantly different results if <italic>p</italic> &lt; <italic>α</italic>. As a consequence we can reject the null-hypothesis <italic>H</italic><sub>0</sub>. Throughout this work we chose a significance level of <italic>α</italic> = 0.05. This implies that, if <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> are significantly different, there is a confidence level of 95% that the differences between the outcomes of the methods are not caused by random variation.</p>
      </sec>
      <sec id="s0065">
        <label>7.2</label>
        <title>Classification results</title>
        <p><xref rid="t0015 t0020" ref-type="table">Tables 3 and 4</xref> show the average classification results we achieved with the different methods (in case of the RGB and the CIELAB color space, respectively). The results represent the mean classification rates obtained by carrying out the experiments with different choices for the number of neighbors for the <italic>k</italic>-NN classifier. To get a picture of the result variations between the different choices for <italic>k</italic>, the tables also contain the observed deviations from the mean rates.</p>
        <p>Since we also aim at answering the question whether the LCVP operator generates results which are significantly different compared to the other LBP variants tested, we carry out McNemar’s test for method pairs only which include either LCVP <italic>A</italic><sup>(1)</sup> or LCVP <italic>A</italic><sup>(2)</sup>. Throughout <xref rid="t0015 t0020" ref-type="table">Tables 3 and 4</xref> a check mark (√) in column <bold>A</bold><sup>(<bold>1</bold>)</sup>(<bold>A</bold><sup>(<bold>2</bold>)</sup>) shows that the observed result differences between the method in the respective table row and LCVP <italic>A</italic><sup>(1)</sup> (LCVP <italic>A</italic><sup>(2)</sup>) are statistically significant according to McNemar’s test. If one of the LCVP variants delivers significantly higher (lower) classification results compared to the method in the respective row this is indicated by ‘+’ (‘−’). To obtain the results for the significance results we fixed the parameters for the methods to values which on average resulted in the best classification results (<italic>k</italic> = 7 for the <italic>k</italic>-NN classifier, an averaging filter kernel width of <italic>K</italic> = 7, <italic>K</italic><sub>1</sub> = 5 in case of multi-scale experiments, and for the method JC-MB-LBP the color channel combinations RB and <italic>L</italic><sup>∗</sup><italic>b</italic><sup>∗</sup> in case of RGB and CIELAB experiments, respectively).</p>
        <sec id="s0070">
          <label>7.2.1</label>
          <title>LCVP and LBP variants</title>
          <p>From <xref rid="t0015 t0020" ref-type="table">Tables 3 and 4</xref> we notice that in most cases there are no significant differences between the overall classification rates obtained by LCVP <italic>A</italic><sup>(1)</sup> and LCVP <italic>A</italic><sup>(2)</sup>. Only in the case of multi-scale experiments carried out in the CIELAB color space LCVP <italic>A</italic><sup>(2)</sup> delivers significantly higher overall classification rates (approximately 5.5 % higher compared to LCVP <italic>A</italic><sup>(1)</sup>). Comparison of single scale and multi-scale overall classification accuracies reveals that the LCVP operator benefits from multiple scales (up to 6.3% higher compared to single scale experiments – in case of CIELAB) as well as from switching to the CIELAB color space (approximately 5.4% higher compared to the results from the RGB experiments – in case of multiple scales). In all cases the classification performance gain is higher for LCVP <italic>A</italic><sup>(2)</sup> compared to LCVP <italic>A</italic><sup>(1)</sup>. The reason for LCVP <italic>A</italic><sup>(2)</sup> delivering higher results in case of the CIELAB color space compared to the RGB color space stems from the fact, that as already indicated earlier, LCVP <italic>A</italic><sup>(2)</sup> is better suited for CIELAB. While measuring the approximate brightness difference between two color vectors in case of RGB, in case of CIELAB LCVP <italic>A</italic><sup>(2)</sup> also measures color differences. We therefore recommend using the multi-scale extension with the CIELAB color space due to its superiority compared to the single scale operator and the RGB results.</p>
          <p>Compared to the OCLBP operator the LCVP operator is always able to achieve significantly higher classification rates. In contrast to the LCVP operator, the OCLBP operator always delivers roughly equal classification rates, no matter whether we switch from single scale to multiple scales or from the RGB color space to CIELAB color space.</p>
          <p>In case of the JC-MB-LBP operator the picture is quite different. When using a single scale only the color version of JC-MB-LBP always delivers significantly higher classification rates compared to both LCVP variants (up to 8.4% in case of the CIELAB experiments). However, when using multiple scales the superiority of JC-MB-LBP vanishes. As a result, in case of RGB experiments JC-MB-LBP delivers only an insignificantly higher overall classification rate compared to both LCVP variants. In case of CIELAB experiments at least LCVP <italic>A</italic><sup>(2)</sup> is able to deliver an approximately equal overall classification rate compared to JC-MB-LBP (again, the classification rate obtained with JC-MB-LBP is only insignificantly higher compared to LCVP <italic>A</italic><sup>(2)</sup>). When applied to grayscale images, the JC-MB-LBP operator delivers either equal or significantly lower classification rates compared to LCVP. Similar to the OCLBP operator, the JC-MB-LBP is not able to benefit from multiple scales or from switching from the RGB colorspace to the CIELAB color space. Although LCVP <italic>A</italic><sup>(2)</sup> approximately reflects brightness differences when applied in the RGB color space, the operator delivers higher classification accuracies compared to JC-MB-LBP when applied to grayscale images. This stems from the fact that the comparison between neighboring pixels is slightly more complex in case of LCVP <italic>A</italic><sup>(2)</sup>.</p>
          <p>Considering the rather low number of images available for our experiments (see <xref rid="t0010" ref-type="table">Table 2</xref>) we may also run into the problem of overfitting in case of operators relying on high-dimensional features (high number of bins used in our histograms). The situation gets even worse since we use the LOPO-CV which reduces the number of training samples available for each image by the total number of images available for the respective patient. As a consequence we might run into a problem commonly known as “curse of dimensionality” (<xref rid="b0020" ref-type="bibr">Bellman, 1961</xref>). This problem is caused by the fact that high-dimensional feature vectors are more likely to be sparsely distributed in the feature space if the pool of training images is not sufficiently large. Hence, especially the operators OCLBP and JC-MBLBP, both using more than 2000 bins (even in the single scale case), may suffer from the “curse of dimensionality”. This indicates that the results we achieved could get even better in practical use due to the higher number of training samples which would then be available.</p>
          <p>From <xref rid="t0015 t0020" ref-type="table">Tables 3 and 4</xref> we also notice that – compared to JC-MB-LBP based on color – the LCVP operator yields a rather low specificity. Experiments with a balanced training set have shown that this most probably is not due to the imbalance in the training set used. Hence, it is quite possible that the LCVP features extracted are not distinctive enough to capture the properties of the non-neoplastic images as good as JC-MB-LBP.</p>
        </sec>
        <sec id="s0075">
          <label>7.2.2</label>
          <title>Previously developed approaches</title>
          <p>To be able to compare the results of previously developed approaches (color-extended version of DT-CWT-based method in (<xref rid="b0125" ref-type="bibr">Kwitt and Uhl, 2007</xref>) and Fourier-based in (<xref rid="b0055" ref-type="bibr">Häfner et al., 2010</xref>)) to the results of the LCVP operator, we repeated the respective experiments on the image database used throughout this work. Compared to alternative feature extraction approaches developed earlier (color-extended version of DT-CWT-based method in (<xref rid="b0125" ref-type="bibr">Kwitt and Uhl, 2007</xref>) and Fourier-based in (<xref rid="b0055" ref-type="bibr">Häfner et al., 2010</xref>)) the overall classification rates yielded by color-based LBP schemes are highly competitive.</p>
          <p>While the Fourier-based method always outperforms all LBP and LCVP variants (the best result is obtained in case of CIELAB), at least in case of LCVP <italic>A</italic><sup>(2)</sup> applied in the CIELAB color space the classification rates are only insignificantly higher (85.0 ± 1.9% for LCVP compared to 87.4% in case of the Fourier features). It must be noted, however, that the Fourier-based method proposed in (<xref rid="b0055" ref-type="bibr">Häfner et al., 2010</xref>) uses a genetic algorithm for feature selection in order to choose the most suitable ring filters (due to the vast amount of such filters which may potentially be used). As a consequence, one explanation for the superiority of this method is the selection of optimal features, which however results in overfitting. Such an optimization is neither used in case of the LBP variants nor in the case of LCVP.</p>
          <p>The overall classification rate of the DT-CWT-based method is significantly higher in case of the RGB color space (86.7% compared to 79.6 ± 0.8% in case of LCVP <italic>A</italic><sup>(2)</sup> with multiple scales). According to McNemar’s test, however, the result of the DT-CWT method applied in the RGB color space is not significantly different from the result achieved with LCVP <italic>A</italic><sup>(2)</sup> using multiple scales in the CIELAB color space (85.0 ± 1.9%). In the CIELAB color space the DT-CWT method yields a significantly lower classification rate (84.4% compared to 85.0 ± 1.9% in case of LCVP <italic>A</italic><sup>(2)</sup> with multiple scales).</p>
          <p>We notice that, compared to the results obtained in (<xref rid="b0125 b0055" ref-type="bibr">Kwitt and Uhl, 2007; Häfner et al., 2010</xref>), the results presented for these methods in this work are considerably lower. The result discrepancies are due to the different kinds of cross-validation protocols used. While in (<xref rid="b0125 b0055" ref-type="bibr">Kwitt and Uhl, 2007; Häfner et al., 2010</xref>) the leave-one-out cross-validation (LOO-CV) has been used, this work employs LOPO-CV, which is more restrictive (the training set must not contain features from the patient from whom an image is currently classified). This results in the limitation that the training set available for each image is smaller. In addition, LOPO-CV also inhibits the use of more than one image from the same polyp. Thus, the images contained within the training set are less likely to be similar to the image to be classified. This makes it more likely that neighbors from wrong image classes are selected as nearest neighbors during the classification, resulting in more classification errors and, therefore, lowered classification accuracies.</p>
        </sec>
      </sec>
      <sec id="s0080">
        <label>7.3</label>
        <title>Performance analysis</title>
        <p><xref rid="t0025" ref-type="table">Table 5</xref> shows the results of our performance measurements (times are given in milliseconds). The column <italic>T</italic><sub><italic>O</italic></sub> shows the time needed to compute the respective operator on a single image including the time consumed by the histogram generation. <italic>T</italic><sub><italic>C</italic></sub> represents the time needed to classify a single image using the <italic>k</italic>-NN classifier with <italic>k</italic> = 1 and the Bhattacharyya distance. Carrying out the timing measurements for <italic>k</italic> = 1 only can be justified by the fact that the choice of this value has no noticeable influence on the time needed for classification. This is due to the fact that, no matter which <italic>k</italic>-value we choose, we have to compute the distances from an unknown sample to all training samples (which is the most time consuming part of the classification).</p>
        <p>To obtain the values <italic>T</italic><sub><italic>O</italic></sub> and <italic>T</italic><sub><italic>C</italic></sub> we carried out the respective computations for all images in our database and divided the resulting times by the number of images. <italic>T</italic><sub><italic>T</italic></sub> = <italic>T</italic><sub><italic>O</italic></sub> + <italic>T</italic><sub><italic>C</italic></sub> is the total time. In case of the CIELAB experiments the color space conversion takes place as a pre-processing step. Since it therefore does not matter which color space we base our experiments on, we present time measurements for the RGB color space only. The column <italic>SF</italic> roughly indicates how much higher the total computational demand is for each method, compared to the fastest method listed in <xref rid="t0025" ref-type="table">Table 5</xref>, which is <italic>A</italic><sup>(2)</sup> (determined separately for single scale experiments and multi-scale experiments).</p>
        <p>As we can easily see from the number of bins needed by each method (see <xref rid="t0025" ref-type="table">Table 5</xref>), the LCVP operator is superior in terms of compactness. In case of the single scale tests we obtain histograms containing 256 bins only, while OCLBP results in 9 such histograms, resulting in a total of 2304 bins. The JC-MB-LBP method contains even more bins since in this case we compute 2D joint-color histograms.</p>
        <p>Hence, it is no surprise that the proposed operator is also the fastest one in terms of the classification for a single image. But it is also obvious that in case of LCVP the time needed for classification does not affect <italic>T</italic><sub><italic>T</italic></sub> very much since <italic>T</italic><sub><italic>O</italic></sub> is significantly higher in these cases. In case of JC-MB-LBP the situation is the other way round. While the time needed for the computation of the JC-MB-LBP data and the construction of the histograms (<italic>T</italic><sub><italic>O</italic></sub>) is considerably lower compared to the other methods, the time needed for classification is significantly higher. Altogether this makes the JC-MB-LBP operator the slowest one.</p>
        <p>In case of the multi-scale experiments a similar behavior can be observed. As indicated in Section <xref rid="s0015" ref-type="sec">3</xref>, in this case the total computational demand for each method roughly multiplies by 3. Regarding <italic>T</italic><sub><italic>T</italic></sub>, LCVP <italic>A</italic><sup>(1)</sup> and <italic>A</italic><sup>(2)</sup> are considerably faster compared to JC-MB-LBP. OCLBP is slower than LCVP <italic>A</italic><sup>(2)</sup> but slightly faster than LCVP <italic>A</italic><sup>(1)</sup>.</p>
        <p>From the definitions of <italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> (see Eqs. <xref rid="e0070 e0075" ref-type="disp-formula">(3) and (4)</xref>) one would expect that <italic>T</italic><sub><italic>O</italic></sub> should be lower for LCVP <italic>A</italic><sup>(1)</sup> compared to LCVP <italic>A</italic><sup>(2)</sup>. However, this is not the case, as we notice from <xref rid="t0025" ref-type="table">Table 5</xref>. The reason for this behavior is that in case of <italic>F</italic><sub>1</sub> the computation of ∥<italic>v</italic><sub>1</sub> − <italic>v</italic><sub>2</sub>∥ (coming along with a rather slow square root) has to be carried out for each possible pair of pixel positions within a neighborhood (see Eq. <xref rid="e0025" ref-type="disp-formula">(7)</xref>). In case of <italic>F</italic><sub>2</sub> the needed color vector lengths are precomputed, hence, Eq. <xref rid="e0025" ref-type="disp-formula">(7)</xref> reduces to the computation of the absolute values of a couple of subtraction results. Considering the huge number of different color vector pairs which must be taken into account in case of <italic>F</italic><sub>1</sub> (approximately 500,000 for our images of size 256 × 256), <italic>A</italic><sup>(2)</sup> will always be faster compared to <italic>A</italic><sup>(1)</sup> since in case of <italic>F</italic><sub>2</sub> we need to compute one square root only per color vector (65,536 in case of our images).</p>
        <p>If we assume an endoscopy video to be captured at a frame rate of 25 frames per second a real-time application demands processing times of at most 40 ms for a single frame. From the timing results in <xref rid="t0025" ref-type="table">Table 5</xref> we notice that this requirement is currently not met by any of the operators. Nevertheless, the significant speedup between JC-MB-LBP and LCVP <italic>A</italic><sup>(2)</sup> represents a step into the direction of a real-time application – especially when considering that both methods perform equally well in case of CIELAB multi-scale experiments.</p>
        <p>In addition, LCVP can potentially be further optimized in terms of the runtime performance by employing hardware acceleration (e.g. GPU-based computing (<xref rid="b0180" ref-type="bibr">Sanders and Kandrot, 2010</xref>)).</p>
      </sec>
    </sec>
    <sec id="s0085">
      <label>8</label>
      <title>Conclusion</title>
      <p>In this work we proposed a novel color texture operator which is based on a noise-robust LBP variant. Treating the color components of pixels as color vectors and introducing suitable similarity measures resulted in a compact descriptor for an image, which nevertheless incorporates all color information available.</p>
      <p>Applied to the classification of chromo-colonoscopic imagery, we have shown that the inclusion of color information in LBP-based classification improves staging. This is supported by the fact that the grayscale variant of JC-MB-LBP is mostly outperformed by the color-based LBP operators (except for OCLBP, which consistently delivers lower classification rates compared to all other methods).</p>
      <p>Compared to OCLBP, the different variants of the LCVP operator are always significantly superior, in terms of speed as well as in terms of the overall classification accuracy. But we notice that the JC-MB-LBP operator in many cases delivers higher classification accuracies as compared to LCVP. However, the multi-scale extension of the LCVP operator in the CIELAB color space either outperforms the JC-MB-LBP operator or at least performs equally well (with no statistically significant differences). In addition, comparing the computational demand between LCVP and all other LBP variants reveals that LCVP is up to 7.5 times faster compared to the other methods. This can be attributed to the compactness of the features used in case of LCVP.</p>
      <p>From <xref rid="t0025" ref-type="table">Table 5</xref> we may further deduce that LCVP results may be improved by employing a stronger classifier at a moderate increase of overall classification time: The time spent in classification (<bold>T</bold><sub><bold>C</bold></sub>) covers only a small share of the total time (<bold>T</bold><sub><bold>T</bold></sub>) for LCVP, while for JC-MB-LBP <bold>T</bold><sub><bold>T</bold></sub> is already dominated by the <bold>T</bold><sub><bold>C</bold></sub> share. Hence, employment of a more costly classification will dramatically increase <bold>T</bold><sub><bold>T</bold></sub> in case of JC-MB-LBP, but will only marginally affect LCVP.</p>
      <p>The promising results and the rather low computational complexity of the LCVP operator make it an alternative to JC-MB-LBP. Moreover, LCVP is also a sensible candidate for inclusion into a classifier ensemble.</p>
      <p>One strength of the multi-scale LCVP operator is the fact that it needs only two parameters (the averaging filter kernel width for the finest scale and the number of scales to be used). Apart from that, since the operator is not tailored to a specific type of imagery we also conclude that the operator is generic and may also be applicable to other application scenarios.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="b0005">
        <mixed-citation publication-type="other">Ameling, S., Wirth, S., Paulus, D., Lacey, G., Vilarino, F., 2009. Texture-based polyp detection in colonoscopy. In: Bildverarbeitung für die Medizin 2009. Springer, Berlin, Heidelberg, pp. 346–350 (Informatik aktuell, No. 15).</mixed-citation>
      </ref>
      <ref id="b0010">
        <mixed-citation publication-type="other">André, B., Vercauteren, T., Perchant, A., Buchner, A.M., Wallace, M.B., Ayache, N., 2009a. Endomicroscopic image retrieval and classification using invariant visual features. In: Proceedings of the 6th IEEE International Symposium on Biomedical Imaging: From Nano to Macro (ISBI’09), Boston, Massachusetts, USA, pp. 346–349.</mixed-citation>
      </ref>
      <ref id="b0015">
        <mixed-citation publication-type="other">André, B., Vercauteren, T., Perchant, A., Wallace, M.B., Buchner, A.M., Ayache, N., 2009b. Introducing space and time in local feature-based endomicroscopic image retrieval. In: Proceedings of the MICCAI 2009 Workshop – Medical Content-based Retrieval for Clinical Decision (MCBR-CDS’09), London, UK, pp. 18–30.</mixed-citation>
      </ref>
      <ref id="b0020">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bellman</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <chapter-title>Adaptive Control Processes: A Guided Tour</chapter-title>
          <year>1961</year>
          <publisher-name>Princeton University Press</publisher-name>
        </element-citation>
      </ref>
      <ref id="b0025">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruno</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Magnification endoscopy, high resolution endoscopy, and chromoscopy; towards a better optical diagnosis</article-title>
          <source>Gut</source>
          <volume>52</volume>
          <year>2003</year>
          <fpage>7</fpage>
          <lpage>11</lpage>
          <pub-id pub-id-type="pmid">12477747</pub-id>
        </element-citation>
      </ref>
      <ref id="b0030">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chang</surname>
              <given-names>C.C.</given-names>
            </name>
            <name>
              <surname>Hsieh</surname>
              <given-names>C.R.</given-names>
            </name>
            <name>
              <surname>Lou</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>C.L.</given-names>
            </name>
            <name>
              <surname>Tiong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>I.V.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.N.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.H.</given-names>
            </name>
          </person-group>
          <article-title>Comparative study of conventional colonoscopy, magnifying chromoendoscopy, and magnifying narrow-band imaging systems in the differential diagnosis of small colonic polyps between trainee and experienced endoscopist</article-title>
          <source>Int. J. Colorectal Dis.</source>
          <volume>24</volume>
          <year>2009</year>
          <fpage>1413</fpage>
          <lpage>1419</lpage>
          <pub-id pub-id-type="pmid">19603174</pub-id>
        </element-citation>
      </ref>
      <ref id="b0035">
        <mixed-citation publication-type="other">Connah, D., Finlayson, G.D., 2006. Using local binary pattern operators for colour constant image indexing. In: Proceedings of the 3rd European Conference on Colour in Graphics, Imaging and Vision (CGIV’06), Leeds, UK.</mixed-citation>
      </ref>
      <ref id="b0040">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Everitt</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <chapter-title>The Analysis of Contingency Tables</chapter-title>
          <year>1977</year>
          <publisher-name>Chapman and Hall</publisher-name>
        </element-citation>
      </ref>
      <ref id="b0045">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gonzalez</surname>
              <given-names>R.C.</given-names>
            </name>
            <name>
              <surname>Woods</surname>
              <given-names>R.E.</given-names>
            </name>
          </person-group>
          <chapter-title>Digital Image Processing</chapter-title>
          <edition>second ed.</edition>
          <year>2002</year>
          <publisher-name>Prentice Hall</publisher-name>
        </element-citation>
      </ref>
      <ref id="b0050">
        <mixed-citation publication-type="other">Gross, S., Stehle, T., Behrens, A., Auer, R., Aach, T., Winograd, R., Trautwein, C., Tischendorf, J., 2009. A comparison of blood vessel features and local binary patterns for colorectal polyp classification. In: Proceedings of Medical Imaging 2008: Computer-Aided Diagnosis, Orlando, USA.</mixed-citation>
      </ref>
      <ref id="b0055">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Häfner</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Brunauer</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Payer</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Resch</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gangl</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Uhl</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vécsei</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wrba</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Computer-aided classification of zoom-endoscopical images using Fourier filters</article-title>
          <source>IEEE Trans. Inform. Technol. Biomed.</source>
          <volume>14</volume>
          <year>2010</year>
          <fpage>958</fpage>
          <lpage>970</lpage>
        </element-citation>
      </ref>
      <ref id="b0060">
        <mixed-citation publication-type="other">Häfner, M., Gangl, A., Kwitt, R., Uhl, A., Vécsei, A., Wrba, F., 2009a. Improving pit-pattern classification of endoscopy images by a combination of experts. In: Proceedings of the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI’09), London, UK, pp. 247–254.</mixed-citation>
      </ref>
      <ref id="b0065">
        <mixed-citation publication-type="other">Häfner, M., Gangl, A., Liedlgruber, M., Uhl, A., Vécsei, A., Wrba, F., 2009b. Pit pattern classification using extended local binary patterns. In: Proceedings of the 9th International Conference on Information Technology and Applications in Biomedicine (ITAB’09), Larnaca, Cyprus.</mixed-citation>
      </ref>
      <ref id="b0070">
        <mixed-citation publication-type="other">Häfner, M., Gangl, A., Liedlgruber, M., Uhl, A., Vécsei, A., Wrba, F., 2009c. Pit pattern classification using multichannel features and multiclassification. In: Exarchos, T.P., Papadopoulos, A. (Eds.), Handbook of Research on Advanced Techniques in Diagnostic Imaging and Biomedical Applications. IGI Global, Hershey, PA, USA, pp. 335–350.</mixed-citation>
      </ref>
      <ref id="b0075">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hurlstone</surname>
              <given-names>D.P.</given-names>
            </name>
          </person-group>
          <article-title>High-resolution magnification chromoendoscopy: common problems encountered in “pit pattern” interpretation and correct classification of flat colorectal lesions</article-title>
          <source>Am. J. Gastroenterol.</source>
          <volume>97</volume>
          <year>2002</year>
          <fpage>1069</fpage>
          <lpage>1070</lpage>
          <pub-id pub-id-type="pmid">12003399</pub-id>
        </element-citation>
      </ref>
      <ref id="b0080">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Iakovidis</surname>
              <given-names>D.K.</given-names>
            </name>
            <name>
              <surname>Maroulis</surname>
              <given-names>D.E.</given-names>
            </name>
            <name>
              <surname>Karkanis</surname>
              <given-names>S.A.</given-names>
            </name>
          </person-group>
          <article-title>An intelligent system for automatic detection of gastrointestinal adenomas in video endoscopy</article-title>
          <source>Comput. Biol. Med.</source>
          <volume>36</volume>
          <year>2006</year>
          <fpage>1084</fpage>
          <lpage>1103</lpage>
          <pub-id pub-id-type="pmid">16293240</pub-id>
        </element-citation>
      </ref>
      <ref id="b0085">
        <mixed-citation publication-type="other">Iakovidis, D.K., Maroulis, D.E., Karkanis, S.A., Brokos, A., 2005. A comparative study of texture features for the discrimination of gastric polyps in endoscopic video. In: Proceedings of the 18th IEEE Symposium on Computer-Based Medical Systems (CBMS’05), Dublin, Ireland, pp. 575–580.</mixed-citation>
      </ref>
      <ref id="b0090">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <chapter-title>Fundamentals of Digital Image Processing</chapter-title>
          <year>1989</year>
          <publisher-name>Prentice Hall</publisher-name>
        </element-citation>
      </ref>
      <ref id="b0095">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Karkanis</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Computer-aided tumor detection in endoscopic video using color wavelet features</article-title>
          <source>IEEE Trans. Inform. Technol. Biomed.</source>
          <volume>7</volume>
          <year>2003</year>
          <fpage>141</fpage>
          <lpage>152</lpage>
        </element-citation>
      </ref>
      <ref id="b0100">
        <mixed-citation publication-type="other">Karkanis, S.A., Iakovidis, D., Karras, D., Maroulis, D., 2001. Detection of lesions in endoscopic video using textural descriptors on wavelet domain supported by artificial neural network architectures. In: Proceedings of the IEEE International Conference in Image Processing (ICIP’01), Thessaloniki, Greece, pp. 833–836.</mixed-citation>
      </ref>
      <ref id="b0105">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kato</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>K.I.</given-names>
            </name>
            <name>
              <surname>Sano</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fujii</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Saito</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Matsuda</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Koba</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Yoshida</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fujimori</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Magnifying colonoscopy as a non-biopsy technique for differential diagnosis of non-neoplastic and neoplastic lesions</article-title>
          <source>World J. Gastroenterol.</source>
          <volume>12</volume>
          <year>2006</year>
          <fpage>1416</fpage>
          <lpage>1420</lpage>
          <pub-id pub-id-type="pmid">16552812</pub-id>
        </element-citation>
      </ref>
      <ref id="b0110">
        <mixed-citation publication-type="other">Krishnan, S.M., Yang, X.L., Chan, K., Kumar, S., Goh, P.M.Y., 1998. Intestinal abnormality detection from endoscopic images. In: Proceedings of the 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS’98), Hong Kong, China, pp. 895–898.</mixed-citation>
      </ref>
      <ref id="b0115">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kudo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hirota</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nakajima</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Hosobe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kusaka</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kobayashi</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Himori</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Yagyuu</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Colorectal tumours and pit pattern</article-title>
          <source>J. Clin. Pathol.</source>
          <volume>47</volume>
          <year>1994</year>
          <fpage>880</fpage>
          <lpage>885</lpage>
          <pub-id pub-id-type="pmid">7962600</pub-id>
        </element-citation>
      </ref>
      <ref id="b0120">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kudo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tamura</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nakajima</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Yamano</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kusaka</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Watanabe</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Diagnosis of colorectal tumorous lesions by magnifying endoscopy</article-title>
          <source>Gastrointest. Endosc.</source>
          <volume>44</volume>
          <year>1996</year>
          <fpage>8</fpage>
          <lpage>14</lpage>
          <pub-id pub-id-type="pmid">8836710</pub-id>
        </element-citation>
      </ref>
      <ref id="b0125">
        <mixed-citation publication-type="other">Kwitt, R., Uhl, A., 2007. Modeling the marginal distributions of complex wavelet coefficient magnitudes for the classification of zoom-endoscopy images. In: Proceedings of the IEEE Computer Society Workshop on Mathematical Methods in Biomedical Image Analysis (MMBIA ’07), Rio de Janeiro, Brasil, pp. 1–8.</mixed-citation>
      </ref>
      <ref id="b0130">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Liao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <chapter-title>Learning multi-scale block local binary patterns for face recognition</chapter-title>
          <source>Advances in Biometrics</source>
          <year>2007</year>
          <publisher-name>Springer</publisher-name>
          <fpage>828</fpage>
          <lpage>837</lpage>
        </element-citation>
      </ref>
      <ref id="b0135">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mackiewicz</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Berens</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fisher</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Wireless capsule endoscopy color video segmentation</article-title>
          <source>IEEE Trans. Med. Imag.</source>
          <volume>27</volume>
          <year>2008</year>
          <fpage>1769</fpage>
          <lpage>1781</lpage>
        </element-citation>
      </ref>
      <ref id="b0140">
        <mixed-citation publication-type="other">Mäenpää, T., Ojala, T., Pietikäinen, M., Soriano, M., 2000. Robust texture classification by subsets of local binary patterns. In: Proceedings of the 15th International Conference on Pattern Recognition (ICPR’00), Barcelona, Spain, pp. 935–938.</mixed-citation>
      </ref>
      <ref id="b0145">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mäenpää</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pietikäinen</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Classification with color and texture: jointly or separately?</article-title>
          <source>Pattern Recognit.</source>
          <volume>37</volume>
          <year>2004</year>
          <fpage>1629</fpage>
          <lpage>1640</lpage>
        </element-citation>
      </ref>
      <ref id="b0150">
        <mixed-citation publication-type="other">Mäenpää, T., Pietikäinen, M., Viertola, J., 2002. Separating color and pattern information for color texture discrimination. In: Proceedings of the 16th International Conference on Pattern Recognition (ICPR’02), Quebec City, Canada, pp. 668–671.</mixed-citation>
      </ref>
      <ref id="b0155">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maroulis</surname>
              <given-names>D.E.</given-names>
            </name>
            <name>
              <surname>Iakovidis</surname>
              <given-names>D.K.</given-names>
            </name>
            <name>
              <surname>Karkanis</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Karras</surname>
              <given-names>D.A.</given-names>
            </name>
          </person-group>
          <article-title>CoLD: a versatile detection system for colorectal lesions in endoscopy video-frames</article-title>
          <source>Comput. Methods Programs Biomed.</source>
          <volume>70</volume>
          <year>2003</year>
          <fpage>151</fpage>
          <lpage>166</lpage>
          <pub-id pub-id-type="pmid">12507791</pub-id>
        </element-citation>
      </ref>
      <ref id="b0160">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ojala</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pietikäinen</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Harwood</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>A comparative study of texture measures with classification based on feature distributions</article-title>
          <source>Pattern Recognit.</source>
          <volume>29</volume>
          <year>1996</year>
          <fpage>51</fpage>
          <lpage>59</lpage>
        </element-citation>
      </ref>
      <ref id="b0165">
        <mixed-citation publication-type="other">Pietikäinen, M., Mäenpää, T., Viertola, J., 2002. Color texture classification with color histograms and local binary patterns. In: Proceedings of the 2nd International Workshop on Texture Analysis and Synthesis, Copenhagen, Denmark, pp. 109–112.</mixed-citation>
      </ref>
      <ref id="b0170">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Raghavendra</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hewett</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Rex</surname>
              <given-names>D.K.</given-names>
            </name>
          </person-group>
          <article-title>Differentiating adenomas from hyperplastic colorectal polyps: narrow-band imaging can be learned in 20 minutes</article-title>
          <source>Gastrointest Endosc.</source>
          <volume>72</volume>
          <year>2010</year>
          <fpage>572</fpage>
          <lpage>576</lpage>
          <pub-id pub-id-type="pmid">20561618</pub-id>
        </element-citation>
      </ref>
      <ref id="b0175">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rastogi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pondugula</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Bansal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wani</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Keighley</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Sugar</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Callahan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Recognition of surface mucosal and vascular patterns of colon polyps by using narrow-band imaging: interobserver and intraobserver agreement and prediction of polyp histology</article-title>
          <source>Gastrointest Endosc.</source>
          <volume>69</volume>
          <year>2009</year>
          <fpage>716</fpage>
          <lpage>722</lpage>
          <pub-id pub-id-type="pmid">19251016</pub-id>
        </element-citation>
      </ref>
      <ref id="b0180">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Sanders</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kandrot</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <chapter-title>CUDA by Example: An Introduction to General-Purpose GPU Programming</chapter-title>
          <year>2010</year>
          <publisher-name>Addison-Wesley Longman</publisher-name>
          <publisher-loc>Amsterdam</publisher-loc>
        </element-citation>
      </ref>
      <ref id="b0185">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Smolka</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Venetsanopoulos</surname>
              <given-names>A.N.</given-names>
            </name>
          </person-group>
          <chapter-title>Noise reduction and edge detection in color images</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Lukac</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Plataniotis</surname>
              <given-names>K.N.</given-names>
            </name>
          </person-group>
          <source>Color Image Processing: Methods and Applications</source>
          <year>2007</year>
          <publisher-name>CRC Press</publisher-name>
          <fpage>75</fpage>
          <lpage>102</lpage>
          <comment>(Image Processing Series)</comment>
        </element-citation>
      </ref>
      <ref id="b0190">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>L.M.W.K.</given-names>
            </name>
            <name>
              <surname>Adler</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Chand</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Conway</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Croffie</surname>
              <given-names>J.M.B.</given-names>
            </name>
            <name>
              <surname>DiSario</surname>
              <given-names>J.A.</given-names>
            </name>
            <name>
              <surname>Mishkin</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Somogyi</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tierney</surname>
              <given-names>W.M.</given-names>
            </name>
            <name>
              <surname>Petersen</surname>
              <given-names>B.T.</given-names>
            </name>
          </person-group>
          <article-title>Chromoendoscopy</article-title>
          <source>Gastrointest Endosc.</source>
          <volume>66</volume>
          <year>2007</year>
          <fpage>639</fpage>
          <lpage>649</lpage>
          <pub-id pub-id-type="pmid">17643437</pub-id>
        </element-citation>
      </ref>
      <ref id="b0195">
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Stehle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Auer</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gross</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Behrens</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wulff</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Aach</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Winograd</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Trautwein</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tischendorf</surname>
              <given-names>J.J.W.</given-names>
            </name>
          </person-group>
          <chapter-title>Classification of colon polyps in NBI endoscopy using vascularization features</chapter-title>
          <source>Medical Imaging 2009: Computer-Aided Diagnosis</source>
          <year>2009</year>
          <publisher-name>SPIE</publisher-name>
          <publisher-loc>Orlando, USA</publisher-loc>
        </element-citation>
      </ref>
      <ref id="b0200">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tischendorf</surname>
              <given-names>J.J.W.</given-names>
            </name>
            <name>
              <surname>Gross</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Winograd</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hecker</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Auer</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Behrens</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Trautwein</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Aach</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Stehle</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Computer-aided classification of colorectal polyps based on vascular patterns: a pilot study</article-title>
          <source>Endoscopy</source>
          <volume>42</volume>
          <year>2010</year>
          <fpage>203</fpage>
          <lpage>207</lpage>
          <pub-id pub-id-type="pmid">20101564</pub-id>
        </element-citation>
      </ref>
      <ref id="b0205">
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zanoni</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Cutait</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Averbach</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Magnifying colonoscopy: interobserver agreement in the assessment of colonic pit patterns and its correlation with histopathological findings</article-title>
          <source>Int. J. Colorectal. Dis.</source>
          <volume>22</volume>
          <year>2007</year>
          <fpage>1383</fpage>
          <lpage>1388</lpage>
          <pub-id pub-id-type="pmid">17579873</pub-id>
        </element-citation>
      </ref>
    </ref-list>
    <bio>
      <p><bold>Michael Häfner</bold> was born on December 9, 1968 in Vienna, Austria. He studied Medicine in Vienna and Perugia, Italy and graduated on January 23, 1995. He started working at the Department of Gastroenterology and Hepatology of the Medical University of Vienna in 1994, where he also was trained in Internal Medicine. He is specialist in Gastroenterology and Hepatology with his main focus on interventional endoscopy. Michael Häfner is currently secretary of the Austrian Society of Gastroenterology and Hepatology and organizer of several courses on interventional endoscopy, including an annual postgraduate course on endoscopy in Vienna. He acts as tutor at various national and international hands-on training courses. Recently he spent half a year in Malaysia to start up an endoscopy unit in Kuala Lumpur. In September 2009 he became chief physician of the Department of Internal Medicine at the St. Elisabeth Hospital, Vienna, Austria.</p>
    </bio>
    <bio>
      <p><bold>Michael Liedlgruber</bold> received his Master’s degree in Computer Sciences from the University of Salzburg (Austria) in May 2006. His Master’s thesis has already been devoted to the classification of colonic polyps using wavelets. Currently he is working on his PhD thesis at the Department of Computer Sciences (University of Salzburg) in the same field of research. His main research interests include medical image processing and pattern recognition.</p>
    </bio>
    <bio>
      <p><bold>Andreas Uhl</bold> is an associate professor at the Department of Computer Sciences (University of Salzburg) where he heads the Multimedia Processing and Security Lab. His research interests include image and video processing and compression, wavelets, media security, medical imaging, biometrics, and number-theoretical numerics.</p>
    </bio>
    <bio>
      <p><bold>Andreas Vécsei</bold> graduated (Dr. Med. Univ.) at the University of Vienna. His post-graduate training in Austria and Kerala (India) made him receive his license to practice general pediatrics. After having been instructed in pediatric gastroenterology according to the syllabus of GPGE (an association of German-speaking pediatric gastroenterologists) at the Medical University of Vienna and Munich, he received his certificate in 2006. Since 2006 he is in charge of the Endoscopy Unit of the St. Anna Children’s Hospital, Vienna. His current research interests include Helicobacter pylori infection, celiac disease and computer-assisted evaluation of endoscopic images including the assessment of villous atrophy on endoscopic examination of the small intestine.</p>
    </bio>
    <bio>
      <p><bold>Friedrich Wrba</bold> is professor of Pathology at the Department of Clinical Pathology of the University of Vienna. After training as a general practitioner in various Austrian hospitals he started his special training in clinical Pathology, which he finished successfully at the Department of Clinical Pathology at the University of Vienna in the year 1987. In 1990–1992 he spent 2.5 years as a Visiting Research Fellow at the Imperial Cancer Research Fund Laboratories in London, England, (Walter Bodmer’s laboratory). Since 1991 he is lecturer and Professor of Pathology, and staff member of the department. His areas of special interest are gastroenterology, hepatology, and molecular pathology.</p>
    </bio>
    <ack>
      <title>Acknowledgments</title>
      <p>This work is partially funded by the Austrian Science Fund (FWF) under Project No. L366-N15 and by the Austrian National Bank “Jubiläumsfonds” Project No. 12514.</p>
    </ack>
  </back>
  <floats-group>
    <fig id="f0005">
      <label>Fig. 1</label>
      <caption>
        <p>Overview of our system for endoscopic image classification.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="f0010">
      <label>Fig. 2</label>
      <caption>
        <p>Schematic illustration of the pit pattern classification along with example images for each pit pattern type.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="f0015">
      <label>Fig. 3</label>
      <caption>
        <p>LBP code generation for one pixel position (a) using block averaging and (b) based on a pre-processed image (using an averaging filter).</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="f0020">
      <label>Fig. 4</label>
      <caption>
        <p>An example pixel neighborhood within some color channel along with the respective computation steps to obtain the final LBP number (for the center pixel denoted by the white box).</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="f0025">
      <label>Fig. 5</label>
      <caption>
        <p>Differences between different variants of LBP applied to the red channel of an example endoscopic image.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
    <fig id="f0030">
      <label>Fig. 6</label>
      <caption>
        <p>Schematic overview of the proposed operator. Each pixel of an (already averaged) input image is treated as a 3D vector, resulting in a color vector field. Then the similarity between pixels is computed using a similarity function. Based on the resulting LCVP numbers we finally create a 1D histogram (the dark gray part within the slices denotes a sample window along with the respective color vectors).</p>
      </caption>
      <graphic xlink:href="gr6"/>
    </fig>
    <fig id="f0035">
      <label>Fig. 7</label>
      <caption>
        <p>Illustration of the similarity measures used throughout this work (between two vectors <italic>v</italic><sub>1</sub> and <italic>v</italic><sub>2</sub>).</p>
      </caption>
      <graphic xlink:href="gr7"/>
    </fig>
    <fig id="f0040">
      <label>Fig. 8</label>
      <caption>
        <p>Illustration of color changes along a certain color vector in RGB color space and LAB color space. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr8"/>
    </fig>
    <fig id="f0045">
      <label>Fig. 9</label>
      <caption>
        <p>Computation of the aggregated similarity between one color vector at (<italic>x</italic><sub><italic>l</italic></sub>, <italic>y</italic><sub><italic>l</italic></sub>) (shown by a white box) and all color vectors (<italic>x</italic><sub><italic>k</italic></sub>, <italic>y</italic><sub><italic>k</italic></sub>) within the neighborhood. This computation is repeated for each color vector <italic>l</italic> = 1, … , 9 within the neighborhood. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p>
      </caption>
      <graphic xlink:href="gr9"/>
    </fig>
    <table-wrap id="t0005" position="float">
      <label>Table 1</label>
      <caption>
        <p>A comparison of LBP-based methods using color information (<italic>N</italic><sub><italic>C</italic></sub> and <italic>N</italic><sub><italic>B</italic></sub> denote the number of color channels considered and the number of histogram bins used, respectively).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Reference</th>
            <th>
              <italic>N</italic>
              <sub>C</sub>
            </th>
            <th>Histogram type</th>
            <th>
              <italic>N</italic>
              <sub>B</sub>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              <xref rid="b0150" ref-type="bibr">Mäenpää et al. (2002)</xref>
            </td>
            <td>3</td>
            <td>Inter-channel histograms</td>
            <td>2304</td>
          </tr>
          <tr>
            <td>
              <xref rid="b0145" ref-type="bibr">Mäenpää and Pietikäinen (2004)</xref>
            </td>
            <td>3</td>
            <td>Concatenated histograms</td>
            <td>768</td>
          </tr>
          <tr>
            <td>
              <xref rid="b0035" ref-type="bibr">Connah and Finlayson (2006)</xref>
            </td>
            <td>3</td>
            <td>Joint histograms</td>
            <td>205,379</td>
          </tr>
          <tr>
            <td>
              <xref rid="b0065" ref-type="bibr">Häfner et al. (2009b)</xref>
            </td>
            <td>2</td>
            <td>Joint histograms</td>
            <td>65,536</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0010" position="float">
      <label>Table 2</label>
      <caption>
        <p>Number of images and patients per class.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th/>
            <th>Non-neoplastic</th>
            <th>Neoplastic</th>
            <th>Total</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td># of images</td>
            <td>198</td>
            <td>518</td>
            <td>716</td>
          </tr>
          <tr>
            <td># of patients</td>
            <td>14</td>
            <td>32</td>
            <td>46</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0015" position="float">
      <label>Table 3</label>
      <caption>
        <p>Comparison of the average classification results for the RGB color space (the methods yielding the highest mean overall classification rates are shown in bold).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Method</th>
            <th>Accuracy</th>
            <th>Specificity</th>
            <th>Sensitivity</th>
            <th>
              <italic>A</italic>
              <sup>(1)</sup>
            </th>
            <th>
              <italic>A</italic>
              <sup>(2)</sup>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="6">Single scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td align="char">75.9 ± 0.3</td>
            <td align="char">33.3 ± 3.0</td>
            <td align="char">92.2 ± 0.7</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td align="char">74.2 ± 0.6</td>
            <td align="char">25.5 ± 4.3</td>
            <td align="char">92.8 ± 1.3</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td align="char">70.3 ± 1.8</td>
            <td align="char">33.5 ± 4.9</td>
            <td align="char">84.4 ± 3.7</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td>
              <bold>JC-MB-LBP</bold>
            </td>
            <td align="char"><bold>82.4</bold> ± <bold>0.8</bold></td>
            <td align="char"><bold>52.4</bold> ± <bold>4.2</bold></td>
            <td align="char"><bold>93.9</bold> ± <bold>1.2</bold></td>
            <td>√ (−)</td>
            <td>√ (−)</td>
          </tr>
          <tr>
            <td>JC-MB-LBP (gray)</td>
            <td align="char">75.4 ± 1.1</td>
            <td align="char">27.8 ± 7.1</td>
            <td align="char">93.6 ± 3.9</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td colspan="6">  </td>
          </tr>
          <tr>
            <td colspan="6">Multi-scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td align="char">79.1 ± 0.5</td>
            <td align="char">45.5 ± 2.0</td>
            <td align="char">91.9 ± 0.7</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td align="char">79.6 ± 0.8</td>
            <td align="char">40.4 ± 5.1</td>
            <td align="char">94.5 ± 1.1</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td align="char">70.0 ± 1.1</td>
            <td align="char">31.9 ± 6.9</td>
            <td align="char">84.6 ± 3.3</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td>
              <bold>JC-MB-LBP</bold>
            </td>
            <td align="char"><bold>82.7</bold> ± <bold>0.8</bold></td>
            <td align="char"><bold>55.1</bold> ± <bold>4.5</bold></td>
            <td align="char"><bold>93.2</bold> ± <bold>0.8</bold></td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>JC-MB-LBP (gray)</td>
            <td align="char">74.6 ± 0.7</td>
            <td align="char">22.0 ± 9.3</td>
            <td align="char">94.7 ± 2.6</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0020" position="float">
      <label>Table 4</label>
      <caption>
        <p>Comparison of the average classification results for the LAB color space (the methods yielding the highest mean overall classification rates are shown in bold).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Method</th>
            <th>Accuracy</th>
            <th>Specificity</th>
            <th>Sensitivity</th>
            <th>
              <italic>A</italic>
              <sup>(1)</sup>
            </th>
            <th>
              <italic>A</italic>
              <sup>(2)</sup>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="6">Single scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td align="char">76.9 ± 0.2</td>
            <td align="char">37.0 ± 1.4</td>
            <td align="char">92.1 ± 0.4</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td align="char">78.7 ± 2.2</td>
            <td align="char">35.9 ± 1.5</td>
            <td align="char">95.1 ± 3.0</td>
            <td/>
            <td/>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td align="char">71.5 ± 0.7</td>
            <td align="char">27.0 ± 4.8</td>
            <td align="char">88.5 ± 2.7</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td>
              <bold>JC-MB-LBP</bold>
            </td>
            <td align="char"><bold>85.3</bold> ± <bold>0.7</bold></td>
            <td align="char"><bold>76.6</bold> ± <bold>9.0</bold></td>
            <td align="char"><bold>88.6</bold> ± <bold>2.5</bold></td>
            <td>√ (−)</td>
            <td>√ (−)</td>
          </tr>
          <tr>
            <td>JC-MB-LBP (gray)</td>
            <td align="char">75.4 ± 1.1</td>
            <td align="char">27.8 ± 7.1</td>
            <td align="char">93.6 ±  3.9</td>
            <td/>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td colspan="6">  </td>
          </tr>
          <tr>
            <td colspan="6">Multi-scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td align="char">79.5 ± 0.5</td>
            <td align="char">46.3 ± 1.4</td>
            <td align="char">92.2 ± 0.7</td>
            <td/>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td align="char">85.0 ± 1.9</td>
            <td align="char">57.6 ± 2.0</td>
            <td align="char">95.5 ± 2.2</td>
            <td>√ (−)</td>
            <td/>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td align="char">71.7 ± 0.8</td>
            <td align="char">28.4 ± 3.9</td>
            <td align="char">88.2 ± 2.5</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
          <tr>
            <td>
              <bold>JC-MB-LBP</bold>
            </td>
            <td align="char"><bold>85.1</bold> ± <bold>0.2</bold></td>
            <td align="char"><bold>65.0</bold> ± <bold>3.2</bold></td>
            <td align="char"><bold>92.8</bold> ± <bold>1.4</bold></td>
            <td>√ (−)</td>
            <td/>
          </tr>
          <tr>
            <td>JC-MB-LBP (gray)</td>
            <td align="char">74.6 ± 0.7</td>
            <td align="char">22.0 ± 9.3</td>
            <td align="char">94.7 ±  2.6</td>
            <td>√ (+)</td>
            <td>√ (+)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap id="t0025" position="float">
      <label>Table 5</label>
      <caption>
        <p>Result of the performance analysis of the different methods (time measurements are given in milliseconds).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th>Method</th>
            <th>Bins</th>
            <th>
              <italic>T</italic>
              <sub>O</sub>
            </th>
            <th>
              <italic>T</italic>
              <sub>C</sub>
            </th>
            <th>
              <italic>T</italic>
              <sub>T</sub>
            </th>
            <th>SF</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="6">Single scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td>256</td>
            <td>166</td>
            <td>6</td>
            <td>172</td>
            <td align="char">2.1</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td>256</td>
            <td>75</td>
            <td>7</td>
            <td>82</td>
            <td align="char">1.0</td>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td>2304</td>
            <td>80</td>
            <td>54</td>
            <td>134</td>
            <td align="char">1.6</td>
          </tr>
          <tr>
            <td>JC-MB-LBP</td>
            <td>65,536</td>
            <td>38</td>
            <td>426</td>
            <td>464</td>
            <td align="char">5.7</td>
          </tr>
          <tr>
            <td colspan="6">  </td>
          </tr>
          <tr>
            <td colspan="6">Multi-scale</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(1)</sup></td>
            <td>768</td>
            <td>505</td>
            <td>26</td>
            <td>531</td>
            <td align="char">2.1</td>
          </tr>
          <tr>
            <td>LCVP <italic>A</italic><sup>(2)</sup></td>
            <td>768</td>
            <td>232</td>
            <td>27</td>
            <td>259</td>
            <td align="char">1.0</td>
          </tr>
          <tr>
            <td>OCLBP</td>
            <td>6912</td>
            <td>254</td>
            <td>158</td>
            <td>412</td>
            <td align="char">1.6</td>
          </tr>
          <tr>
            <td>JC-MB-LBP</td>
            <td>196,608</td>
            <td>126</td>
            <td>1824</td>
            <td>1950</td>
            <td align="char">7.5</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </floats-group>
</article>