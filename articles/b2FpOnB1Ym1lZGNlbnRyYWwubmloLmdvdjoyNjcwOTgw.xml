<article xmlns="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dtd.nlm.nih.gov/2.0/xsd/archivearticle http://dtd.nlm.nih.gov/2.0/xsd/archivearticle.xsd" article-type="research-article">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Curr Biol</journal-id>
      <journal-title>Current Biology </journal-title>
      <issn pub-type="ppub">0960-9822</issn>
      <issn pub-type="epub">1879-0445</issn>
      <publisher>
        <publisher-name>Cell Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2670980</article-id>
      <article-id pub-id-type="pmid">19285400</article-id>
      <article-id pub-id-type="publisher-id">CURBIO7109</article-id>
      <article-id pub-id-type="doi">10.1016/j.cub.2009.02.033</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Decoding Neuronal Ensembles in the Human Hippocampus</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Hassabis</surname>
            <given-names>Demis</given-names>
          </name>
          <email>d.hassabis@fil.ion.ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="cor1" ref-type="corresp">∗</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Chu</surname>
            <given-names>Carlton</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Rees</surname>
            <given-names>Geraint</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="aff2" ref-type="aff">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Weiskopf</surname>
            <given-names>Nikolaus</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Molyneux</surname>
            <given-names>Peter D.</given-names>
          </name>
          <xref rid="aff3" ref-type="aff">3</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Maguire</surname>
            <given-names>Eleanor A.</given-names>
          </name>
          <email>e.maguire@fil.ion.ucl.ac.uk</email>
          <xref rid="aff1" ref-type="aff">1</xref>
          <xref rid="cor2" ref-type="corresp">∗∗</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line><sup>1</sup>Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London WC1N 3BG, UK</addr-line>
      </aff>
      <aff id="aff2">
        <addr-line><sup>2</sup>Institute of Cognitive Neuroscience, University College London, 17 Queen Square, London WC1N 3AR, UK</addr-line>
      </aff>
      <aff id="aff3">
        <addr-line><sup>3</sup>Lionhead Studios, 1 Occam Court, Surrey Research Park, Guildford, Surrey GU2 7YQ, UK</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1"><label>∗</label>Corresponding author <email>d.hassabis@fil.ion.ucl.ac.uk</email></corresp>
        <corresp id="cor2"><label>∗∗</label>Corresponding author <email>e.maguire@fil.ion.ucl.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="ppub">
        <day>14</day>
        <month>4</month>
        <year>2009</year>
      </pub-date>
      <volume>19</volume>
      <issue>7-3</issue>
      <fpage>546</fpage>
      <lpage>554</lpage>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>10</month>
          <year>2008</year>
        </date>
        <date date-type="rev-recd">
          <day>27</day>
          <month>1</month>
          <year>2009</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>2</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2009 ELL &amp; Excerpta Medica.</copyright-statement>
        <copyright-year>2009</copyright-year>
        <copyright-holder>Elsevier Ltd</copyright-holder>
        <license>
          <p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</p>
        </license>
      </permissions>
      <abstract>
        <title>Summary</title>
        <sec>
          <title>Background</title>
          <p>The hippocampus underpins our ability to navigate, to form and recollect memories, and to imagine future experiences. How activity across millions of hippocampal neurons supports these functions is a fundamental question in neuroscience, wherein the size, sparseness, and organization of the hippocampal neural code are debated.</p>
        </sec>
        <sec>
          <title>Results</title>
          <p>Here, by using multivariate pattern classification and high spatial resolution functional MRI, we decoded activity across the population of neurons in the human medial temporal lobe while participants navigated in a virtual reality environment. Remarkably, we could accurately predict the position of an individual within this environment solely from the pattern of activity in his hippocampus even when visual input and task were held constant. Moreover, we observed a dissociation between responses in the hippocampus and parahippocampal gyrus, suggesting that they play differing roles in navigation.</p>
        </sec>
        <sec>
          <title>Conclusions</title>
          <p>These results show that highly abstracted representations of space are expressed in the human hippocampus. Furthermore, our findings have implications for understanding the hippocampal population code and suggest that, contrary to current consensus, neuronal ensembles representing place memories must be large and have an anisotropic structure.</p>
        </sec>
      </abstract>
      <kwd-group>
        <kwd>SYSNEURO</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>Introduction</title>
      <p>Information about the environment is thought to be encoded in the brain by activity in large populations of neurons <xref rid="bib1 bib2 bib3" ref-type="bibr">[1–3]</xref>. In order to understand the properties and dynamics of population codes, it is necessary to specify how they can be decoded in order to extract the precise information that they represent <xref rid="bib2" ref-type="bibr">[2]</xref>. This enterprise is at the heart of neuroscience and provides a substantial challenge <xref rid="bib3" ref-type="bibr">[3]</xref>. Decoding the activity of single, or small numbers of, neurons has been highly successful, with the best characterized example being the memory-related response of hippocampal place cells that fire invariantly when an animal is at a particular spatial location <xref rid="bib4 bib5 bib6" ref-type="bibr">[4–6]</xref>. It is not clear, however, what information such place cells represent at the population level, given that recording in vivo from thousands of hippocampal neurons simultaneously is not currently possible <xref rid="bib3 bib7 bib8 bib9" ref-type="bibr">[3, 7–9]</xref>. Other techniques such as immediate early gene imaging have provided some insights into memory representations at the population level <xref rid="bib10 bib11" ref-type="bibr">[10, 11]</xref> but have limited temporal resolution (in the order of minutes) and do not provide an in vivo measure, making it difficult to isolate with precision the specific feature of a stimulus, memory, or behavior associated with gene expression.</p>
      <p>Recently, invasive approaches to examining how neurons encode information <xref rid="bib5 bib12" ref-type="bibr">[5, 12]</xref> have been complemented by multivariate pattern analyses of noninvasive human functional MRI (fMRI) data <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref>. Functional MRI measures signals that are indirectly correlated with neuronal activity simultaneously in many individual voxels. Each voxel, depending on its size and location, contains thousands of neurons. Conventional univariate fMRI analysis methods focus on activity in each individual voxel in isolation. In contrast, multivariate pattern analyses harvest information from local patterns of activity expressed across multiple voxels and, hence, large neuronal populations. Not only can such novel analyses infer the presence of neuronal representations previously thought below the spatial resolution of fMRI <xref rid="bib15 bib16" ref-type="bibr">[15, 16]</xref>, but the ensemble activity of such distributed patterns can predict the perceptual state or intention of an individual with high accuracy <xref rid="bib17" ref-type="bibr">[17]</xref>. However, to date, there has been only limited application of this approach to memory <xref rid="bib18" ref-type="bibr">[18]</xref> and none that has focused specifically on decoding activity in the hippocampus, despite its critical mnemonic role <xref rid="bib19" ref-type="bibr">[19]</xref>. This is, perhaps, not surprising because making discriminations on the basis of activity in the hippocampus and surrounding medial temporal lobe (MTL) regions only presents a far more challenging classification problem than simply using whole-brain information in a category-based design that results in large activity differences across multiple brain regions <xref rid="bib18" ref-type="bibr">[18]</xref>.</p>
      <p>However, successful decoding from focal hippocampal fMRI signals would have significant implications for understanding how information is represented within neuronal populations in the human hippocampus and for appreciating fundamental properties of the hippocampal population code. The current consensus from invasive animal studies <xref rid="bib10 bib11" ref-type="bibr">[10, 11]</xref> and computational models <xref rid="bib20 bib21" ref-type="bibr">[20, 21]</xref> is that this population code is random and uniformly distributed, casting doubt on some earlier studies that suggested a potential functional structure in the hippocampus <xref rid="bib22 bib23" ref-type="bibr">[22, 23]</xref>. However, if there is a functional organization to the hippocampal population code, then activity at the voxel level should also be nonuniform, making classification possible with multivariate methods applied to human fMRI data <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref>.</p>
      <p>We set out to test this hypothesis by combining fMRI at high spatial resolution with multivariate pattern analysis techniques <xref rid="bib13 bib14 bib24" ref-type="bibr">[13, 14, 24]</xref> to investigate whether it was possible to accurately predict the precise position of an individual within an environment from patterns of activity across hippocampal voxels alone. We used an interactive virtual reality (VR) spatial navigation task (<xref rid="fig1" ref-type="fig">Figure 1</xref>), given that spatial navigation critically relies on the hippocampus <xref rid="bib4 bib19" ref-type="bibr">[4, 19]</xref>. Importantly, by holding visual inputs and task constant after successful navigation to a position within the VR environment, we could isolate and characterize the “abstract” (i.e., independent of current sensory inputs) internal representation of the environment's layout. With this approach, we show that noninvasive in vivo measurements of activity across the population of neurons in the human hippocampus can be used to precisely decode and accurately predict the position of an individual within their environment.</p>
    </sec>
    <sec id="sec2">
      <title>Results</title>
      <p>We acquired blood-oxygen level-dependent (BOLD) contrast, high spatial resolution fMRI images focused on the hippocampus and wider MTL (see the <xref rid="sec5" ref-type="sec">Experimental Procedures</xref> and <xref rid="fig2" ref-type="fig">Figure 2</xref>B) while participants navigated as quickly and accurately as possible between four arbitrarily chosen target positions (A, B, C, and D) in each of two well-learned virtual reality environments: a blue room and a green room (<xref rid="fig1" ref-type="fig">Figure 1</xref>). These two environments were designed to be austere to minimize the impact of extraneous sensory inputs. Apart from color, which acted as a simple, unambiguous retrieval cue for each room and is processed in extrastriate cortex <xref rid="bib25" ref-type="bibr">[25]</xref>, the two environments were well matched, with no significant difference between navigation times or overall time spent in either room (see <xref rid="app2" ref-type="sec">Table S1</xref> available online for behavioral findings). Prior to our main multivariate pattern analysis, a conventional univariate analysis <xref rid="bib26" ref-type="bibr">[26]</xref> performed with a general linear model confirmed that there was no significant difference in average brain activity between the two environments or any of the positions even at liberal thresholds, which was as expected given their almost identical macroscopic characteristics (see <xref rid="app2" ref-type="sec">Supplemental Results</xref>).</p>
      <sec id="sec2.1">
        <title>Discriminating between Two Positions</title>
        <p>We first investigated whether we could accurately predict where a participant was located within a room solely from the pattern of fMRI BOLD responses across multiple voxels in the hippocampus and MTL. To do this, we initially made comparisons between arbitrarily selected pairs of positions (A versus B and C versus D) in both rooms. Importantly, after navigation, when participants reached a target position, the default horizontal viewpoint transitioned smoothly downward by 90° so that the entire visual display was occupied solely by an identical view of the floor (<xref rid="fig1" ref-type="fig">Figure 1</xref>C). Critically, only volumes capturing fMRI activity during this stationary phase (<xref rid="fig1" ref-type="fig">Figure 1</xref>D) at the target positions when the participant was viewing the floor were entered into the analysis. This is a key aspect of our study design because visual stimuli such as objects and boundaries are known to be processed by the MTL <xref rid="bib12 bib27 bib28 bib29 bib30" ref-type="bibr">[12, 27–30]</xref>. By removing visual input as a confounding factor, we were thus able to isolate the internal representation of spatial location as the only difference between conditions. Moreover, the task design (see <xref rid="app2" ref-type="sec">Supplemental Data</xref>) controlled for other potential confounding psychological factors during this period, as confirmed in the debriefing. The imaging data were then divided into independent training and test sets (see <xref rid="fig2" ref-type="fig">Figure 2</xref>), with the former used to train a linear support vector machine (SVM) classifier (see <xref rid="sec5" ref-type="sec">Experimental Procedures</xref>). The performance of this classifier was evaluated by running it on the independent test data and obtaining a percentage prediction accuracy value.</p>
        <p>By using a multivariate “searchlight” approach to feature selection <xref rid="bib14 bib17 bib24" ref-type="bibr">[14, 17, 24]</xref>, we stepped through a large search space encompassing the MTL (<xref rid="fig2" ref-type="fig">Figure 2</xref>) and identified spherical cliques of voxels whose spatial patterns of activity enabled the classifier to correctly discriminate between two positions significantly above chance (p &lt; 0.05 uncorrected, by using the statistically conservative approach of nonparametric permutation testing and accounting for the multiple comparisons problem <xref rid="bib31 bib32" ref-type="bibr">[31, 32]</xref>; see <xref rid="sec5" ref-type="sec">Experimental Procedures</xref> and <xref rid="app2" ref-type="sec">Tables S2 and S3</xref>). Voxels at the center of cliques whose accuracies survived this thresholding and were, therefore, important for accurately distinguishing between the two experimental conditions (e.g., position A versus position B) were then reprojected back onto the structural brain image of the participant to produce “prediction maps.” Remarkably, this process revealed large numbers of voxels in the body-posterior of the hippocampus bilaterally that accurately discriminated the position of the participant (<xref rid="fig3" ref-type="fig">Figure 3</xref>).</p>
      </sec>
      <sec id="sec2.2">
        <title>Discriminating between Four Positions</title>
        <p>We next investigated whether there were voxels in the hippocampus capable of discriminating simultaneously between all four target positions in a room. By using the same protocol as above, we performed all six possible pairwise classifiers for each room (comparing positions A versus B, A versus C, A versus D, B versus C, B versus D, and C versus D against each other; see <xref rid="fig1" ref-type="fig">Figure 1</xref>) and combined their results into error correcting output codes from which resultant predictions were determined by computing the nearest Hamming distance to a real label code (see <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref>). Although these four-way classifications are dependent on a linear combination of the pairwise classifications above, they provide distinct information about the data because significant voxel accuracy in pairwise classification does not necessitate significant accuracy in four-way classification. Significant voxels were again reprojected back onto the structural brain image of a participant to produce prediction maps. This revealed a focal cluster of voxels in the body-posterior of the hippocampus bilaterally, which allowed for accurate differentiation between all four positions in a room, again independent of visual input (<xref rid="fig4" ref-type="fig">Figure 4</xref>), a result that was markedly consistent across participants. There were very few discriminating voxels elsewhere in the MTL, thus demonstrating the specific involvement of the hippocampus in representing spatial positions.</p>
      </sec>
      <sec id="sec2.3">
        <title>Discriminating between the Two Environments</title>
        <p>Though spatial positions of the participant within the environment were represented almost exclusively in the hippocampus, our findings also highlighted an interesting dissociation between the hippocampus and parahippocampal gyrus. In a separate multivariate analysis, we tested whether it was possible to accurately predict which environment—the blue or green room—a participant was in during navigation. The prediction maps obtained revealed voxels in the parahippocampal gyrus bilaterally, which allowed for differentiation between environments (<xref rid="fig5" ref-type="fig">Figure 5</xref>). In contrast to the position analysis, minimal numbers of voxels were found in the hippocampus that accurately discriminated between the two environments.</p>
        <p>For each classification type, we formally quantified the differences in numbers of discriminating voxels present in the hippocampus and parahippocampal gyrus, respectively, by performing a difference of population proportions <xref rid="bib33" ref-type="bibr">[33]</xref> significance test on the two anatomically defined regions (see the <xref rid="sec5" ref-type="sec">Experimental Procedures</xref>). For the pairwise and four-way position classifications, we found that there was a significantly higher proportion of voxels active in the hippocampus than the parahippocampal gyrus for all participants (all p &lt; 0.05; see the <xref rid="app2" ref-type="sec">Supplemental Results</xref>). For the environment classification, there was a significantly higher proportion of voxels active in the parahippocampal gyrus than the hippocampus for all participants (all p &lt; 0.05; see the <xref rid="app2" ref-type="sec">Supplemental Results</xref>). Note that these significant findings also mitigate against the multiple comparisons problem; if active voxels were just false positives due to chance, one would expect a uniform distribution of active voxels (see the <xref rid="app2" ref-type="sec">Supplemental Results</xref>).</p>
      </sec>
    </sec>
    <sec id="sec3">
      <title>Discussion</title>
      <p>Our results demonstrate that fine-grained spatial information can be accurately decoded solely from the pattern of fMRI activity across spatially distributed voxels in the human hippocampus. This shows that the population of hippocampal neurons representing place must necessarily be large, robust, and nonuniform. Thus, our findings imply that, contrary to prevailing theories, there may be an underlying functional organization to the hippocampal neural code. Our data also revealed a dissociation, permitting conclusions about anatomical specificity. Whereas spatial positions were expressed in the hippocampus, by contrast, voxels in the parahippocampal gyrus discriminated between the two environments.</p>
      <p>Extending the pairwise position classification findings (<xref rid="fig3" ref-type="fig">Figure 3</xref>) to discriminate between four arbitrary environmental positions (<xref rid="fig4" ref-type="fig">Figure 4</xref>) revealed a region of the hippocampus that is involved in the general storage and/or manipulation of position representations. The involvement of neuronal populations located specifically in the body-posterior of the hippocampus <xref rid="bib19" ref-type="bibr">[19]</xref> as indicated by our data is highly consistent with findings from human and animal studies of spatial memory that use other investigative techniques <xref rid="bib34 bib35 bib36" ref-type="bibr">[34–36]</xref>. Therefore, we propose that these individual abstracted position representations aggregated together form the basis of the allocentric cognitive map <xref rid="bib4" ref-type="bibr">[4]</xref>, or the set of invariant spatial relationships <xref rid="bib37" ref-type="bibr">[37]</xref>, representing the layout of an environment. Due to the constraint that pattern classifiers require a certain number of consistent examples for training purposes <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref>, discrete localized positions had to be used as target locations. However, there is nothing special about the target locations used in this study; any positions in the rooms could have been chosen. Indeed, within each target location, a participant's stationary position varied subtly trial by trial, given that the target area measured 1.5 m × 1.5 m in size. Thus, we suggest that the spatial code for an environment is likely to be continuous, with subtle differences in the neuronal code between adjacent positions.</p>
      <p>The volumes acquired during an environment block while in the blue or green room (see <xref rid="fig1" ref-type="fig">Figure 1</xref>D) comprised fMRI activity from a large number of different “snapshot” views of a room at numerous spatial positions within it (not only our four target positions). Hence, we believe that the classifier operating on hippocampal voxels did not discriminate between the two environments because this would have necessitated these voxels to have identifiably similar patterns of activity across environment block volumes (i.e., volumes acquired while in the blue or green room). However, hippocampal voxels were instead acutely tuned to individual spatial positions within a block and, therefore, displayed differing patterns of activity during navigation in an environment block that encompassed numerous spatial positions. By contrast, it is clear that the parahippocampal gyrus performed a distinct but complementary function. We speculate that this may have involved extracting the salient contextual features of each environment <xref rid="bib27 bib29" ref-type="bibr">[27, 29]</xref>, such as object-in-place associations <xref rid="bib28" ref-type="bibr">[28]</xref> and orienting wall object configurations from multiple visual snapshots for input to the hippocampal place representations <xref rid="bib30" ref-type="bibr">[30]</xref>. Thus, the classifier operating on parahippocampal gyrus voxels was able to discriminate between the two environments, although we cannot exclude the possibility that this region might have also been sensitive to the color differences between the two environments. Further studies will be needed to ascertain the exact nature and function of the representations in the parahippocampal gyrus during navigation and, indeed, in other neocortical areas such as the prefrontal and parietal cortices, which are also known to be involved in navigation <xref rid="bib38" ref-type="bibr">[38]</xref> but were outside of the scanning coverage of this study.</p>
      <p>The rigorous design of our paradigm—in particular, the careful matching of visual input at the destination locations, the counterbalancing of starting and destination location combinations, and the use of an incidental visual task to maintain attention during the stationary phase—allows us to conclude that any informative patterns of voxels found by our multivariate analyses must code for the internal representation of spatial location only and not for any other aspects of the task. In addition to these design features, our analysis was robust to any residual cognitive differences that may conceivably have occurred. Classifiers can be thought of as distinguishing between learned commonalities across multiple training examples of two experimental conditions. Therefore, in order for the classifier to successfully decode brain activity, the difference between two conditions must be systematic and consistent across the majority of the training examples. We carefully designed the paradigm to ensure that the only possible systematic difference between stationary periods was the internal representation of the current position. This was further confirmed by a number of additional control analyses that were performed to ensure that other factors such as the identity of the destination labels themselves or nearby orienting objects could not have significantly contributed to the successful decoding (see the <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref> and <xref rid="app2" ref-type="sec">Supplemental Results</xref>).</p>
      <p>Hence, it is with some confidence that we can say that the hippocampal voxels that survived the rigorously controlled thresholding that we employed were associated with internal representations of position within the environment alone. A further point to note, specifically in relation to the effect of previously seen landmarks on the BOLD signal during the stationary phase, is that paths and approaches taken to target positions were not identical across trials and the timings of any views of landmarks en route varied widely. The effect of such substantial variability in paths to the target position in effect introduced a self-paced random jitter with respect to the influence of any landmarks seen on the BOLD signal during the stationary periods. Therefore, landmarks cannot be a contributing factor to the successful performance of the classifier on the position discrimination (see the <xref rid="app2" ref-type="sec">Supplemental Results</xref>).</p>
      <p>Our finding that it is possible to distinguish between well-matched spatial positions with human fMRI has significant implications for understanding the neuronal population code in the hippocampus. It has been proposed that information is encoded in the brain as a sequence of cell assemblies, with each activated clique encapsulating a fundamental unit of information <xref rid="bib1 bib2 bib3" ref-type="bibr">[1–3]</xref>. Cell assembly synchronization is thought to take place over timescales of ∼30 ms <xref rid="bib2" ref-type="bibr">[2]</xref>, in contrast to the time frame of human neuroimaging, which measures activity averaged over ∼6 s. Although the BOLD signal is only an indirect measure of neuronal activity and there is ongoing debate about the relationship between the two <xref rid="bib39" ref-type="bibr">[39]</xref>, there is a robust correlation between BOLD responses and local field potentials <xref rid="bib39 bib40" ref-type="bibr">[39, 40]</xref>. Therefore, patterns of voxel activations acquired during a single fMRI volume and capable of discriminating between well-matched positions are likely to reflect the average synaptic activity within many cell assemblies that, taken together, can represent high-level information such as spatial location within an environment.</p>
      <p>Although neural codes in the hippocampus and wider MTL are generally considered to be “sparse” <xref rid="bib12 bib41" ref-type="bibr">[12, 41]</xref>, that term has been used to describe a wide range of different representational scales, from single “grandmother” cells <xref rid="bib42" ref-type="bibr">[42]</xref> to more than two million cells in other accounts <xref rid="bib41" ref-type="bibr">[41]</xref>. The human hippocampus contains ∼40 million principal neurons <xref rid="bib19" ref-type="bibr">[19]</xref>, and even at the high spatial resolution of the scanning employed here, this translates to ∼10<sup>4</sup> neurons per voxel. Given the relatively coarse and noisy nature of human neuroimaging in both the temporal and spatial domains, it is striking that it was possible to robustly distinguish between positions of a participant in the environment that vary in only subtle ways. To the extent that multivariate classification with fMRI reflects biased sampling of a distributed anisotropic neuronal representation <xref rid="bib16" ref-type="bibr">[16]</xref>, our results are consistent with the notion that hippocampal neuronal ensembles representing place memories are large and have an anisotropic predictable structure. Moreover, the prediction maps that we obtained indicated the presence of information sufficient to decode position from voxels distributed spatially throughout the hippocampus. Our data, therefore, are broadly supportive of two previous invasive studies that have suggested that there may be some form of clustering <xref rid="bib23" ref-type="bibr">[23]</xref> or topographical functional organization <xref rid="bib22" ref-type="bibr">[22]</xref> in the hippocampus. Although numerous invasive studies have reported that the population code is random and uniformly distributed <xref rid="bib10 bib11" ref-type="bibr">[10, 11]</xref>, a point often implicitly assumed by computational models <xref rid="bib20 bib21" ref-type="bibr">[20, 21]</xref>, this would result in uniform patterns of activity at the voxel level, thus rendering classification impossible <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref>. However, there are ways in which these opposing views and our findings can be potentially reconciled. For instance, the spacing of tetrodes randomly sampling single neurons <xref rid="bib11" ref-type="bibr">[11]</xref> could be out of phase with the structure of the underlying functional organization <xref rid="bib22" ref-type="bibr">[22]</xref>. Disparate findings might also arise from differences in the clustering analyses used (see <xref rid="bib23" ref-type="bibr">[23]</xref> compared with <xref rid="bib11" ref-type="bibr">[11]</xref>). The effect of cell assembly synchronization on single-cell spike output may also be a contributing factor but is, as yet, largely unknown <xref rid="bib2" ref-type="bibr">[2]</xref>.</p>
    </sec>
    <sec id="sec4">
      <title>Conclusions</title>
      <p>Here, we focused on the cross-species behavior of navigation, demonstrating that highly abstracted representations of space are expressed across tens of thousands of coordinated neurons in the human hippocampus in a structured manner. In so doing, we have shown that, contrary to current consensus, neuronal ensembles representing place memories must be large, stable, and have an anisotropic structure. Spatial representations of the type investigated here have been suggested to form the scaffold upon which episodic memories are built <xref rid="bib4 bib30 bib43" ref-type="bibr">[4, 30, 43]</xref>, but the precise mechanism by which the hippocampus achieves this is still unknown. This crucial question is difficult to address in nonhumans, wherein even the existence of episodic memory has been challenged <xref rid="bib44" ref-type="bibr">[44]</xref>. By showing that it is possible to detect and discriminate between memories of adjacent spatial positions, our combination of noninvasive in vivo high-resolution fMRI and multivariate analyses opens up a new avenue for exploring episodic memory at the population level. In the future, it may be feasible to decode individual episodic memory traces from the activity of neuronal ensembles in the human hippocampus. This brings ever closer the tantalizing prospect of discovering how a person's lifetime of experiences is coded by the neurons of the brain.</p>
    </sec>
    <sec sec-type="materials-methods" id="sec5">
      <title>Experimental Procedures</title>
      <sec id="sec5.1">
        <title>Participants</title>
        <p>Four healthy right-handed males with prior experience of playing first-person video games participated in the experiment (mean age 24.3 years, SD 3.2, age range 21–27). All had normal or corrected-to-normal vision. All participants gave informed written consent to participate in accordance with the local research ethics committee.</p>
      </sec>
      <sec id="sec5.2">
        <title>Task and Stimuli</title>
        <p>During scanning, participants were required to navigate as quickly as possible between four arbitrary target locations in two different virtual reality environments (<xref rid="fig1" ref-type="fig">Figure 1</xref>). The virtual reality environment was implemented with a modified version of the graphics engine used in the video game Fable (<ext-link xlink:href="http://www.lionhead.com/fable/index.html" ext-link-type="uri">http://www.lionhead.com/fable/index.html</ext-link>). The room interiors were designed in the architectural package Sketch-up (<ext-link xlink:href="http://sketchup.google.com" ext-link-type="uri">http://sketchup.google.com</ext-link>) and imported into the graphics engine. The code for the environment, controls, and scanner pulse synchronization was written in C++ with Microsoft Visual Studio (<ext-link xlink:href="http://msdn.microsoft.com/en-gb/vstudio/products/default.aspx" ext-link-type="uri">http://msdn.microsoft.com/en-gb/vstudio/products/default.aspx</ext-link>). Participants controlled their movement through the environment with a four-button MRI-compatible control pad. The buttons were configured to move forward, rotate left, rotate right, and signal that a target destination had been reached. Participants were extensively trained in the VR environments prior to scanning (for details of the prescan training procedure, see the <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref>). Each room was 15 m × 15 m, and perspective was set at the height of an average person, around 1.8 m above ground. The four target positions (A, B, C, and D) were situated 3 m in from the corners and visually delineated by identical cloth rugs. Each rug (and hence each target area) was 1.5 m × 1.5 m. Identical small square tables were placed in each corner to aid visibility and were irrelevant as cues for the navigation task. The two rooms were matched in terms of size, shape, luminosity, emotional salience, contents, and floor color. The rooms were designed so that spatial relationships between neighboring object categories as well as the target position labels were orthogonal for each room. Participants navigated through the rooms at a fast walking speed of 1.9 m/s. It was important for movement to be at a realistic speed and under participant control because self-motion is thought to play an important part in the spatial updating process <xref rid="bib30 bib45" ref-type="bibr">[30, 45]</xref>. Hence, the use of interactive virtual reality was highly suited for extraction of position information that was as ecologically valid as possible.</p>
        <p>Once a target location was reached, the viewpoint transitioned downward so that the identical floor texture occupied the entire field of view, thus ensuring that visual input was matched perfectly across positions. At this point, a 5 s countdown was given, followed by the letter of the next location, displayed for 2 s, during which time the participant was stationary and viewing the floor (“stationary phase”). The viewpoint then transitioned back to the horizontal, and the participant navigated to the next location as quickly and accurately as possible. Navigation blocks consisting of two to four individual trials were interspersed with a 13 s period of rest, during which a fixation cross was presented on a plain black screen. The label of the next target position was then displayed for 2 s before the participant was placed anew in one of the rooms with his back facing the closed door as if he had just entered the room. The trial and room orders were pseudorandomized and fully counterbalanced across participants. Each environment (i.e., blue or green room) was visited 20 times during the scanning session, giving 40 environment blocks in total. Within each room, every target position was visited 14 times, giving 112 trials in total. In order to maintain attention during the stationary countdown period, catch trials were included that involved an incidental visual task. The countdown numbers were displayed in white text, but occasionally one would flash red for 200 ms. Participants were instructed to press the trigger button as quickly as possible upon spotting a red number. There were eight catch trials spread throughout the scanning session—one at each target position and always at the end of a block. The volumes acquired during these catch trials were excluded from the analyses. After scanning, participants were debriefed and asked about the navigational strategies that they adopted (for details of the postscan debriefing procedure, see the <xref rid="app2" ref-type="sec">Supplemental Data</xref>).</p>
      </sec>
      <sec id="sec5.3">
        <title>Image Acquisition</title>
        <p>A 3T Magnetom Allegra head scanner (Siemens Medical Solutions, Erlangen, Germany) operated with the standard transmit-receive head coil was used to acquire functional data with a T2<sup>∗</sup>-weighted single-shot echo-planar imaging (EPI) sequence (in-plane resolution = 1.5 × 1.5 mm<sup>2</sup>; matrix = 128 × 128; field of view = 192 × 192 mm<sup>2</sup>; 35 slices acquired in an interleaved order; slice thickness = 1.5 mm with no gap between slices; echo time TE = 30 ms; asymmetric echo shifted forward by 26 phase-encoding (PE) lines; echo spacing = 560 μs; repetition time TR = 3.57 s; flip angle α = 90°). All data were recorded in one single uninterrupted functional scanning session (total volumes acquired for each participant: s1 636 volumes; s2 640 volumes; s3 658 volumes; s4 670 volumes). An isotropic voxel size of 1.5 × 1.5 × 1.5 mm<sup>3</sup> was chosen for an optimal tradeoff between BOLD sensitivity and spatial resolution. Further, the isotropic voxel dimension reduced resampling artifacts when applying motion correction. In order to minimize repetition time while also optimizing coverage of the regions of interest in the medial temporal lobe, we captured partial functional volumes angled at 5° in the anterior-posterior axis (see <xref rid="fig2" ref-type="fig">Figure 2</xref>B). Susceptibility induced loss of BOLD sensitivity in the medial temporal lobe was intrinsically reduced by the high spatial resolution and adjusting the EPI parameters for the given slice tilt (z-shim gradient prepulse moment = 0 mT/m × ms; positive PE polarity). A T1-weighted, high-resolution, whole-brain structural MRI scan was acquired for each participant after the main scanning session (1 mm isotropic resolution, 3D MDEFT).</p>
      </sec>
      <sec id="sec5.4">
        <title>Imaging Data Preprocessing</title>
        <p>This consisted of realignment to correct for motion effects and minimal spatial smoothing with a 3 mm FWHM Gaussian kernel. The first six “dummy” volumes were discarded to allow for T1 equilibration effects <xref rid="bib26" ref-type="bibr">[26]</xref>.</p>
      </sec>
      <sec id="sec5.5">
        <title>Multivariate Pattern Classification</title>
        <p>A standard univariate statistical analysis was performed with a general linear model implemented in SPM5 (<ext-link xlink:href="http://www.fil.ion.ucl.ac.uk/spm" ext-link-type="uri">www.fil.ion.ucl.ac.uk/spm</ext-link>) (for details of this analysis, see the <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref>). We then performed a multivariate pattern analysis <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref> designed to identify brain regions where distributed fMRI activation patterns carried information about the environment that a participant was in or information about his position in that environment. A linear detrend was run on the preprocessed images to remove any noise due to scanner drift or other possible background sources <xref rid="bib46" ref-type="bibr">[46]</xref>. Next, we convolved the image data with the canonical hemodynamic response function to increase the signal-to-noise ratio effectively acting as a low-pass filter <xref rid="bib26" ref-type="bibr">[26]</xref>. BOLD signal has an inherent delay of around 6 s to peak response relative to stimulus onset due to the hemodynamic response function <xref rid="bib26" ref-type="bibr">[26]</xref>, and applying this convolution in effect delayed the peak by another 6 s, giving a total delay of 12 s. To best compensate for this delay, all onset times were shifted forward in time by three volumes, yielding the best approximation to the 12 s delay given a TR of 3.57 s and rounding to the nearest volume <xref rid="bib13 bib14" ref-type="bibr">[13, 14]</xref>. The first volume and the last four volumes of each environmental block were discarded to allow for any orientation effects to settle (due to appearing suddenly in a room) and to exclude catch trials (always at the end of a block when present). Three separate multivariate classifications were carried out to (1) discriminate between which of two target positions in a single room the participant was standing (“pairwise”), (2) discriminate between all four target positions in a single room (“four-way”), and (3) discriminate between which of the two room environments the participant was in (“environment”). The same technique, described next, was used in all three types of classification.</p>
        <p>In order to search in an unbiased fashion for informative voxels and maximize sensitivity, we used a novel variant of the “searchlight” approach <xref rid="bib17 bib24" ref-type="bibr">[17, 24]</xref>, a multivariate feature selection method <xref rid="bib14 bib24" ref-type="bibr">[14, 24]</xref> that examines the information in the local spatial patterns surrounding each voxel v<sub>i</sub> (<xref rid="fig2" ref-type="fig">Figure 2</xref>). This approach has another important advantage in that it results in statistical maps that allow for the anatomical mapping of the spatial pattern of informative voxels to be appreciated. Thus, for each v<sub>i</sub>, we investigated whether its local environment contained information that would allow accurate decoding of the current position. For a given voxel v<sub>i</sub>, we first defined a small spherical clique of N voxels c<sub>1.N</sub> with a radius of three voxels centered on v<sub>i</sub>. A radius of three voxels was reported to be the optimal size for a clique by Kriegeskorte et al. <xref rid="bib24" ref-type="bibr">[24]</xref>, although this may be partially dependent on the resolution of the acquired images. For each voxel c<sub>1.N</sub> in the fixed local clique, we extracted the voxel intensity from each image, yielding an N-dimensional pattern vector for each image. Multivariate pattern recognition was then used to assess how much position and environment information was encoded in these local pattern vectors. This was achieved by splitting the image data (now in the form of pattern vectors) into two segments: a “training” set used to train a linear support vector pattern classifier (with fixed regularization hyperparameter C = 1) to identify response patterns related to the two conditions being discriminated and a “test” set used to independently test the classification performance. The classification was performed with a support vector machine (SVM) <xref rid="bib47" ref-type="bibr">[47]</xref> by using the LIBSVM implementation (<ext-link xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" ext-link-type="uri">http://www.csie.ntu.edu.tw/∼cjlin/libsvm/</ext-link>). We used a standard k fold crossvalidation testing regime <xref rid="bib13 bib15 bib47" ref-type="bibr">[13, 15, 47]</xref>, wherein k equaled the number of blocks, with each block set aside, in turn, as the test data and the rest of the blocks used to train the classifier (see <xref rid="fig2" ref-type="fig">Figure 2</xref>F). This procedure was then repeated until all blocks had been assigned once as the test data (the crossvalidation step). Thus, the pairwise position classification involved a 28-fold crossvalidation step (14 position X and 14 position Y miniblocks of two volumes each; because the stationary phase lasted 7 s and the scanning repetition time was 3.57 s, this consisted of the two volumes immediately following the onset of the stationary period); the four-way position classification involved a 56-fold crossvalidation step (14 miniblocks of length two volumes for each of four positions); and the environment classification involved a 40-fold crossvalidation step (20 blue room and 20 green room blocks, with an average of seven volumes per block). Every volume within a test block was individually classified following the crossvalidation step, thus yielding an overall percentage accuracy for the clique centered around voxel v<sub>i</sub> for all of the volumes in the entire experimental session (see <xref rid="fig2" ref-type="fig">Figure 2</xref>G). This decoding accuracy was stored with voxel v<sub>i</sub> for subsequent reprojection as a “prediction map” (see “Reprojection and Thresholding” below), and the entire procedure was repeated on a voxel-by-voxel basis until all voxels in a previously defined region of interest had been considered. In this case, the search regions were anatomically defined with two large rectangular bounding boxes (each composed of 6750 voxels; see <xref rid="fig2" ref-type="fig">Figure 2</xref>B) covering both the right and left medial temporal lobes and thus encompassing our apriori regions of interest, i.e., the hippocampus and parahippocampal gyrus. Good overall classification accuracy for a voxel v<sub>i</sub> implies that patterns in the surrounding local clique of voxels encode information about the current position and environment of the participant. A final multiclass classification procedure was performed for the four-way position classification (see the <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref> for details).</p>
      </sec>
      <sec id="sec5.6">
        <title>Reprojection and Thresholding</title>
        <p>Once the classifications were completed and the decoding accuracies were stored for each voxel in the search region, we proceeded to reproject these values back into structural brain image space to allow the resultant prediction maps to be visually inspected. These prediction maps were then thresholded at a percentage accuracy value that was significantly above that expected by chance. This significance threshold was determined by using the classical method of nonparametric permutation testing <xref rid="bib31 bib32" ref-type="bibr">[31, 32]</xref>, requiring minimal assumptions (for example, about the shape of the population distribution) for validity. The entire classification procedure outlined above was performed 100 times with a different random permutation of the training labels for each classification type for each participant. The individual voxel accuracy values from each of these 100 random runs were then concatenated into one population, and the accuracy value at the 95th percentile of this aggregated distribution was calculated. Therefore, this procedure yielded a percentage accuracy value for each individual participant, above which a voxel's accuracy was considered significant, equating to a confidence level of p &lt; 0.05 uncorrected in a standard t test.</p>
        <p>We accounted for the multiple comparisons problem by performing a standard test for the difference between two population proportions <xref rid="bib33" ref-type="bibr">[33]</xref>. If significant voxels are false positives due to random variation, the proportion of significant voxels should be uniform over the entire search space (see <xref rid="fig2" ref-type="fig">Figure 2</xref>). To test this null hypothesis, we created two anatomical masks, one covering the hippocampus bilaterally and the other the parahippocampal gyrus bilaterally, for each individual participant by hand with MRIcro (<ext-link xlink:href="http://www.sph.sc.edu/comd/rorden/mricro.html" ext-link-type="uri">http://www.sph.sc.edu/comd/rorden/mricro.html</ext-link>) by using each participant's structural MRI scan for guidance. The proportion of significant voxels for each region was determined (i.e., active voxels/total voxels) for each prediction map (i.e., pairwise position, four-way position in blue room, four-way position in green room, and environment) for each participant. A two-tailed test for difference between proportions was performed for each of the prediction maps to determine whether the proportion of active voxels in the hippocampus was significantly different from that in the parahippocampal gyrus.</p>
        <p>In addition to the voxel count difference of proportions test described above, we used a second analytic approach to test for a region x classification type interaction between the hippocampus and parahippocampal gyrus. We assessed the informational content of BOLD signals for each type of classification (i.e., pairwise position and context) for both regions. A classifier trained on A versus B in the blue room (comprising primarily hippocampal voxels; see <xref rid="fig3" ref-type="fig">Figure 3</xref>) was tested on discriminating between the blue room and green room contexts. Conversely, a classifier trained on blue versus green room context (comprising primarily parahippocampal voxels; see <xref rid="fig5" ref-type="fig">Figure 5</xref>) was tested on discriminating between positions A versus B in the blue room. To enable comparison, we generated a single accuracy value for each classifier (see the <xref rid="app2" ref-type="sec">Supplemental Experimental Procedures</xref> and <xref rid="app2" ref-type="sec">Supplemental Results</xref>).</p>
      </sec>
    </sec>
    <sec id="app1" sec-type="supplementary-material">
      <title>Supplemental Data</title>
      <p>Supplemental Data include Supplemental Experimental Procedures, Supplemental Results, five tables, and one figure and can be found with this article online at <ext-link xlink:href="http://www.current-biology.com/supplemental/S0960-9822(09)00741-6" ext-link-type="uri">http://www.current-biology.com/supplemental/S0960-9822(09)00741-6</ext-link>.</p>
    </sec>
    <sec id="app2" sec-type="supplementary-material">
      <title>Supplemental Data</title>
      <p>
        <supplementary-material content-type="local-data" id="mmc1">
          <caption>
            <title>Document S1. Supplemental Experimental Procedures, Supplemental Results, Five Tables, and One Figure</title>
          </caption>
          <media xlink:href="mmc1.pdf" mimetype="application" mime-subtype="pdf"/>
        </supplementary-material>
      </p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>This work was supported by the Wellcome Trust and the Brain Research Trust. We thank K. Friston, R. Dolan, N. Burgess, P. Dayan, J. Mourao-Miranda, J.D. Haynes, J. Ashburner, T. Niccoli, C. Barry, M. Bicknell, and D. Kumaran for helpful discussions and R. Davis, P. Aston, D. Bradbury, J. Glensman, A. Perrotin, N. Furl, D. Mobbs, and A. Langridge for technical assistance.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="bib1">
        <label>1</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hebb</surname>
              <given-names>D.O.</given-names>
            </name>
          </person-group>
          <article-title>The Organization of Behavior</article-title>
          <year>1949</year>
          <publisher-name>Wiley</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
      <ref id="bib2">
        <label>2</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Harris</surname>
              <given-names>K.D.</given-names>
            </name>
          </person-group>
          <article-title>Neural signatures of cell assembly organization</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2005</year>
          <volume>6</volume>
          <fpage>399</fpage>
          <lpage>407</lpage>
          <pub-id pub-id-type="pmid">15861182</pub-id>
        </citation>
      </ref>
      <ref id="bib3">
        <label>3</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buzsaki</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale recording of neuronal ensembles</article-title>
          <source>Nat. Neurosci.</source>
          <year>2004</year>
          <volume>7</volume>
          <fpage>446</fpage>
          <lpage>451</lpage>
          <pub-id pub-id-type="pmid">15114356</pub-id>
        </citation>
      </ref>
      <ref id="bib4">
        <label>4</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>O'Keefe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Nadel</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>The hippocampus as a cognitive map</article-title>
          <year>1978</year>
          <publisher-name>Oxford University Press</publisher-name>
          <publisher-loc>Oxford, England</publisher-loc>
        </citation>
      </ref>
      <ref id="bib5">
        <label>5</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ekstrom</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Kahana</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Caplan</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Fields</surname>
              <given-names>T.A.</given-names>
            </name>
            <name>
              <surname>Isham</surname>
              <given-names>E.A.</given-names>
            </name>
            <name>
              <surname>Newman</surname>
              <given-names>E.L.</given-names>
            </name>
            <name>
              <surname>Fried</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Cellular networks underlying human spatial navigation</article-title>
          <source>Nature</source>
          <year>2003</year>
          <volume>425</volume>
          <fpage>184</fpage>
          <lpage>188</lpage>
          <pub-id pub-id-type="pmid">12968182</pub-id>
        </citation>
      </ref>
      <ref id="bib6">
        <label>6</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moser</surname>
              <given-names>E.I.</given-names>
            </name>
            <name>
              <surname>Kropff</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Moser</surname>
              <given-names>M.B.</given-names>
            </name>
          </person-group>
          <article-title>Place cells, grid cells, and the brain's spatial representation system</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <year>2008</year>
          <volume>31</volume>
          <fpage>69</fpage>
          <lpage>89</lpage>
          <pub-id pub-id-type="pmid">18284371</pub-id>
        </citation>
      </ref>
      <ref id="bib7">
        <label>7</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wilson</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>McNaughton</surname>
              <given-names>B.L.</given-names>
            </name>
          </person-group>
          <article-title>Dynamics of the hippocampal ensemble code for space</article-title>
          <source>Science</source>
          <year>1993</year>
          <volume>261</volume>
          <fpage>1055</fpage>
          <lpage>1058</lpage>
          <pub-id pub-id-type="pmid">8351520</pub-id>
        </citation>
      </ref>
      <ref id="bib8">
        <label>8</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Osan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Shoham</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zuo</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tsien</surname>
              <given-names>J.Z.</given-names>
            </name>
          </person-group>
          <article-title>Identification of network-level coding units for real-time representation of episodic experiences in the hippocampus</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2005</year>
          <volume>102</volume>
          <fpage>6125</fpage>
          <lpage>6130</lpage>
          <pub-id pub-id-type="pmid">15833817</pub-id>
        </citation>
      </ref>
      <ref id="bib9">
        <label>9</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dombeck</surname>
              <given-names>D.A.</given-names>
            </name>
            <name>
              <surname>Khabbaz</surname>
              <given-names>A.N.</given-names>
            </name>
            <name>
              <surname>Collman</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Adelman</surname>
              <given-names>T.L.</given-names>
            </name>
            <name>
              <surname>Tank</surname>
              <given-names>D.W.</given-names>
            </name>
          </person-group>
          <article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title>
          <source>Neuron</source>
          <year>2007</year>
          <volume>56</volume>
          <fpage>43</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">17920014</pub-id>
        </citation>
      </ref>
      <ref id="bib10">
        <label>10</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guzowski</surname>
              <given-names>J.F.</given-names>
            </name>
            <name>
              <surname>McNaughton</surname>
              <given-names>B.L.</given-names>
            </name>
            <name>
              <surname>Barnes</surname>
              <given-names>C.A.</given-names>
            </name>
            <name>
              <surname>Worley</surname>
              <given-names>P.F.</given-names>
            </name>
          </person-group>
          <article-title>Environment-specific expression of the immediate-early gene Arc in hippocampal neuronal ensembles</article-title>
          <source>Nat. Neurosci.</source>
          <year>1999</year>
          <volume>2</volume>
          <fpage>1120</fpage>
          <lpage>1124</lpage>
          <pub-id pub-id-type="pmid">10570490</pub-id>
        </citation>
      </ref>
      <ref id="bib11">
        <label>11</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Redish</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Battaglia</surname>
              <given-names>F.P.</given-names>
            </name>
            <name>
              <surname>Chawla</surname>
              <given-names>M.K.</given-names>
            </name>
            <name>
              <surname>Ekstrom</surname>
              <given-names>A.D.</given-names>
            </name>
            <name>
              <surname>Gerrard</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Lipa</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Rosenzweig</surname>
              <given-names>E.S.</given-names>
            </name>
            <name>
              <surname>Worley</surname>
              <given-names>P.F.</given-names>
            </name>
            <name>
              <surname>Guzowski</surname>
              <given-names>J.F.</given-names>
            </name>
            <name>
              <surname>McNaughton</surname>
              <given-names>B.L.</given-names>
            </name>
            <name>
              <surname>Barnes</surname>
              <given-names>C.A.</given-names>
            </name>
          </person-group>
          <article-title>Independence of firing correlates of anatomically proximate hippocampal pyramidal cells</article-title>
          <source>J. Neurosci.</source>
          <year>2001</year>
          <volume>21</volume>
          <fpage>RC134</fpage>
          <pub-id pub-id-type="pmid">11222672</pub-id>
        </citation>
      </ref>
      <ref id="bib12">
        <label>12</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quiroga</surname>
              <given-names>R.Q.</given-names>
            </name>
            <name>
              <surname>Reddy</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Koch</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fried</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Invariant visual representation by single neurons in the human brain</article-title>
          <source>Nature</source>
          <year>2005</year>
          <volume>435</volume>
          <fpage>1102</fpage>
          <lpage>1107</lpage>
          <pub-id pub-id-type="pmid">15973409</pub-id>
        </citation>
      </ref>
      <ref id="bib13">
        <label>13</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Decoding mental states from brain activity in humans</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2006</year>
          <volume>7</volume>
          <fpage>523</fpage>
          <lpage>534</lpage>
          <pub-id pub-id-type="pmid">16791142</pub-id>
        </citation>
      </ref>
      <ref id="bib14">
        <label>14</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Norman</surname>
              <given-names>K.A.</given-names>
            </name>
            <name>
              <surname>Polyn</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Detre</surname>
              <given-names>G.J.</given-names>
            </name>
            <name>
              <surname>Haxby</surname>
              <given-names>J.V.</given-names>
            </name>
          </person-group>
          <article-title>Beyond mind-reading: Multi-voxel pattern analysis of fMRI data</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2006</year>
          <volume>10</volume>
          <fpage>424</fpage>
          <lpage>430</lpage>
          <pub-id pub-id-type="pmid">16899397</pub-id>
        </citation>
      </ref>
      <ref id="bib15">
        <label>15</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex</article-title>
          <source>Nat. Neurosci.</source>
          <year>2005</year>
          <volume>8</volume>
          <fpage>686</fpage>
          <lpage>691</lpage>
          <pub-id pub-id-type="pmid">15852013</pub-id>
        </citation>
      </ref>
      <ref id="bib16">
        <label>16</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kamitani</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Decoding the visual and subjective contents of the human brain</article-title>
          <source>Nat. Neurosci.</source>
          <year>2005</year>
          <volume>8</volume>
          <fpage>679</fpage>
          <lpage>685</lpage>
          <pub-id pub-id-type="pmid">15852014</pub-id>
        </citation>
      </ref>
      <ref id="bib17">
        <label>17</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haynes</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Sakai</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Rees</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Gilbert</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Passingham</surname>
              <given-names>R.E.</given-names>
            </name>
          </person-group>
          <article-title>Reading hidden intentions in the human brain</article-title>
          <source>Curr. Biol.</source>
          <year>2007</year>
          <volume>17</volume>
          <fpage>323</fpage>
          <lpage>328</lpage>
          <pub-id pub-id-type="pmid">17291759</pub-id>
        </citation>
      </ref>
      <ref id="bib18">
        <label>18</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Polyn</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Natu</surname>
              <given-names>V.S.</given-names>
            </name>
            <name>
              <surname>Cohen</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Norman</surname>
              <given-names>K.A.</given-names>
            </name>
          </person-group>
          <article-title>Category-specific cortical activity precedes retrieval during memory search</article-title>
          <source>Science</source>
          <year>2005</year>
          <volume>310</volume>
          <fpage>1963</fpage>
          <lpage>1966</lpage>
          <pub-id pub-id-type="pmid">16373577</pub-id>
        </citation>
      </ref>
      <ref id="bib19">
        <label>19</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Andersen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Amaral</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Bliss</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>O'Keefe</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>The Hippocampus Book</article-title>
          <year>2007</year>
          <publisher-name>Oxford University Press</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
      <ref id="bib20">
        <label>20</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hartley</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Burgess</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lever</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Cacucci</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>O'Keefe</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Modeling place fields in terms of the cortical inputs to the hippocampus</article-title>
          <source>Hippocampus</source>
          <year>2000</year>
          <volume>10</volume>
          <fpage>369</fpage>
          <lpage>379</lpage>
          <pub-id pub-id-type="pmid">10985276</pub-id>
        </citation>
      </ref>
      <ref id="bib21">
        <label>21</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Samsonovich</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>McNaughton</surname>
              <given-names>B.L.</given-names>
            </name>
          </person-group>
          <article-title>Path integration and cognitive mapping in a continuous attractor neural network model</article-title>
          <source>J. Neurosci.</source>
          <year>1997</year>
          <volume>17</volume>
          <fpage>5900</fpage>
          <lpage>5920</lpage>
          <pub-id pub-id-type="pmid">9221787</pub-id>
        </citation>
      </ref>
      <ref id="bib22">
        <label>22</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hampson</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Simeral</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Deadwyler</surname>
              <given-names>S.A.</given-names>
            </name>
          </person-group>
          <article-title>Distribution of spatial and nonspatial information in dorsal hippocampus</article-title>
          <source>Nature</source>
          <year>1999</year>
          <volume>402</volume>
          <fpage>610</fpage>
          <lpage>614</lpage>
          <pub-id pub-id-type="pmid">10604466</pub-id>
        </citation>
      </ref>
      <ref id="bib23">
        <label>23</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eichenbaum</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wiener</surname>
              <given-names>S.I.</given-names>
            </name>
            <name>
              <surname>Shapiro</surname>
              <given-names>M.L.</given-names>
            </name>
            <name>
              <surname>Cohen</surname>
              <given-names>N.J.</given-names>
            </name>
          </person-group>
          <article-title>The organization of spatial coding in the hippocampus: A study of neural ensemble activity</article-title>
          <source>J. Neurosci.</source>
          <year>1989</year>
          <volume>9</volume>
          <fpage>2764</fpage>
          <lpage>2775</lpage>
          <pub-id pub-id-type="pmid">2769365</pub-id>
        </citation>
      </ref>
      <ref id="bib24">
        <label>24</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kriegeskorte</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Goebel</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bandettini</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Information-based functional brain mapping</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2006</year>
          <volume>103</volume>
          <fpage>3863</fpage>
          <lpage>3868</lpage>
          <pub-id pub-id-type="pmid">16537458</pub-id>
        </citation>
      </ref>
      <ref id="bib25">
        <label>25</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gegenfurtner</surname>
              <given-names>K.R.</given-names>
            </name>
            <name>
              <surname>Kiper</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Color vision</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <year>2003</year>
          <volume>26</volume>
          <fpage>181</fpage>
          <lpage>206</lpage>
          <pub-id pub-id-type="pmid">12574494</pub-id>
        </citation>
      </ref>
      <ref id="bib26">
        <label>26</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Frackowiak</surname>
              <given-names>R.S.J.</given-names>
            </name>
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>C.D.</given-names>
            </name>
            <name>
              <surname>Dolan</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Price</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>Zeki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ashburner</surname>
              <given-names>J.T.</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>W.D.</given-names>
            </name>
          </person-group>
          <article-title>Human Brain Function</article-title>
          <year>2004</year>
          <publisher-name>Elsevier Academic Press</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
      <ref id="bib27">
        <label>27</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Epstein</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kanwisher</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>A cortical representation of the local visual environment</article-title>
          <source>Nature</source>
          <year>1998</year>
          <volume>392</volume>
          <fpage>598</fpage>
          <lpage>601</lpage>
          <pub-id pub-id-type="pmid">9560155</pub-id>
        </citation>
      </ref>
      <ref id="bib28">
        <label>28</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Janzen</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>van Turennout</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Selective neural representation of objects relevant for navigation</article-title>
          <source>Nat. Neurosci.</source>
          <year>2004</year>
          <volume>7</volume>
          <fpage>673</fpage>
          <lpage>677</lpage>
          <pub-id pub-id-type="pmid">15146191</pub-id>
        </citation>
      </ref>
      <ref id="bib29">
        <label>29</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bar</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Visual objects in context</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2004</year>
          <volume>5</volume>
          <fpage>617</fpage>
          <lpage>629</lpage>
          <pub-id pub-id-type="pmid">15263892</pub-id>
        </citation>
      </ref>
      <ref id="bib30">
        <label>30</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bird</surname>
              <given-names>C.M.</given-names>
            </name>
            <name>
              <surname>Burgess</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>The hippocampus and memory: Insights from spatial processing</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2008</year>
          <volume>9</volume>
          <fpage>182</fpage>
          <lpage>194</lpage>
          <pub-id pub-id-type="pmid">18270514</pub-id>
        </citation>
      </ref>
      <ref id="bib31">
        <label>31</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Fisher</surname>
              <given-names>R.A.</given-names>
            </name>
          </person-group>
          <article-title>The design of experiments</article-title>
          <year>1935</year>
          <publisher-name>Oliver Boyd</publisher-name>
          <publisher-loc>Edinburgh</publisher-loc>
        </citation>
      </ref>
      <ref id="bib32">
        <label>32</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nichols</surname>
              <given-names>T.E.</given-names>
            </name>
            <name>
              <surname>Holmes</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2002</year>
          <volume>15</volume>
          <fpage>1</fpage>
          <lpage>25</lpage>
          <pub-id pub-id-type="pmid">11747097</pub-id>
        </citation>
      </ref>
      <ref id="bib33">
        <label>33</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Daniel</surname>
              <given-names>W.W.</given-names>
            </name>
            <name>
              <surname>Terrell</surname>
              <given-names>J.C.</given-names>
            </name>
          </person-group>
          <article-title>Business Statistics for Management and Economics</article-title>
          <year>1995</year>
          <publisher-name>Houghton Mifflin</publisher-name>
          <publisher-loc>Boston</publisher-loc>
        </citation>
      </ref>
      <ref id="bib34">
        <label>34</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maguire</surname>
              <given-names>E.A.</given-names>
            </name>
            <name>
              <surname>Woollett</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Spiers</surname>
              <given-names>H.J.</given-names>
            </name>
          </person-group>
          <article-title>London taxi drivers and bus drivers: A structural MRI and neuropsychological analysis</article-title>
          <source>Hippocampus</source>
          <year>2006</year>
          <volume>16</volume>
          <fpage>1091</fpage>
          <lpage>1101</lpage>
          <pub-id pub-id-type="pmid">17024677</pub-id>
        </citation>
      </ref>
      <ref id="bib35">
        <label>35</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moser</surname>
              <given-names>M.B.</given-names>
            </name>
            <name>
              <surname>Moser</surname>
              <given-names>E.I.</given-names>
            </name>
          </person-group>
          <article-title>Functional differentiation in the hippocampus</article-title>
          <source>Hippocampus</source>
          <year>1998</year>
          <volume>8</volume>
          <fpage>608</fpage>
          <lpage>619</lpage>
          <pub-id pub-id-type="pmid">9882018</pub-id>
        </citation>
      </ref>
      <ref id="bib36">
        <label>36</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Colombo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fernandez</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Nakamura</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Gross</surname>
              <given-names>C.G.</given-names>
            </name>
          </person-group>
          <article-title>Functional differentiation along the anterior-posterior axis of the hippocampus in monkeys</article-title>
          <source>J. Neurophysiol.</source>
          <year>1998</year>
          <volume>80</volume>
          <fpage>1002</fpage>
          <lpage>1005</lpage>
          <pub-id pub-id-type="pmid">9705488</pub-id>
        </citation>
      </ref>
      <ref id="bib37">
        <label>37</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Cohen</surname>
              <given-names>N.J.</given-names>
            </name>
            <name>
              <surname>Eichenbaum</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Memory, Amnesia and the Hippocampal System</article-title>
          <year>1993</year>
          <publisher-name>MIT Press</publisher-name>
          <publisher-loc>Cambridge, MA</publisher-loc>
        </citation>
      </ref>
      <ref id="bib38">
        <label>38</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spiers</surname>
              <given-names>H.J.</given-names>
            </name>
            <name>
              <surname>Maguire</surname>
              <given-names>E.A.</given-names>
            </name>
          </person-group>
          <article-title>Thoughts, behaviour, and brain dynamics during navigation in the real world</article-title>
          <source>Neuroimage</source>
          <year>2006</year>
          <volume>31</volume>
          <fpage>1826</fpage>
          <lpage>1840</lpage>
          <pub-id pub-id-type="pmid">16584892</pub-id>
        </citation>
      </ref>
      <ref id="bib39">
        <label>39</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>What we can do and what we cannot do with fMRI</article-title>
          <source>Nature</source>
          <year>2008</year>
          <volume>453</volume>
          <fpage>869</fpage>
          <lpage>878</lpage>
          <pub-id pub-id-type="pmid">18548064</pub-id>
        </citation>
      </ref>
      <ref id="bib40">
        <label>40</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Goense</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Logothetis</surname>
              <given-names>N.K.</given-names>
            </name>
          </person-group>
          <article-title>Neurophysiology of the BOLD fMRI signal in awake monkeys</article-title>
          <source>Curr. Biol.</source>
          <year>2008</year>
          <volume>18</volume>
          <fpage>631</fpage>
          <lpage>640</lpage>
          <pub-id pub-id-type="pmid">18439825</pub-id>
        </citation>
      </ref>
      <ref id="bib41">
        <label>41</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quiroga</surname>
              <given-names>R.Q.</given-names>
            </name>
            <name>
              <surname>Kreiman</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Koch</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fried</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Sparse but not ‘grandmother-cell’ coding in the medial temporal lobe</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2008</year>
          <volume>12</volume>
          <fpage>87</fpage>
          <lpage>91</lpage>
          <pub-id pub-id-type="pmid">18262826</pub-id>
        </citation>
      </ref>
      <ref id="bib42">
        <label>42</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gross</surname>
              <given-names>C.G.</given-names>
            </name>
          </person-group>
          <article-title>Genealogy of the “Grandmother cell”</article-title>
          <source>Neuroscientist</source>
          <year>2002</year>
          <volume>8</volume>
          <fpage>512</fpage>
          <lpage>518</lpage>
          <pub-id pub-id-type="pmid">12374433</pub-id>
        </citation>
      </ref>
      <ref id="bib43">
        <label>43</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hassabis</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kumaran</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Vann</surname>
              <given-names>S.D.</given-names>
            </name>
            <name>
              <surname>Maguire</surname>
              <given-names>E.A.</given-names>
            </name>
          </person-group>
          <article-title>Patients with hippocampal amnesia cannot imagine new experiences</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2007</year>
          <volume>104</volume>
          <fpage>1726</fpage>
          <lpage>1731</lpage>
          <pub-id pub-id-type="pmid">17229836</pub-id>
        </citation>
      </ref>
      <ref id="bib44">
        <label>44</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tulving</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Episodic memory: From mind to brain</article-title>
          <source>Annu. Rev. Psychol.</source>
          <year>2002</year>
          <volume>53</volume>
          <fpage>1</fpage>
          <lpage>25</lpage>
          <pub-id pub-id-type="pmid">11752477</pub-id>
        </citation>
      </ref>
      <ref id="bib45">
        <label>45</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Byrne</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Burgess</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Remembering the past and imagining the future: A neural model of spatial memory and imagery</article-title>
          <source>Psychol. Rev.</source>
          <year>2007</year>
          <volume>114</volume>
          <fpage>340</fpage>
          <lpage>375</lpage>
          <pub-id pub-id-type="pmid">17500630</pub-id>
        </citation>
      </ref>
      <ref id="bib46">
        <label>46</label>
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>LaConte</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Strother</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Cherkassky</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Support vector machines for temporal classification of block design fMRI data</article-title>
          <source>Neuroimage</source>
          <year>2005</year>
          <volume>26</volume>
          <fpage>317</fpage>
          <lpage>329</lpage>
          <pub-id pub-id-type="pmid">15907293</pub-id>
        </citation>
      </ref>
      <ref id="bib47">
        <label>47</label>
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Duda</surname>
              <given-names>O.R.</given-names>
            </name>
            <name>
              <surname>Hart</surname>
              <given-names>P.E.</given-names>
            </name>
            <name>
              <surname>Stork</surname>
              <given-names>D.G.</given-names>
            </name>
          </person-group>
          <article-title>Pattern Classification</article-title>
          <year>2001</year>
          <publisher-name>Wiley</publisher-name>
          <publisher-loc>New York</publisher-loc>
        </citation>
      </ref>
    </ref-list>
  </back>
  <floats-wrap>
    <fig id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>The Experimental Task</p>
        <p>(A) The virtual reality environment comprised two separate and distinct environments, a blue room and a green room. Each room was 15 m × 15 m and contained four “target” positions, which participants were instructed to navigate between as quickly and accurately as possible following extensive pretraining.</p>
        <p>(B) Schematic of the room layouts with the four target positions, labeled A, B, C, and D. These targets were visually delineated by identical cloth rugs (i.e., not by letters, which are depicted here only for ease of reference) placed on the floor at those positions and each 1.5 m × 1.5 m. Single objects (door, chair, picture, and clock with different exemplars per room but of similar size and color) were placed along the center of each wall to act as orientation cues. Identical small tables were placed in all four corners of the rooms to help visually delineate the wall boundaries. Single trials involved participants being instructed to navigate to a given target position with a keypad. The trial order was designed to ensure that the number of times that a target position was visited starting from another target position was matched across positions to control for goal and head direction. Once the intended destination was reached, the participant pressed a trigger button, causing the viewpoint to smoothly transition to look vertically downward at the floor (as if bowing one's head) to reveal the rug on the floor marking the target position, shown in (C).</p>
        <p>(C) At this point, a 5 s countdown was given, denoted by numerals displayed in white text overlaid on the rug (the number “3” is shown here as an example) and followed by the text label of the next target position (i.e., “A,” “B,” “C,” or “D”). The viewpoint then smoothly transitioned back to the horizontal, and navigation control was returned to the participant.</p>
        <p>(D) Environment blocks in each room consisted of two to four navigation trials and were counterbalanced across participants.</p>
      </caption>
      <graphic xlink:href="gr1"/>
    </fig>
    <fig id="fig2">
      <label>Figure 2</label>
      <caption>
        <p>Multivariate Pattern Analysis</p>
        <p>An example multivariate analysis of a pairwise position classification, in this case discriminating between position A and position B in the blue room (see <xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
        <p>(A) Only volumes acquired while the participant was standing at these two blue room positions were entered into the analysis.</p>
        <p>(B) Coverage for functional scanning is shown as a white bounding box. The search space for the searchlight algorithm <xref rid="bib14 bib24" ref-type="bibr">[14, 24]</xref>, anatomically defined to encompass the entire hippocampus and wider MTL bilaterally, is shown as a red bounding box.</p>
        <p>(C–E) The search space was stepped through voxel by voxel (C). For each voxel v<sub>i</sub> (example v<sub>i</sub> outlined in red), a spherical clique (radius 3 voxels) of N voxels c<sub>1.N</sub> was extracted with voxel v<sub>i</sub> at its center (D) to produce an N-dimensional pattern vector for each volume (E).</p>
        <p>(F) Each pattern vector was labeled according to the corresponding experimental condition (position A versus position B) and then partitioned into a training set (solid lines) and an independent test set (dashed line and indented). Patterns of activity across the voxel clique from the training set were used to train a linear SVM classifier, which was then used to make predictions about the labels of the test set. A standard k-fold crossvalidation testing regime was implemented, ensuring that all pattern vectors were used once as the test data set.</p>
        <p>(G and H) This crossvalidation step, therefore, yielded a predicted label for every pattern vector in the analysis that was then compared to the real labels to produce an overall prediction accuracy for that voxel clique (G). This accuracy value was stored with the voxel v<sub>i</sub> for later thresholding and reprojection back into structural image space (H). The whole procedure was then repeated for the next voxel v<sub>i+1</sub> (outlined in white in [C]) along in the search space until all voxels in the search space had been considered.</p>
      </caption>
      <graphic xlink:href="gr2"/>
    </fig>
    <fig id="fig3">
      <label>Figure 3</label>
      <caption>
        <p>Pairwise Position Classification</p>
        <p>Prediction maps showing the accuracies of the voxels at the center of searchlight cliques that discriminate between two arbitrarily chosen target positions in a room (apriori selected to be A versus B and C versus D) significantly above chance (50%). The resultant prediction map for a participant, bounded by the search space (indicated by the red box in <xref rid="fig2" ref-type="fig">Figure 2</xref>B), is projected onto their structural brain image. A sagittal section for each participant is displayed, showing that voxels in the body-posterior of the hippocampus bilaterally are crucial for accurate position discrimination by the classifier. The findings are highly consistent across participants. The red bar indicates percentage accuracy values as a fraction (significance threshold set at 66.07% for all participants; see <xref rid="app2" ref-type="sec">Tables S2 and S3</xref> for thresholding and comparison pair details). “R” and “L” are right and left sides of the brain, respectively.</p>
      </caption>
      <graphic xlink:href="gr3"/>
    </fig>
    <fig id="fig4">
      <label>Figure 4</label>
      <caption>
        <p>Four-Way Position Classification</p>
        <p>Prediction maps, bounded by the search space (indicated by the red box in <xref rid="fig2" ref-type="fig">Figure 2</xref>B) and projected onto each participant's structural brain image, showing the accuracies of the voxels at the center of searchlight cliques that discriminate between all four target positions in the same room significantly above chance (25%). Sagittal and coronal sections for each participant are displayed on left and right panels, respectively, showing that voxels in the body-posterior of the hippocampus bilaterally are crucial for accurate four-way position discrimination by the classifier. The findings are highly consistent across participants. The red bar indicates percentage accuracy values as a fraction (significance threshold set at 33.04% for all participants; see <xref rid="app2" ref-type="sec">Tables S2 and S3</xref> for thresholding details). Four-way position discrimination in the green room is shown for participants 1 and 2 and in the blue room for participants 3 and 4. “R” and “L” are right and left sides of the brain, respectively.</p>
      </caption>
      <graphic xlink:href="gr4"/>
    </fig>
    <fig id="fig5">
      <label>Figure 5</label>
      <caption>
        <p>Environment Classification</p>
        <p>Prediction maps, bounded by the search space (indicated by the red box in <xref rid="fig2" ref-type="fig">Figure 2</xref>B) and projected onto each participant's structural brain image, showing the accuracies of the voxels at the center of searchlight cliques that discriminate between the blue room and the green room significantly above chance. A representative sagittal section for each participant is displayed, showing that voxels in the posterior parahippocampal gyrus bilaterally are crucial for accurate discrimination between the two environments by the classifier. The result is consistent across participants. Note the dissociation between the parahippocampal gyrus prediction maps here and the hippocampus prediction maps observed for position discrimination (see <xref rid="fig3 fig4" ref-type="fig">Figures 3 and 4</xref>). The red bar indicates percentage accuracy values as a fraction (significance thresholds were set for each participant between 57.45% and 58.00%; see <xref rid="app2" ref-type="sec">Tables S2 and S3</xref>). “R” and “L” are right and left sides of the brain, respectively.</p>
      </caption>
      <graphic xlink:href="gr5"/>
    </fig>
  </floats-wrap>
</article>